# [@Erb12] Concurrent Programming for Scalable Web Architectures

* 2.3.5 Concurrency, Programming Languages and Distributed Systems
  * we consider the strong relationship between concurrent programming, programming languages and distributed systems when building large architectures. Programming distributed systems introduces a set of additional challenges compared to regular programming. The “Fallacies of Distributed Computing” [RGO06] provide a good overview on some of the important pitfalls thatmust be addressed.
  * From a soſtware engineering perspective, themajor challenges are fault tolerance, integration of distribution aspects and reliability. As we have already seen before, distributed systems are inherently concurrent and parallel, thus concurrency control is also essential.
  * Programming languages to be used for distributed systemsmust either incorporate appropriate language idioms and features tomeet these requirements. Otherwise, frameworks are necessary to provide additional features on top of the core language.
  * Ghosh et al. [Gho11] have considered the impact of programming languages on distributed systems. They pointed out thatmainstreamlanguages like Java and C++ are still themost popular choice of developing distributed systems. They are combined with middleware frameworksmost of the time, providing additional features. However, the strengths of general purpose languages do not cover themain requirements of distributed systems to a great extent.
  * The experiences with RPC-based systems (see Kendall et al. [Ken94]) and their object-based descendents (see Vinoski [Vin08]) have raised some questions to this approach.Middleware systems providing distributability compensate for features missing at the core of a language. Thus, the systems actuallymeet thenecessary requirements, but they are oſten also cumbersome touse and introduce superfluous complexity.
  * Recently, there has been an increasing interest in various alternative programming languages embracing high-level concurrency and distributed computing. Being less general, these languages focus on important concepts and idioms for distributed systems, such as component abstractions, fault tolerance and distributionmechanisms. It is interesting for our considerations thatmost of these languages oriented towards distributed computing also incorporate alternative concurrency approaches.We will have a brief look on some of these languages as part of chapter 5.
* 2.4.3 Scalability and Concurrency
  * The relation between scalability and concurrency is twofold. Fromone perspective, concurrency is a feature that can make an application scalable. Increasing load is opposed to increasing concurrency and parallelism inside the application. Thanks to concurrency, the application stays operational and utilizes the underlying hardware to its full extent. That is, above all, scaling the execution of the application amongmultiple available CPUs/cores. Although it is important to differentiate between increased performance and scalability, we can apply some rules to point out the positive impacts of parallelism for scalability. Certain problems can be solved faster when more resources are available. By speeding up tasks, we are able to conductmore work at the same time. This is especially effective when the work is composed of small, independent tasks.
  * We will now have a look at a basic law that describes the speed-up of parallel executions. Amdahl’s law [Goe06], as seen in equation 2.1, describes themaximum improvement of a system to expect when resources are added to a system under the assumption of parallel execution. A key point hereof is the ratio of serial and parallel subtasks. N is the number of processors (or cores) available, and F denotes the fraction of calculations to be executed serially.
  * From a different angle, concurrency mechanisms themselves have some kind of scalability property. That is basically the ability to support increasing numbers of concurrent activities or flows of execution inside the concurrencymodel. In practice, this involves the language idioms representing flows of executions and correspondingmappings to underlying concepts such as threads.
* 5.2.1 The Implications of Shared and Mutable State
  * Conceptually, a thread describes a sequential flow of control, that is isolated fromother activities at first glance. Unlike processes, threads share the same address space though. That implies that multiple independent threadsmay access the same variables and states concurrently. Even worse, sequential programming is built on the concept of mutable state, which means that multiple threadsmay compete forwrite operations, too.Multithreading is principally usedwith preemptive scheduling. As a result, the exact switches and interleavings betweenmultiple threads are not known in advance. This represents a strong formof indeterminacy.Without further care,mutable state and indeterminacy introduce the strong hazard of race conditions.
  * A race condition occurs when two or more thread compete for access to critical section, a section that contains state shared between threads. Due to the variety of possible interleavings, the race condition may result in various inconsistent states. For instance, a thread may read stale state while another thread is already updating it.Whenmultiple threads alter the state at the same time, either one of the changesmay last and the others get lost, or even a inconsistent state affected bymultiple changesmay persist. Eventually, we needmechanisms to guard critical sections and enforce synchronized access.
* 5.3.3 The Transactional Memory / Garbage Collection Analogy
  * Last but not least, the continuing success of Clojure1 testify thematurity of newer STMimplementations. Clojure is the first programming language that has a STMas first-class, built-in concurrency concept. Prior to Clojure, STMimplementations were mainly found as extensions to ConcurrentHaskell, based on special monads.
  * Probably themost interesting notion in this argument around TMis the analogy to garbage collection [Gro07]. While garbage collection addresses managed references, TM addresses managed state. Both concepts operate on the memory at runtime and take difficult work out of the hands of application developers.
* 9.2.3 New Takes on Concurrency and Distributed Programming
  * Distributed Dataflow Programming
    * Declarative dataflow programming provides a concurrencymodel with inherent coordination, entirely hidden from the developer. Massively parallel data-centric computing frameworks such asMapReduce [Dea08] have shown the strong points of dataflow programming. However, programming abstractions likeMapReduce heavily constrain the expressiveness compared to pure, non-distributed dataflow languages. Thus, only a small amount of existing algorithms can be applied forMapReduce-based computations. Combining an expressive programmingmodel including dataflow concurrency with a scalable and fault-tolerant distributed execution engine represents a sweet spot for programming in the large.
