
Concurrency {#ch-concurrency}
===========


~ Epigraph { caption: "Rick Hickey"}
What matters for simplicity is that there's no interleaving.
~


~LitNote
* [@Agh85b] describes 3 "foundational issues" of concurrent systems
    * Shared resources
    * Dynamic Reconfiguration
    * Inherent Parallelism
* [@Agh93] "Abstraction and Modularity Mechanisms for Concurrent Computing"
    * Allgemeine Argumente für Concurrency und Parallel Exec.
* [@Fel90] "Language and System Support for Concurrent Programming"
* [@Bac03] "Operating Systems: concurrent and distributed software design"
* [@Les09] "Concurrent Programming Paradigms, A Comparison in Scala"
* siehe [@But14] zu den 4 Teilen:
  * Bit-Level Parallelism
  * Instruction-Level Parallelism
  * Data Parallelism
  * Task_level Parallelism (das ist das für mich wichtige)
    * hier wird zwischen Shared-memory und Distributed-memory unterschieden, mit guter Erklärung
* [@And83] "Concepts and Notations for Concurrent Programming"
~

Computation can be viewed and expressed in a most basic form as the sequential execution of instructions. Such activities are called *sequential*. Under certain conditions [welche wären das? will ich das nicht lieber anders schreiben?]{.mind}, given two or more sequential activities can be executed either pseudo simultaniously (in alternation on one single processing unit) or truely simultaneously (on multiple processing units). We call this characteristic *concurrency*. 

[@Ben90; @And83; @Bus90; @Fel90]

In a most basic form, computation is by executing statements in a sequential form as a sequence of statements   

...

Concurrent computation, or simply concurrency, is a characteristic of a program or software system [citation]{.mind}. It describes...

...

For the scope of this thesis, it is important to gain a notion of concurrent computation, especially *where* it is happening. We will focus on three levels, namly the 

...


The overall requisite for any kind of concurrency is the simultaneous presence of multiple active computational units. Depending on context in which concurrency is being discussed, different terminologies are established. On a programming language level, usually *thread* is used for a concurrent unit, while in concurrency theory the *process* construct is used. However, the later will interfere with other notions of executional units discussed later on. In order to be able to refere to different concepts without any aliasing among the terminologies, we will follow the suggestion of Ben-Ari [@Ben90] and denote abstract units of activity as *tasks* through the remainder of this thesis, as a homage to the Ada programming language where "task" referes to an activity associated with a concurrent unit. The term is well known and offers a neutral way to refere to any kind of concurrently executed computation within a logical unit.

...

Baeten [@Bae05] gives a general definition:

> "*Concurrency theory is the theory of interacting, parallel and/or distributed systems*"


~ red
concurrent distributed processes [irgendwo gelesen]{.mind}
~

## Foundational Issues

~lit
Foundational Issues of all Concurrent Programming Notions
~

Many different approaches and concepts exists to approach concurrent execution. 

When programming concurrency, there are 

### Indicating Concurrent Execution

### Communication

Process/Task Communication

### Synchronization

controll access to shared state vs message passing (communication + synchronization)

critical region

## Concurrency, Parallelism and Distribution

Until now, concurrency has been discussed as a generic term denoting a simultanious execution of different program [parts]{.red}. Yet there are more diverse notions regarding how concurrent execution is implemented. 

...

~Lit
* [@Ben90]
    * "We distinguish between nodes and processes: A node is intended to represent a physically identifiable object like a computer, while the individual computers may be running multiple processes, either by sharing a single processor or on multiple processors. We assume that the internal synchronization among processes in a node is accomplished using shared-memory primitives, while processes in different nodes can communicate only by sending and receiving messages"
~

...

~LitNote
* [@Roe15] "Whereas parallelization is all about executing processes simultaneously, concur-rency concerns itself with defining processes that can function simultaneously, or can overlap in time, but don’t necessarily need to run simultaneously. A concurrent system is not by definition a parallel system. Concurrent processes can, for example, be exe-cuted on one CPU through the use of time slicing, where every process gets a certain amount of time to run on the CPU, one after another."
*  A historically denotion has been *multiprogramming* when multiple tasks are multiplexed among the processor(s), and *multiprocessing* when tasks truely parallel on multiple CPUs [@And83].
~

...

* siehe [@Bus90], ch 1.1.2

Distributed computation is regarded its own research discipline separate from parallel computation. However, both concepts are based on the same fundamental idea: Truely concurrent execution (as in *at the same time*) of physically distributed processes [@Agh99]. Agha, Frølund and Kim formulated a simple and sound argumentation [@Agh93]:

> "In a parallel computation some actions overlap in time; by implication these events must be distributed in space."

The major differences to parallel execution is that the task units cannot not share main memory [@Bus90] and ...


~Todo
define difference between process and node
~

...

~Todo
Irgendwo bei Actors schreibe ich, dass unterschied zwischen Parallel und Distributed ist, dass 1) die Componenten weiter auseinander sind, und 2) man bei Distr. nicht annehmen darf, dass der Kanal "sicher" ist. Hier kann ich gleich die ganzen Fallities of Distibuted sytems aufzählen.
~

...

**Distinguishing concurrent programs and concurrent systems**:

* siehe [@Bus90], ch 1.1.3

---

Thus, parallel and distributed execution are forms of concurrent execution, and concurrency may or may not occur in a parallel or distributed manner. The concrete nature of the concurrent execution depends on the realization used. Different abstractions used to conceive concurrent computation may 

## Correctness Properties

On a fundamental level, the basic porpose of any program is the computation of a result. A sequential program should always conclude the same results for the same inputs[^fn-sequential-side-effects], and thus it's correctness can be verified using theoretical methods, although these are not widely adoped in practice. For concurrent programs, this in not generally applicable anymore, due to the intrinsic interleaving of computations [@Ben90].

[^fn-sequential-side-effects]: Any form of side effects, like IO, are of course also forms of input to a program, thus they have to be stable as well. 

Many different issues regarding concurrent computation are well known and described in the literature. Examples are deadlocks, livelocks, starvation, race conditions, mutual exclusion and fairness. Due to the high-level view on concurrency in this thesis, we will not immerse into detailed discussions on these, as is often found in other literature concerned with concurrency concepts. Here, simply two types of properties will be of relevance, which have an effect on the correctness of concurrent programs. All the issues given above can be classified in terms of these two property types:

* Safety
  : asserts the operations that are allowed (safe) to be performed [@Bus90]. As a result, given correct inputs will result in correct outputs [@Swa14]. Examples of safety properties are race conditions, deadlock freedom and mutual exclusion.

* Liveness
  : asserts the operations that have to be performed, such that a certain state will be reached eventually (progress) [@Bus90]. In other words, given correct inputs are guaranteed correct outputs in finite time (cf. termination in sequential programs) [@Swa14]. Examples of liveness properties are fairness, starvation and reliable communication.

Safety is *invariant*, such that a property `P` holds in *every* state of *every* execution. In contrast, liveness of `P` demands it to hold in *some* state of *every* execution. As a result, there is a so-called *duality* between safety and liveness, such that the negation of a member of one type is a member of the other [@Ben90].

We've found the role of *deadlocks* (blocking operations which for some reason do not unblock) to be controversial. One the one hand, an execution path must not lead into a deadlock (safety), while they also threaten the progression of a program, thus its liveness. This is also somewhat reflected in safety being related to partial correctness, while liveness is to total correctness [@Bus90]. In contrast, so-called *livelocks* (loops never meeting their termination condition) are, as the name suggest, clearly related to liveness, for their operations are safe while the program does not progress.

## Concurrent Programming Abstraction

AKA: Realizations of Concurrent Execution; Levels of realizing Concurrent Execution

...

### Language-Construct Approach {#sec-concurrency-language-level}

~ LitNote
* allgemeines über die Abstraktionen von Nebenläufigkeit, zB Threads and Locks, Futures, STM, etc
* "explicit concurrency" [@Bus90], because "specified by the program designer"
~

#### Case Study: Concurrency in Java

Java is an object-oriented programming language with a C-inspired syntax for the so-called *Java Virtual Machine* (JVM). It offers support for expressing concurrency via threads and basic  concepts for handling access to any shared resources between such. Concurrent computation is to be defined through the so-called `Runnable`{language:java} interface. Its default implementation is available as the `Thread`{language:java} class [@Goe06]. The following example illustrates the usage for the subsequent discussion:

```{language:java}
class State {
    public int x = 0;
}

final State s = new State();

final Runnable task = () -> {
    synchronized(s) {
        x.a += 1;
        println(Thread.currentThread().getName()+" "+s.x);
    }
};

new Thread(task).start();
new Thread(task).start();
```

The offered `synchronized`{language:java} primitive is used to express mutual exlusion to a shared resource when concurrent modification to it is possible. In this case, some state denoted by `s` has to be managed between two threads having both access to it through the scope of the defined `Runnable`{language:java}. Note that though `s` is declared `final`{language:java}, its publicly visible member `x` remains mutable.

The mechanism behind Java's synchronization is *locking* on a common reference among all involved threads, the so-called *monitor* object [@Goe06]. When using `synchronized`{language:java} as a block construct, this monitor has to be provided as an argument. In this case, the state variable can simply double as the monitor in addition to being the shared resource. 

Alternatively, the `synchronized`{language:java} keyword could also have been used as part of the signature of a method in `State`{language:java} holding the logic. Such signature usage is equal to defining `synchronized(this)`{language:java} around the whole method's body, where `this`{language:java} would then refere the object `s`. Thus the method's object reference acts as the monitor, just like in the example.

A more modern alternative towards synchronization is offered by the `Lock`{language:java} interface. In contrast to `synchronized`{language:java}, where locks are always exclusive to one thread at a time, the various implementations of `Lock`{language:java} offer options of control on locking, e.g. simultanious access for multiple readers [@Sub11;@Goe06].

Expressing concurrency on the programming language level has its perils due to the overlapings of  languages concepts, e.g. the introduction of mutable state due to scopes or via visibility. Many concepts might have an influence on concurrency considerations. Shared mutability and synchronization especially require utmost care by the programmer for handling access to any data.


### Operating System Approach {#sec-concurrency-os-level}

~LitNote
* The Process, Inter-process Communication, inherent concurrency
* "implicit concurrency" [@Bus90]
* C like fork-join pattern was always a basic a way of write concurrency (outsourcing the concurrency aspect to the OS) --> Literatur?!
* Middlewares/Virtual machines can provide a transparent way to distribute threads to multiple cores --> Literatur?!
* [@Fel90] "Language and System Support for Concurrent Programming"
    * Kapitel 3: Concurrency at the Operating System Level
~


#### Case Study: Concurrent Processes in C {#sec-concurrent-c}

Only with C11[^fn-c11] did the C programming language add native support for expressing concurrency via threads. Prior to this, more operating system depending approaches like the POSIX[^fn-posix] threads binding `pthreads` had to be utilized. An additional strategy was to compose concurrent computation in an ad hoc way be relying on the operating systems scheduler to assign proccesses to processors in a concurrent way [@Bac03; @Fel90]. 

[^fn-c11]: The C standard revision of 2011, more specifically *ISO/IEC 9899:2011*. It succeeds C99, the 1999 revision.
[^fn-posix]: **P**ortable **O**perating **S**ystem **I**nterface, a collection of standardized programming interfaces for operating systems. The **X** is a remnant from the original draft name "IEEE-IX".

An operating system allows a process to spawn new processes. The party refered to as the *parent* uses system calls provided by the OS to spawn a new process called the *child*. For example, the `exec` family of Unix calls allows to instantiate arbitrary proccesses from executables referenced by a filesystem path. However the spawned child replaces the parent process, thus such calls alone are insufficient to compose concurrency [@Lov07]. 

The expedient path is the alternative `fork()` call. Such replicates the current processes address space into a spearate address space of a newly spawned child [@Bac03; @Fel90]. The following example illustrates the control flow:

```{language:cpp}
void parentBehavior(int fd[]);
void childBehavior(int fd[]);

int main(void) 
{
    int fd[2];
    pipe(fd);

    pid_t pid = fork();

    if (pid == 0) 
        childBehavior(fd);
    else
        parentBehavior(fd);
}
```

Both processes are based on the same program image. The parent receives the *Process Identifier* (PID) of the newly spawned process as a result of `fork()` while a child gets no information. This can be used to destinct the further program flow of both processes, effectivly allowing two separate behaviors. Consecutive forks are of course possible. 

A so-called *pipe* can be set up as IPC between any such two processes by utilizing additional Unix system calls. They are a form of byte stream across the memory boundries of the respective processes [@Bac03]. It's endpoints are symbolized by two file descriptors, the first `fd[0]`{language:cpp} opened in read mode, and the second `fd[1]`{language:cpp} in write mode. For example, the `parentBehavior` could write data to `fd[1]`{language:cpp}, and `childBehavior` would then be able to it from `fd[0]`{language:cpp}, thus the data crosses memory boundries. Be using this mechanism, tasks generally avoid the critical region problem [muss in dem kapitel vorher mal drauf hingewiesen werden]{.important}.

...


### Network Approach {#sec-concurrency-network-level}

~ Epigraph { caption: "Unknown"}
Distributed is truely concurrent
~

~ LitNote
* "implicit concurrency" [@Bus90]
* "Distributed is truely concurrent"
    * Networked programs are always concurrent due to the isolation and completely independent execution of their resources
    * networked can also mean a network communication of programs on the same machine / same OS, so concurrency is OS level, abstracted to the network level
* **this chapter should explain why distribution is just another form of concurrency within a system**
* [@Weg90] "Concepts and Paradigms of Object-Oriented Programming"
    * "Partitioning the state into disjoint, encapsulated chunks is the defining feature of the distributed paradigm."
* [@But14]
  * shared-memory: inter-process comm. through memory (faster)
  * distributed memory: each own local memory, inter-process comm. through network (slow, latency, all network errors)
    * hier passen die "Fallaties of distributed systems" gut rein! siehe [@RGO06]
~
