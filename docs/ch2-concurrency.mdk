
Concurrency {#ch-concurrency}
======================

~LitNote
* [@Agh85b] describes 3 "foundational issues" of concurrent systems
    * Shared resources
    * Dynamic Reconfiguration
    * Inherent Parallelism
* [@Agh93] "Abstraction and Modularity Mechanisms for Concurrent Computing"
    * Allgemeine Argumente für Concurrency und Parallel Exec.
* [@Fel90] "Language and System Support for Concurrent Programming"
* [@Bac03] "Operating Systems: concurrent and distributed software design"
* [@Les09] "Concurrent Programming Paradigms, A Comparison in Scala"
* siehe [@But14] zu den 4 Teilen:
  * Bit-Level Parallelism
  * Instruction-Level Parallelism
  * Data Parallelism
  * Task_level Parallelism (das ist das für mich wichtige)
    * hier wird zwischen Shared-memory und Distributed-memory unterschieden, mit guter Erklärung
* [@And83] "Concepts and Notations for Concurrent Programming"
~


For the scope of this thesis, it is important to gain a notion of *where* concurrency is happening. We will focus on three levels, namly the 


## Execution Order and Nondeterminism


## Synchronization and Coordination as Concurrency Control


* Threads, Locks and Shared State
* Message passing

## Concurrent vs. Parallel Execution

~LitNote
* [@Roe15] "Whereas parallelization is all about executing processes simultaneously, concur-rency concerns itself with defining processes that can function simultaneously, or can overlap in time, but don’t necessarily need to run simultaneously. A concurrent system is not by definition a parallel system. Concurrent processes can, for example, be exe-cuted on one CPU through the use of time slicing, where every process gets a certain amount of time to run on the CPU, one after another."
~

## Implicit and explicit concurrency

* siehe [@Bus90], ch 1.1.1

## Distinguishing concurrent, parallel, and distributed programs

* siehe [@Bus90], ch 1.1.2

## Distinguishing concurrent programs and concurrent systems

* siehe [@Bus90], ch 1.1.3

## Concurrency at the Programming Language Level {#sec-concurrency-language-level}

~ LitNote
allgemeines über die Abstraktionen von Nebenläufigkeit, zB Threads and Locks, Futures, STM, etc
~

## Concurrency at the Operating System Level {#sec-concurrency-os-level}

~LitNote
* The Process, Inter-process Communication, inherent concurrency
* C like fork-join pattern was always a basic a way of write concurrency (outsourcing the concurrency aspect to the OS) --> Literatur?!
* Middlewares/Virtual machines can provide a transparent way to distribute threads to multiple cores --> Literatur?!
* [@Fel90] "Language and System Support for Concurrent Programming"
    * Kapitel 3: Concurrency at the Operating System Level
~

## Concurrency at the Network Level {#sec-concurrency-network-level}

~ Epigraph { caption: "Unknown"}
Distributed is truely concurrent
~

~ LitNote
* "Distributed is truely concurrent"
    * Networked programs are always concurrent due to the isolation and completely independent execution of their resources
    * networked can also mean a network communication of programs on the same machine / same OS, so concurrency is OS level, abstracted to the network level
* **this chapter should explain why distribution is just another form of concurrency within a system**
* [@Weg90] "Concepts and Paradigms of Object-Oriented Programming"
    * "Partitioning the state into disjoint, encapsulated chunks is the defining feature of the distributed paradigm."
* [@But14]
  * shared-memory: inter-process comm. through memory (faster)
  * distributed memory: each own local memory, inter-process comm. through network (slow, latency, all network errors)
    * hier passen die "Fallaties of distributed systems" gut rein! siehe [@RGO06]
~