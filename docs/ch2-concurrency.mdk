
Concurrency {#ch-concurrency}
===========


~ Epigraph { caption: "Rick Hickey"}
What matters for simplicity is that there's no interleaving.
~


~LitNote
* [@Agh85b] describes 3 "foundational issues" of concurrent systems
    * Shared resources
    * Dynamic Reconfiguration
    * Inherent Parallelism
* [@Agh93] "Abstraction and Modularity Mechanisms for Concurrent Computing"
    * Allgemeine Argumente für Concurrency und Parallel Exec.
* [@Fel90] "Language and System Support for Concurrent Programming"
* [@Bac03] "Operating Systems: concurrent and distributed software design"
* [@Les09] "Concurrent Programming Paradigms, A Comparison in Scala"
* siehe [@But14] zu den 4 Teilen:
  * Bit-Level Parallelism
  * Instruction-Level Parallelism
  * Data Parallelism
  * Task_level Parallelism (das ist das für mich wichtige)
    * hier wird zwischen Shared-memory und Distributed-memory unterschieden, mit guter Erklärung
* [@And83] "Concepts and Notations for Concurrent Programming"
~

Computation can be viewed and expressed in a most basic form as the sequential execution of instructions. Such activities are called *sequential*. Under certain conditions [welche wären das? will ich das nicht lieber anders schreiben?]{.mind}, given two or more sequential activities can be executed either pseudo simultaniously (in alternation on one single processing unit) or truely simultaneously (on multiple processing units). We call this characteristic *concurrency*. 

[@Ben90; @And83; @Bus90; @Fel90]

In a most basic form, computation is by executing statements in a sequential form as a sequence of statements   

...

Concurrent computation, or simply concurrency, is a characteristic of a program or software system [citation]{.mind}. It describes...

...

For the scope of this thesis, it is important to gain a notion of concurrent computation, especially *where* it is happening. We will focus on three levels, namly the 

...


The overall requisite for any kind of concurrency is the simultaneous presence of multiple active computational units. To be able to discuss different concepts and notions, we will therefore denote such units of activity as *tasks* through the remainder of this thesis. In the Ada programming language, "task" referes to an activity associated with a concurrent unit. We employ this term for it is well known and will not intersect with any of the other concepts terminologies discussed in due course, thus offering us a neutral way of refering to any kind of concurrently executed computation within a logical unit.


...

Baeten [@Bae05] gives a general definition:

> "*Concurrency theory is the theory of interacting, parallel and/or distributed systems*"

## Execution Order and Nondeterminism


## Synchronization and Coordination as Concurrency Control


~LitNote
* [@And83] "Concepts and Notations for Concurrent Programming"
    * ch3 + ch4
~

* Threads, Locks and Shared State
* Message passing

## Concurrent vs. Parallel Execution

~LitNote
* [@Roe15] "Whereas parallelization is all about executing processes simultaneously, concur-rency concerns itself with defining processes that can function simultaneously, or can overlap in time, but don’t necessarily need to run simultaneously. A concurrent system is not by definition a parallel system. Concurrent processes can, for example, be exe-cuted on one CPU through the use of time slicing, where every process gets a certain amount of time to run on the CPU, one after another."
~

## Implicit and explicit concurrency

* siehe [@Bus90], ch 1.1.1

## Distinguishing Concurrency, Parallelism and Distribution

* siehe [@Bus90], ch 1.1.2


Distributed computation is regarded its own research discipline separate from parallel computation. However, both concepts are based on the same fundamental idea: Truely concurrent execution (as in *at the same time*) of physically distributed processes [@Agh99]. Agha, Frølund and Kim formulated a simple and sound argumentation [@Agh93]:

> "In a parallel computation some actions overlap in time; by implication these events must be distributed in space."

The major differences to parallel execution is that the task units cannot not share main memory [@Bus90] and 


~Todo
define difference between process and node
~

...

~Todo
Irgendwo bei Actors schreibe ich, dass unterschied zwischen Parallel und Distributed ist, dass 1) die Componenten weiter auseinander sind, und 2) man bei Distr. nicht annehmen darf, dass der Kanal "sicher" ist. Hier kann ich gleich die ganzen Fallities of Distibuted sytems aufzählen.
~

## Distinguishing concurrent programs and concurrent systems

* siehe [@Bus90], ch 1.1.3

## Concerns of Concurrent Computation

Deadlock, Livelocks, Safety, Liveness, Races, etc...

## Realizations of Concurrent Execution



### Language-Construct Approach to Concurrency {#sec-concurrency-language-level}

~ LitNote
allgemeines über die Abstraktionen von Nebenläufigkeit, zB Threads and Locks, Futures, STM, etc
~

#### Case Study: Concurrency in Java

Java is an object-oriented programming language with a C-inspired syntax for the so-called *Java Virtual Machine* (JVM). It offers support for expressing concurrency via threads and basic  concepts for handling access to any shared resources between such. Concurrent computation is to be defined through the so-called `Runnable`{language:java} interface. Its default implementation is available as the `Thread`{language:java} class [@Goe06]. The following example illustrates the usage for the subsequent discussion:

```{language:java}
class State {
    public int x = 0;
}

final State s = new State();

final Runnable task = () -> {
    synchronized(s) {
        x.a += 1;
        System.out.println(Thread.currentThread().getName() + " " + s.x);
    }
};

new Thread(task).start();
new Thread(task).start();
```

The offered `synchronized`{language:java} primitive is used to express mutual exlusion to a shared resource when concurrent modification to it is possible. In this case, some state denoted by `s` has to be managed between two threads having both access to it through the scope of the defined `Runnable`{language:java}. Note that though `s` is declared `final`{language:java}, its publicly visible member `x` remains mutable.

The mechanism behind Java's synchronization is *locking* on a common reference among all involved threads, the so-called *monitor* object [@Goe06]. When using `synchronized`{language:java} as a block construct, this monitor has to be provided as an argument. In this case, the state variable can simply double as the monitor in addition to being the shared resource. 

In contrast, the `synchronized`{language:java} keyword could also be used as part of the signature of a method in `State`{language:java} holding the logic. Such signature usage is equal to defining `synchronized(this)`{language:java} around the whole method's body, where `this`{language:java} would then refere the object `s`. Thus the method's object reference acts as the monitor, just like in the example.

A more modern alternative towards synchronization is offered by the `Lock`{language:java} interface. In contrast to `synchronized`{language:java}, where locks are always exclusive to one thread at a time, the various implementations of `Lock`{language:java} offer options of control on locking, e.g. simultanious access for multiple readers [@Sub11;@Goe06].

Expressing concurrency on the programming language level has its perils due to the overlapings of  languages concepts, e.g. the introduction of mutable state due to scopes or via visibility. Many concepts might have an influence on concurrency considerations. Shared mutability and synchronization especially require utmost care by the programmer for handling access to any data.


### Operating System Approach to Concurrency {#sec-concurrency-os-level}

~LitNote
* The Process, Inter-process Communication, inherent concurrency
* C like fork-join pattern was always a basic a way of write concurrency (outsourcing the concurrency aspect to the OS) --> Literatur?!
* Middlewares/Virtual machines can provide a transparent way to distribute threads to multiple cores --> Literatur?!
* [@Fel90] "Language and System Support for Concurrent Programming"
    * Kapitel 3: Concurrency at the Operating System Level
~


#### Case Study: Concurrent Processes in C {#sec-concurrent-c}

Only with C11[^fn-c11] did the C programming language add native support for expressing concurrency via threads. Prior to this, more operating system depending approaches like the POSIX[^fn-posix] threads binding `pthreads` had to be utilized. An additional strategy was to compose concurrent computation in an ad hoc way be relying on the operating systems scheduler to assign proccesses to processors in a concurrent way [@Bac03; @Fel90]. 

[^fn-c11]: The C standard revision of 2011, more specifically *ISO/IEC 9899:2011*. It succeeds C99, the 1999 revision.
[^fn-posix]: **P**ortable **O**perating **S**ystem **I**nterface, a collection of standardized programming interfaces for operating systems. The **X** is a remnant from the original draft name "IEEE-IX".

An operating system allows a process to spawn new processes. The party refered to as the *parent* uses system calls provided by the OS to spawn a new process called the *child*. For example, the `exec` family of Unix calls allows to instantiate arbitrary proccesses from executables referenced by a filesystem path. However the spawned child replaces the parent process, thus such calls alone are insufficient to compose concurrency [@Lov07]. 

The expedient path is the alternative *fork* call. Such replicates the current processes address space into a spearate address space of a newly spawned child [@Bac03; @Fel90]. The following example illustrates the control flow:

```{language:cpp}
void parentBehavior(int fd[]);
void childBehavior(int fd[]);

int main(void) 
{
    int fd[2];
    pipe(fd);

    pid_t pid = fork();

    if (pid == 0) 
        childBehavior(fd);
    else
        parentBehavior(fd);
}
```

Both processes are based on the same program image. The parent receives the *Process Identifier* (PID) of the newly spawned process as a result of `fork()` while a child gets no information. This can be used to destinct the further program flow of both processes, effectivly allowing two separate behaviors. Consecutive forks are of course possible. 

A so-called *pipe* can be set up between any such two processes by utilizing additional Unix system calls. They are a form of byte stream across the memory boundries of the respective processes [@Bac03]. It's endpoints are symbolized by two file descriptors, the first `fd[0]`{language:cpp} opened in read mode, and the second `fd[1]`{language:cpp} in write mode. For example, the `parentBehavior` could write data to `fd[1]`{language:cpp}, and `childBehavior` would then be able to it from `fd[0]`{language:cpp}, thus the data crosses memory boundries. Be using this mechanism, tasks generally avoid the critical region problem [muss in dem kapitel vorher mal drauf hingewiesen werden]{.important}, and therefore pipes are heavily employed in the C programming language.

...


### Network Approach to Concurrency {#sec-concurrency-network-level}

~ Epigraph { caption: "Unknown"}
Distributed is truely concurrent
~

~ LitNote
* "Distributed is truely concurrent"
    * Networked programs are always concurrent due to the isolation and completely independent execution of their resources
    * networked can also mean a network communication of programs on the same machine / same OS, so concurrency is OS level, abstracted to the network level
* **this chapter should explain why distribution is just another form of concurrency within a system**
* [@Weg90] "Concepts and Paradigms of Object-Oriented Programming"
    * "Partitioning the state into disjoint, encapsulated chunks is the defining feature of the distributed paradigm."
* [@But14]
  * shared-memory: inter-process comm. through memory (faster)
  * distributed memory: each own local memory, inter-process comm. through network (slow, latency, all network errors)
    * hier passen die "Fallaties of distributed systems" gut rein! siehe [@RGO06]
~


#### Case Study: Distributed Database Systems 

~LitNote
* <https://pdfs.semanticscholar.org/cd48/1697170f25b366dee904dcfca7988bb87645.pdf>
* <http://www.ict.griffith.edu.au/~rwt/uoe/1.4.dcc.html>
* <http://www.vldb.org/pvldb/vol10/p553-harding.pdf>
~