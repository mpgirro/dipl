
# Concurrent Computation {#ch-concurrency}


~ Epigraph { caption: "Rich Hickey"}
What matters for simplicity is that there's no interleaving.
~


Computation is conceived through the execution of instructions. We call activities *sequential*, if their list of atomic statements execute in a sequential manner. Given two or more sequential activities execute either pseudo-simultaneously (in alternation on a single processing unit), or truly simultaneously (on multiple processing units), they interleave and we therefore call these activities *concurrent*. Interleaving weakens the total ordering from sequential actions to a merely partial ordering. As a result, concurrency is *nondeterministic*. Repeated invocations on the same input can result in different outputs in general [@Ben90;@Sco06;@Agh93].

The overall requisite for every kind of concurrency is the simultaneous presence of multiple active computational units. Depending on the context in which scholar discuss concurrency, they established different terminologies. The programming language level often uses the *thread* as the concurrent unit. Concurrency theory uses the *process* construct in general [@Sco06]. However, the term "process" interferes with other notions of executional units we discuss in due course. In order to be able to refer to different concepts without aliasing among the terminologies, we follow the suggestion of Ben-Ari [@Ben90] and denote abstract units of activity as *tasks* throughout the remainder of this thesis. This designation is a homage to the Ada programming language, where "task" refers to an activity associated with a concurrent unit. The term is well-known and offers a neutral way to refer to every kind of concurrently executed computation within a logical unit.


## Foundational Issues {#sec-foundational-issues}


Many different approaches to concurrency exist. Regardless of the chosen notion, we always have to pay attention to three basic concerns [@And83]:

* Expression of concurrent execution
  : Concurrent computation must be *indicated*. The literature proposes various abstrations and subsequently numerous implementations exist in practive. We discuss some of these abstractions in due course. In general, all concurrency abstractions need to provide the possibility to define as well as manage tasks [@Bac03]. Examples are *channels*, *coroutines*, *fork and joins*, *futures* and *threads*. The interfaces for the creation of tasks can be arbitrary, e.g.\ as primitives directly within a programming language, as libraries and through operating system calls.  

* Communication
  : Tasks must be able to interaction and cooperate. *Communication* allows tasks to influence each other [@And83]. The *shared state* communication style rests on commonly accessible memory (e.g.\ variables, objects, etc.). Several tasks then interact by reading and writing to the same state. In contrast, *message passing* communication forgoes any access to shared data. Instead, it builds on the exchange of messages (fixed, immutable data) sent through communication *links*. The links are additionally required elements. Shared memory is only possible among tasks who gain access to a common memory section. A shared physical machine is the most basic form to get access to shared memory between tasks. The network can also simulate a mutual memory region. Message passing on the other hand does not even concern the locality or remoteness of memory. Messages can transit numerous links between sender and recipient. Therefore, messages easily bypass machine boundaries [@Fel90]. Message passing can happen in either *synchronous* fashion (messages are sent and the execution delays until the response is available) or *asynchronous* fashion (the execution resumes immediately after sending a message) [@And83].

* Synchronization
  : Although concurrent execution has merely a partial ordering, communication still requires some order constraints. We must perform an action before we can detect its effect. *Synchronization* refers to mechanisms we use to ensure such constraints [@And83;@Mey97]. *Semaphores*, *locks* and *transactional memory* are prominent examples. The literature mostly discusses synchronization techniques regarding the shared state scenario, since the communication requires state modification by the sender before the receiver is allowed to read the information. Also, only one task can modify state at a time to avoid unexpected behavior due to low-level data races. The modification or evaluation of shared state occurs within a *critical section*. Synchronization mechanisms realize *mutual exclusion* where no task can access the shared state while another is within its critical section [@Ben90;@Sco06].
  : Message passing on the other hand provides an implicit form of synchronization. Intrinsically, a message must be sent before it can be received. As a result, the semantics of message passing constrains the order by design [@And83]. Synchronous passing additionally constraints the sender from resuming its computation until it receives an answer.
  : There are two kinds of synchronization mechanisms. *Enforced primitives* guarantee no access to the state outside the primitive, thus ensuring order. *Unenforced primitives* grant a certain degree of freedom in their usage and therefore provide no guarantee of mutual exclusion, as do e.g.\ semaphores [@Fel90]. 


## Concurrency, Parallelism and Distribution


So far, we've discussed concurrency as a generic term that denotes a simultaneous execution of activities. Yet there are more diverse notions that regard the implementation of concurrent execution. A strict distinction in our terminology is therefore in order. We use *concurrency* to refer to the multiplexing of multiple tasks among one or more processors. We cannot make more specific assumptions in general. On a single CPU, the interleaving of computation is merely pseudo-simultaneous via time slicing, since all computation is still sequential [@Roe15].

*Parallelism* refers to truly simultaneous execution on different CPUs. If parallel execution is possible depends on the concurrency abstraction and its implementation [@Bac03]. On a programming language level, referencing components is usually subject to physical limitations regarding the programs memory. For example, objects can only reference other objects that are inside the memory space of the same program in general [@Mey97]. We regard a notion of concurrent objects that is able surmount this restriction in due course. This limitation on memory space does not prevent us from writing concurrent code in general. But the limitation certainly complicates the writing of parallel code, e.g.\ when we use shared state communication. Parallel execution requires code execution on different CPU cores *at the same time*, which usually means distinct process memory boundaries. Inter-component communication must happen across memory and process boundaries. If a programming language uses a virtual machine (VM) to execute code, we can charge it to this VM to provide us with transparent inter-component communication across boundaries. For example, the *Java Virtual Machine* (JVM) has different approaches to implement threads. One is to map Java threads to system processes for parallelization [@Hal09]. The JVM hides the resulting gap in memory sections transparently. Writing explicit parallel code, e.g.\ with a *Fork/Join* framework, can be painful and requires us to explicitly prepare code segments and data that we can process in parallel [@Sco06;@Lea00].

*Distributed computation* is regarded by the literature as its own research discipline separate from parallel computation. However, both concepts build on the same fundamental idea: Truly concurrent execution (as in *at the same time*) of physically distributed tasks [@Agh99]. Agha, FrÃ¸lund and Kim formulated a simple and sound argumentation [@Agh93]: 

> "In a parallel computation some actions overlap in time; by implication these events must be distributed in space."

This argument suggests that every parallel task is also a distributed task in a certain sense. The major distinction is that we expect parallel tasks to be physically closer to each other (same CPU) than distributed tasks (distinct CPUs and machines). Due to this distance, distributed tasks cannot share main memory directly [@Bus90;@Agh99]. Distribution therefore relies on message passing communication over the network. Of course, we can use the network to create abstractions for shared memory, so-called *spaces*. One example for a space is the *Linda* model [@Ben90;@Sco06].

Due to this physical separation, distributed tasks execute on different locations (host machines) in general. We also refer to these hosts as *nodes* [@Ben90]. A single node can have one or more processors on which one or more tasks run concurrently. As a result, we make three fundamental observations about the interrelations of concurrency, parallelism, and distribution:

1. Concurrent systems can be parallel and distributed.
2. Parallel and distributed systems are inherently concurrent.
3. Distributed systems with two or more nodes are parallel. 

Baeten [@Bae05] gives a general definition that incorporates these interrelations and which reflects our view on concurrency within this thesis:

> "Concurrency theory is the theory of interacting, parallel and/or distributed systems."

In subsequent sections, we merit attention to two selected task abstractions. Both conceive concurrent computation in general. Additionally, they are able to provide parallelization and even distribution in a transparent way.


## Correctness Properties


On a fundamental level, the basic purpose of every program is the computation of a result. From a sequential program, we always expect the same results for the same inputs[^fn-sequential-side-effects]. We can verify the correctness of a program using theoretical methods, although these methods are not widely adopted in practice. For concurrent programs, many standard verification techniques do not hold anymore, due to the intrinsic interleaving of computations [@Ben90].

[^fn-sequential-side-effects]: All sorts of side effects, like IO, are also forms of input to a program and must be stable as well. 

Many different issues regarding concurrent computation are well-known in the literature. Examples are deadlocks, livelocks, starvation, race conditions, mutual exclusion and fairness. Due to the high-level view on concurrency in this thesis, we don't immerse into detailed discussions on each of these issues, as we often find it in other literature that concerns concurrency concepts. Here, simply two types of properties are relevant to us. Both types have an effect on the correctness of concurrent programs. We can classify all the issues we gave above in terms of these two property types [@Bus90;@Swa14;@Sin09]:

* Safety
  : asserts the operations that are allowed (safe) perform. As a result, given correct inputs result in correct outputs, while the computation never enters an undesired state. Examples of safety properties are race conditions, deadlock freedom, and mutual exclusion. Informally, safety guarantees that "nothing bad will happen".

* Liveness
  : asserts the operations that have to be performed, such that a certain state will be reached eventually (progress). In other words, given we provide correct inputs, we have the guarantee for correct outputs in finite time (cf.\ termination in sequential programs). Examples of liveness properties are fairness, starvation, and reliable communication. Informally, liveness guarantees that "something good will happen".

Safety is *invariant*, such that a property `P` holds in *every* state of *every* execution. In contrast, liveness of `P` demands that the property holds in *some* state of *every* execution. As a result, safety and liveness have a so-called *duality* relationship. The negation of a member of one type is a member of the other [@Ben90]. Safety relates to *partial correctness* (the result is correct if the program terminates). Liveness on the other hand relates to *total correctness* (the programs terminates with a correct result) [@Bus90].

We've found *deadlocks* (blocking operations which for some reason do not unblock) to have a controversial role. [On the one hand, an execution path must not lead into a deadlock (safety), while a deadlock also threatens the progression of a program, thus its liveness]{.red}. In contrast, so-called *livelocks* (loops never meeting their termination condition) are, as the name suggest, clearly related to liveness. The operations of a livelock are safe while the program does not progress.


## Programming Abstractions {#sec-concurrency-programming-abstractions}


Most programs are concurrent in some way. An example of *implicit concurrency* is *input/output* (IO). There, we trigger IO devices to perform operations simultaneous to the executing program [@Bus90]. Also, compilers and interpreters exploit the concurrency inherent to a language's constructs. On the other hand, *explicit concurrency* must be indicated. We require appropriate programming abstractions. In general, we require concurrency abstractions to be powerful and expressive models, fit harmoniously into the programming language in terms of their interface, and exploit the underlying hardware resources efficiently [@Shi97]. 


### Language-Construct Approach {#sec-concurrency-language-level}


Many different approaches to explicitly express concurrent computation on a programming language level were proposed over the decades and are now in use. A programming language either provides concurrency constructs by design, or we can utilize such constructs through libraries and frameworks [@Ben90;@Sco06]. Therefore, most concurrent task abstractions are available in most programming languages. The remainder of this thesis is in the context of Java and its virtual machine. A brief discussion of Java's basic approach towards concurrency is in order, since alternative abstractions have to build on it.   


#### Case Study: Concurrency in Java


Java is an object-oriented programming language with a C-inspired syntax for the JVM. The language expresses concurrency via threads and offers basic concepts to manage the access to shared resources. We define concurrent computation through the `Runnable`{language:java} interface. The default implementation of `Runnable`{language:java} is available in the `Thread`{language:java} class [@Goe06;@Gos15]. The following example illustrates the principle approach:

```{language:java}
class State {
    public int x = 0;
}

final State s = new State();
final Runnable task = () -> {
    synchronized(s) {
        s.x += 1;
        out.println(Thread.currentThread().getName() + " " + s.x);
    }
};

new Thread(task).start();
new Thread(task).start();
```

The `synchronized`{language:java} primitive allows us to express mutual exclusion to a shared resource whenever concurrent modification to this resource is possible. In this example, `s` denotes some state. Two threads both have access to `s` through the scope of the `Runnable`{language:java} lambda. Note that though we declared `s` as `final`{language:java}, its publicly visible member `x` remains mutable.

The mechanism behind Java's synchronization is *locking* on a common reference among all involved threads, the so-called *monitor* object [@Goe06;@Gos15]. When we use `synchronized`{language:java} as a block construct, we must provide this monitor as an argument. In our example, the state variable simply doubles as the monitor in addition to being the shared resource. Alternatively, we could have used the `synchronized`{language:java} keyword also as a part of the signature of a method in `State`{language:java} which holds the logic. A `synchronized`{language:java} method signature is equal to `synchronized(this)`{language:java} around the whole method's body, where `this`{language:java} refers to the object `s`. The method's object reference then acts as the monitor, just as in our example.

A more modern alternative towards synchronization is the `Lock`{language:java} interface. The previous `synchronized`{language:java} is an enforced synchronization primitive. The locks of `synchronized`{language:java} are always exclusive to one thread at a time. In contrast, the various implementations of `Lock`{language:java} don't need to be enforced. `Lock`{language:java}s can therefore offer fine-grained options to control the locking, e.g.\ for simultaneous access of multiple readers [@Sub11;@Goe06]. To provide this degree of freedom, Java neither detects shared state nor requires its synchronization per se. As a result, programmers can easily introduce data races when they simply omit access control. Alternative concurrency abstractions for Java, e.g.\ through libraries and frameworks, always have to take this into account.

Expressing concurrency on the programming language level has its perils due to the overlapping of  language concepts. We've already demonstrated the introduction of mutable state via scopes and visibility. Many concepts have an influence on concurrency considerations. Shared mutability and synchronization especially require utmost care of the programmer for handling access to the data.


### Operating System Approach {#sec-concurrency-os-level}


Operating systems (OS) use the *process* as their computational model. A process describes the instantiation of a program image with associated *resources* (e.g.\ memory). Processes express dynamic execution. In the most basic case, a single processor alternately executes these computational units. *Scheduling* is the activation and passivation of processes and it is in the responsibility of the operating system. Scheduling results in a quasi-parallel execution of active processes. If we utilize multiple processors, the execution is truly parallel [@Bac03;@Sco06]. As a result, we can state that processes are inherently concurrent units due to their execution modality.

In contrast, *threads* are tasks *inside* a process. One process can have numerous threads which all share the same memory space [@Tan07]. Since threads within the same process have access to the same memory locations, we are free to introduce shared state among them. In the Java case study, we already demonstrated that shared state requires synchronization. In contrast to the JVM, an operating system strictly enforces the memory boundaries between processes. Communication between two processes requires either an explicit arrangement of so-called *shared memory*, or another means of message passing communication which we subsume as *inter-process communication*[^fn-ipc] (IPC) [@Bac03;@Lov07]. We regard an extended focus on OS-conceived concurrent tasks which rely on IPC in due course. A consolidating example for future reference is in order.

[^fn-ipc]: In concurrency theory, *process* is also the general term for a concurrent unit, as we've mentioned. Therefore, the literature often denotes all communication between concurrent units as *inter-process communication*. To avoid confusion, we use the IPC designation only for communication mechanisms between OS processes.


#### Case Study: Concurrent Processes in C {#sec-concurrent-c}


Only with C11[^fn-c11] did the programming language add native support for the expression of concurrency via threads. Prior to this, programmers had to use more operating system depending approaches like the POSIX[^fn-posix] threads binding `pthreads`. An additional strategy was to compose concurrent computation in an ad hoc way by relying on the operating system's scheduler to assign processes to processors in a concurrent way [@Bac03; @Fel90]. 

[^fn-c11]: The C standard revision of 2011, specifically *ISO/IEC 9899:2011*. It succeeds C99, the 1999 revision.
[^fn-posix]: __P__ortable __O__perating __S__ystem __I__nterface, a collection of standardized programming interfaces for operating systems. The **X** is a remnant from the original draft name "IEEE-IX".

An operating system allows a process to spawn new processes. The party we call the *parent* uses system calls that the OS provides to spawn a new process we call the *child*. For example, the `exec`-family of Unix calls allows us to instantiate arbitrary processes from executables we reference through a filesystem path. However, the new child replaces the parent process. `exec`-calls alone are therefore not insufficient to compose concurrency [@Lov07]. The expedient path is the alternative `fork`-call. It replicates the current processe's address space into a separate address space of a new child [@Bac03; @Fel90]. The following example illustrates the control flow:

```{language:cpp}
void 
parentBehavior(int fd[]);

void 
childBehavior(int fd[]);

int 
main(void) 
{
    int fd[2];
    pipe(fd);

    pid_t pid = fork();

    if (pid == 0) 
        childBehavior(fd);
    else
        parentBehavior(fd);
}
```

Both processes are based on the same program image. The parent receives the *process identifier* (PID) of the child process as the result of `fork()`. The child does not receive its own PID information. We can therefore use the PID to distinguish the further program flow of both processes. This mechanism effectively allows two separate behaviors. Consecutive forks are possible of course. 

By using additional Unix system calls, we install a so-called *pipe* as the IPC between the two processes. Pipes are a form of byte stream across the memory boundaries of the respective processes [@Bac03]. Since Unix follows an *everything is a file* design principle, two file descriptors symbolize the endpoints of the pipe. This principle makes the interfaces simple and consistent [@Shi97]. The first descriptor `fd[0]`{language:cpp} is in read mode and the second `fd[1]`{language:cpp} in write mode. For example, the `parentBehavior` writes data to `fd[1]`{language:cpp} and the `childBehavior` subsequently reads this data from `fd[0]`{language:cpp}. Hence, the data crosses memory boundaries. Pipes are a communication link for message passing which avoid the critical region problem.


### Network Approach {#sec-concurrency-network-level}


As we've outlined, distribution is another approach to the conception of concurrent execution within a system. Generally, besides the lack of shared memory, the distinguishing characteristic between parallel and distributed computing is the geographical distance between tasks. Therefore, the communication between distributed tasks happens via networked message passing mechanisms. Networks introduce a wide range of perils. We can neither assume that the communication links are reliable nor static. Also, messages are more costly in terms of time (latency) and effort (e.g.\ due to data serialization) [@Agh99]. The famous *Fallacies of Distributed Computing* by Deutsch subsume many of the problematic aspects [@Tan07]:

* Fallacy 1: *the network is reliable*.
* Fallacy 2: *latency is zero*.
* Fallacy 3: *bandwidth is infinite*.
* Fallacy 4: *the network is secure*.
* Fallacy 5: *topology doesn't change*.
* Fallacy 6: *there is one administrator*.
* Fallacy 7: *transport cost is zero*.
* Fallacy 8: *the network is homogeneous*[^fn-8th-fallacy].
{ list-style-type:none }

Fallacies 4.\ and 6.\ are outside the scope of this thesis. The remaining aspects are relevant to some concepts we already and soon discuss. For example, Fallacy 2.\ affects synchronous communication. Asynchronous messaging does not concern the latencies which delay the travel time of messages. Time constrains synchronous communication and therefore the network-induced latencies also affect synchronization.

[^fn-8th-fallacy]: Fallacy 8.\ was not part of Deutsch's original proposal, but later added by Gosling. Hence, the literature sometimes refers to merely seven fallacies. 

All concurrency through distribution is subject to these fallacies in general. Network programming interfaces depend on the concrete network mechanism. A very basic example is the concept of *sockets* that Unix introduced. The standard interface of sockets provides the means to write data to the network. Alternatively, we use a socket to receive data from the network [@Tan07;@Bac03]. Practically every operating system provides sockets and bindings exist for almost all programming languages. Therefore, sockets are a technology heterogenous mechanism, although rather low-level (transport layer). More high-level is for example HTTP (__H__yper__t__ext __T__ransfer __P__rotocol), a generic, originally stateless and text-based communication protocol that provides platform-neutral data transfer on the application-level [@Cou05].