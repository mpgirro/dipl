
Notions of Concurrency {#ch-concurrency}
======================

~LitNote
* [@Agh85b] describes 3 "foundational issues" of concurrent systems
    * Shared resources
    * Dynamic Reconfiguration
    * Inherent Parallelism
* [@Agh93] "Abstraction and Modularity Mechanisms for Concurrent Computing"
    * Allgemeine Argumente für Concurrency und Parallel Exec.
* [@Fel90] "Language and System Support for Concurrent Programming"
* [@Bac03] "Operating Systems: concurrent and distributed software design"
* [@Les09] "Concurrent Programming Paradigms, A Comparison in Scala"
* siehe [@But14] zu den 4 Teilen:
  * Bit-Level Parallelism
  * Instruction-Level Parallelism
  * Data Parallelism
  * Task_level Parallelism (das ist das für mich wichtige)
    * hier wird zwischen Shared-memory und Distributed-memory unterschieden, mit guter Erklärung
* [@And83] "Concepts and Notations for Concurrent Programming"
~

Concurrent computation, or simply concurrency, is a characteristic of a program or software system [citation]{.mind}. It describes...

...

For the scope of this thesis, it is important to gain a notion of concurrent computation, especially *where* it is happening. We will focus on three levels, namly the 

...



## Execution Order and Nondeterminism


## Synchronization and Coordination as Concurrency Control


* Threads, Locks and Shared State
* Message passing

## Concurrent vs. Parallel Execution

~LitNote
* [@Roe15] "Whereas parallelization is all about executing processes simultaneously, concur-rency concerns itself with defining processes that can function simultaneously, or can overlap in time, but don’t necessarily need to run simultaneously. A concurrent system is not by definition a parallel system. Concurrent processes can, for example, be exe-cuted on one CPU through the use of time slicing, where every process gets a certain amount of time to run on the CPU, one after another."
~

## Implicit and explicit concurrency

* siehe [@Bus90], ch 1.1.1

## Distinguishing concurrent, parallel, and distributed programs

* siehe [@Bus90], ch 1.1.2

## Distinguishing concurrent programs and concurrent systems

* siehe [@Bus90], ch 1.1.3

## Concurrency at the Programming Language Level {#sec-concurrency-language-level}

~ LitNote
allgemeines über die Abstraktionen von Nebenläufigkeit, zB Threads and Locks, Futures, STM, etc
~

### Case Study: Concurrency in Java

Java is an object-oriented programming language with a C-inspired syntax for the so-called *Java Virtual Machine* (JVM). It offers support for expressing concurrency via threads and basic  concepts for handling access to any shared resources between such. Concurrent computation is to be defined through the so-called `Runnable`{language:java} interface. Its default implementation is available as the `Thread`{language:java} class [irgendwas zitieren!]{.mind}. The following example illustrates the usage for the subsequent discussion:

```{language:java}
class State {
  public int a = 0;
}

final State s = new State();

final Runnable task = () -> {
  synchronized(s) {
    s.a += 1;
    System.out.println(Thread.currentThread().getName() + " " + s.a);
  }
};

new Thread(task).start();
new Thread(task).start();
```


The offered `synchronized`{language:java} primitive is used to express mutual exlusion to a shared resource when concurrent modification to it is possible. In this case, some state denoted by `s` has to be managed between two threads having both access to it through the scope of the defined `Runnable`{language:java}. Note that though `s` is declared `final`{language:java}, its member `a` remains mutable and is publicly visible.

The mechanism behind Java's synchronization is *locking* on a common reference among all involved threads, the so-called *monitor* object [@Goe06]. When using `synchronized`{language:java} as a block construct, this monitor has to be provided as an argument. In this case, the state variable can simply double as the monitor in addition to being the shared resource. 

In contrast, the `synchronized`{language:java} keyword could also be used as part of the signature of a method in `State`{language:java} holding the logic. Such signature usage is equal to defining `synchronized(this)`{language:java} around the whole method's body, where `this`{language:java} would then refere the object `s`. Thus the method's object reference acts as the monitor, just like in the example.

A more modern alternative towards synchronization is offered by the `Lock`{language:java} interface. In contrast to `synchronized`{language:java}, where locks are always exclusive to one thread at a time, the various implementations of `Lock`{language:java} offer options of control on locking, e.g. simultanious access for multiple readers [@Sub11].

Expressing concurrency on the programming language level has its perils due to the overlapings of  languages concepts, e.g. the introduction of mutable state due to scopes or via visibility. Many concepts might have an influence on concurrency considerations. Shared mutability and synchronization especially require utmost care by the programmer for handling access to any data.

## Concurrency at the Operating System Level {#sec-concurrency-os-level}

~LitNote
* The Process, Inter-process Communication, inherent concurrency
* C like fork-join pattern was always a basic a way of write concurrency (outsourcing the concurrency aspect to the OS) --> Literatur?!
* Middlewares/Virtual machines can provide a transparent way to distribute threads to multiple cores --> Literatur?!
* [@Fel90] "Language and System Support for Concurrent Programming"
    * Kapitel 3: Concurrency at the Operating System Level
~

### Case Study: Concurrent Processes in C {#sec-concurrent-c}

~Todo
Weil es jetzt weiter oben ist, muss ich hier noch Pipes erklären! Und das UNIX system. Und POSIX!
~

Only with C11[^fn-c11] did the C programming language add explicit support for expressing concurrency via threads. At its core it is a rather old language (though still heavily used), thus the strategy was for decades to compose concurrent computation in an ad hoc way be relying on the operating systems scheduler to assign proccesses to processors in a concurrent way [@Bac03; @Fel90]. 

[^fn-c11]: The C standard revision of 2011, more specifically *ISO/IEC 9899:2011*. It succeeds C99, the 1999 revision.

An operating system allows a process to spawn new processes. The party refered to as the *parent* uses system calls provided by the OS to spawn a new process called the *child*. For example, the `exec` family of Unix calls allows to instantiate arbitrary proccesses from executables referenced by a filesystem path. However the spawned child replaces the parent process, thus such calls alone are insufficient to compose concurrency [@Lov07]. 

The expedient path is the alternative *fork* call. Such replicates the current processes address space into a spearate address space of a newly spawned child [@Bac03; @Fel90]. The following example illustrates the control flow:

```{language:cpp}
void parentBehavior(void);
void childBehavior(void);

int main(void) 
{
    pid=fork();
    if (pid == 0) 
        childBehavior();
    else
        parentBehavior();
}
```

Both processes are based on the same program image. The parent receives its *process ID* (PID) as a result of `fork()` while a child gets no information. This offers to destinct the further program flow of both processes, effectivly allowing two separate behaviors. Consecutive forks are of course possible. 

A pipe can be set up between any such two processes as an IPC by utilizing additional system calls. Thus they are heavily employed in the C programming language. 

...

## Concurrency at the Network Level {#sec-concurrency-network-level}

~ Epigraph { caption: "Unknown"}
Distributed is truely concurrent
~

~ LitNote
* "Distributed is truely concurrent"
    * Networked programs are always concurrent due to the isolation and completely independent execution of their resources
    * networked can also mean a network communication of programs on the same machine / same OS, so concurrency is OS level, abstracted to the network level
* **this chapter should explain why distribution is just another form of concurrency within a system**
* [@Weg90] "Concepts and Paradigms of Object-Oriented Programming"
    * "Partitioning the state into disjoint, encapsulated chunks is the defining feature of the distributed paradigm."
* [@But14]
  * shared-memory: inter-process comm. through memory (faster)
  * distributed memory: each own local memory, inter-process comm. through network (slow, latency, all network errors)
    * hier passen die "Fallaties of distributed systems" gut rein! siehe [@RGO06]
~


### Case Study: Distributed Database Systems 

~LitNote
* <https://pdfs.semanticscholar.org/cd48/1697170f25b366dee904dcfca7988bb87645.pdf>
* <http://www.ict.griffith.edu.au/~rwt/uoe/1.4.dcc.html>
* <http://www.vldb.org/pvldb/vol10/p553-harding.pdf>
~