
Notions of Concurrency {#ch-concurrency}
======================

~LitNote
* [@Agh85b] describes 3 "foundational issues" of concurrent systems
    * Shared resources
    * Dynamic Reconfiguration
    * Inherent Parallelism
* [@Agh93] "Abstraction and Modularity Mechanisms for Concurrent Computing"
    * Allgemeine Argumente für Concurrency und Parallel Exec.
* [@Fel90] "Language and System Support for Concurrent Programming"
* [@Bac03] "Operating Systems: concurrent and distributed software design"
* [@Les09] "Concurrent Programming Paradigms, A Comparison in Scala"
* siehe [@But14] zu den 4 Teilen:
  * Bit-Level Parallelism
  * Instruction-Level Parallelism
  * Data Parallelism
  * Task_level Parallelism (das ist das für mich wichtige)
    * hier wird zwischen Shared-memory und Distributed-memory unterschieden, mit guter Erklärung
* [@And83] "Concepts and Notations for Concurrent Programming"
~


For the scope of this thesis, it is important to gain a notion of concurrent computation, especially *where* it is happening. We will focus on three levels, namly the 

...



## Execution Order and Nondeterminism


## Synchronization and Coordination as Concurrency Control


* Threads, Locks and Shared State
* Message passing

## Concurrent vs. Parallel Execution

~LitNote
* [@Roe15] "Whereas parallelization is all about executing processes simultaneously, concur-rency concerns itself with defining processes that can function simultaneously, or can overlap in time, but don’t necessarily need to run simultaneously. A concurrent system is not by definition a parallel system. Concurrent processes can, for example, be exe-cuted on one CPU through the use of time slicing, where every process gets a certain amount of time to run on the CPU, one after another."
~

## Implicit and explicit concurrency

* siehe [@Bus90], ch 1.1.1

## Distinguishing concurrent, parallel, and distributed programs

* siehe [@Bus90], ch 1.1.2

## Distinguishing concurrent programs and concurrent systems

* siehe [@Bus90], ch 1.1.3

## Concurrency at the Programming Language Level {#sec-concurrency-language-level}

~ LitNote
allgemeines über die Abstraktionen von Nebenläufigkeit, zB Threads and Locks, Futures, STM, etc
~

## Concurrency at the Operating System Level {#sec-concurrency-os-level}

~LitNote
* The Process, Inter-process Communication, inherent concurrency
* C like fork-join pattern was always a basic a way of write concurrency (outsourcing the concurrency aspect to the OS) --> Literatur?!
* Middlewares/Virtual machines can provide a transparent way to distribute threads to multiple cores --> Literatur?!
* [@Fel90] "Language and System Support for Concurrent Programming"
    * Kapitel 3: Concurrency at the Operating System Level
~

### Case Study: Concurrent Processes in C {#sec-concurrent-c}

~Todo
Weil es jetzt weiter oben ist, muss ich hier noch Pipes erklären! Und das UNIX system. Und POSIX!
~

Only with C11[^fn-c11] did the C programming language add explicit support for expressing concurrency via threads. At its core it is a rather old language (though still heavily used), thus the strategy was for decades to compose concurrent computation in an ad hoc way be relying on the operating systems scheduler to assign proccesses to processors in a concurrent way [@Bac03]. 

[^fn-c11]: The C standard revision of 2011, more specifically *ISO/IEC 9899:2011*. It succeeds C99, the 1999 revision.

An operating system allows a process to spawn new processes. The party refered to as the *parent* uses system calls provided by the OS to spawn a new process called the *child*. For example, the `exec` family of Unix calls allows to instantiate arbitrary proccesses from executables referenced by a filesystem path. However the spawned child replaces the parent process, thus such calls alone are insufficient to compose concurrency [@Lov07]. 

The expedient path is the alternative *fork* call. Such replicates the current processes address space into a spearate address space of a newly spawned child [@Bac03; @Fel90]. The following example illustrates the control flow:

```{language:cpp}
void parentBehavior(void);
void childBehavior(void);

int main(void) 
{
    pid=fork();
    if (pid == 0) 
        childBehavior();
    else
        parentBehavior();
}
```

Both processes are based on the same program image. The parent receives its *process ID* (PID) as a result of `fork()` while a child gets no information. This offers to destinct the further program flow of both processes, effectivly allowing two separate behaviors. Consecutive forks are of course possible. 

A pipe can be set up between any such two processes as an IPC by utilizing additional system calls. Thus they are heavily employed in the C programming language. 

...

## Concurrency at the Network Level {#sec-concurrency-network-level}

~ Epigraph { caption: "Unknown"}
Distributed is truely concurrent
~

~ LitNote
* "Distributed is truely concurrent"
    * Networked programs are always concurrent due to the isolation and completely independent execution of their resources
    * networked can also mean a network communication of programs on the same machine / same OS, so concurrency is OS level, abstracted to the network level
* **this chapter should explain why distribution is just another form of concurrency within a system**
* [@Weg90] "Concepts and Paradigms of Object-Oriented Programming"
    * "Partitioning the state into disjoint, encapsulated chunks is the defining feature of the distributed paradigm."
* [@But14]
  * shared-memory: inter-process comm. through memory (faster)
  * distributed memory: each own local memory, inter-process comm. through network (slow, latency, all network errors)
    * hier passen die "Fallaties of distributed systems" gut rein! siehe [@RGO06]
~