
Concurrent Computation {#ch-concurrency}
===========


~ Epigraph { caption: "Rich Hickey"}
What matters for simplicity is that there's no interleaving.
~


Computation is conceived through the execution of instructions. Activities are called *sequential*, if their list of atomic statements are executed in a sequential manner. Given two or more sequential activities are executed either pseudo-simultaneously (in alternation on a single processing unit) or truly simultaneously (on multiple processing units), they are interleaving and therefore called *concurrent*. Interleaving weakens the total ordering from sequential actions to a merely partial ordering. As a result, concurrency is *nondeterministic*. Repeated invocations on the same input can result in different outputs in general [@Ben90;@Sco06;@Agh93].

The overall requisite for every kind of concurrency is the simultaneous presence of multiple active computational units. Depending on the context in which concurrency is being discussed, different terminologies have been established. On a programming language level, the *thread* is often the concurrent unit. In concurrency theory, the *process* construct is used in general [@Sco06]. However, the term "process" will interfere with other notions of executional units we will discuss in due course. In order to be able to refer to different concepts without aliasing among the terminologies, we will follow the suggestion of Ben-Ari [@Ben90] and denote abstract units of activity as *tasks* through the remainder of this thesis. This designation is a homage to the Ada programming language, where "task" refers to an activity associated with a concurrent unit. The term is well-known and offers a neutral way to refer to every kind of concurrently executed computation within a logical unit.


## Foundational Issues {#sec-foundational-issues}


Many different approaches to concurrency exist. Regardless of the chosen notion, we have always pay attention to three basic concerns [@And83]:

* Expression of concurrent execution
  : Concurrent computation has to be *indicated*. Various abstractions have been proposedin the literature and subsequently implemented for practical use. We will discuss some in due course. In general, all concurrency abstractions are required to provide the possibility to define as well as manage tasks [@Bac03]. Examples are *channels*, *coroutines*, *fork and joins*, *futures* and *threads*. The interfaces for the creation of tasks can be arbitrary, e.g.\ as primitives directly included into a programming language, as libraries or through operating system calls.  

* Communication
  : Interaction and cooperation among tasks has to be possible. *Communication* allows tasks to influence each other [@And83]. The *shared state* communication style is founded on memory (e.g.\ variables, objects, etc.). Several tasks then interact by reading and writing to this state. In contrast, *message passing* communication forgoes any access to shared data, and instead is based on the exchange of messages (fixed, immutable data) sent through communication *links*. The links are additionally required elements. Shared memory is only possible among tasks who can gain access to a common memory section. A shared physical machine is the most basic form to gain access to shared memory between tasks, but the memory can also be simulated on the network. Message passing on the other hand does not even concern the locality or remoteness of memory. Messages can transit numerous links between sender and recipient. Machine boundries are therefore easily bypassed [@Fel90]. Message passing can happen in either *synchronous* (messages are sent and execution is delayed until a response has been received) or *asynchronous* (execution is resumed immediately after sending a message) fashion [@And83].

* Synchronization
  : Although ordering in concurrent execution is merely partial, communication still requires some order constraints. An action must be performed before its effect can be detected. *Synchronization* refers to mechanisms used to ensure such constraints [@And83;@Mey97]. *Semaphores*, *locks* and *transactional memory* are prominent examples. They are mostly discussed regarding the shared state scenario, as the communication requires state to be modified by the sender before it gets read by the receiver. Also, only one task can modify state at a time to avoid unexpected behavior due to low-level data races. The modification or evaluation of shared state occurs within a *critical section*. Synchronization mechanisms realize *mutual exclusion* where no task can access the shared state while another is within its critical section [@Ben90;@Sco06].

  : Message passing on the other hand provides an implicit form of synchronization. Each message intrinsically has to be written and sent before it can be received. As a result, the order is constraint by design [@And83]. Synchronous passing additionally constraints the sender from resuming computation until the message has been answered.  

  : There are two kinds of synchronization mechanisms. *Enforced primitives* guarantee no access to the state outside the primitive, thus ensuring order. *Unenforced primitives* 8grant a certain degree of freedom in their usage and therefore provide no guarantee of mutual exclusion, as do e.g.\ semaphores [@Fel90]. 


## Concurrency, Parallelism and Distribution


So far, we've discussed concurrency as a generic term denoting a simultaneous execution of activities. Yet there are more diverse notions regarding how concurrent execution is implemented. A strict distinction in our terminology is therefore in order. We use *concurrency* to refer to the multiplexing of multiple tasks among one or more processors. No more specific assumptions can be made in general. On a single CPU, interleaving of computation is merely pseudo-simultaneous via time slicing, since all computation is still sequential [@Roe15].

*Parallelism* refers to truly simultaneous execution on different CPUs. If parallel execution is possible depends on the used concurrency abstraction and its implementation [@Bac03]. On a programming language level, referencing components is usually subject to physical limitations regarding the programs memory. For example, objects can only reference objects inside the same programs memory space in general [@Mey97]. We will regard a notion of objects able to surmount this restriction in due course. Though the limitation of memory section does not affect providing decent approaches for writing concurrent code in general, it certainly hinders providing decent constructs for writing parallel code, e.g.\ when using shared state communication. Parallel execution requires code execution on different CPU cores *at the same time*, which usually means distinct process memory boundaries. Inter-component communication has to happen across memory and process boundaries. If a virtual machine (VM) is part of the language used for the executed code, we can charge it to the VM to provide transparent inter-component communication across boundries. For example, the *Java Virtual Machine* (JVM) has different approaches to implement threads. One is to map Java threads to system processes for parallelization [@Hal09]. The JVM hides the resulting gap in memory sections transparently. Writing explicit parallel code, e.g.\ with a *Fork/Join* framework, can be painful and requires us to explicitly prepare code segments and data that can be processed in parallel [@Sco06;@Lea00].

*Distributed computation* is regarded to be an own research discipline separate from parallel computation. However, both concepts are based on the same fundamental idea: Truly concurrent execution (as in *at the same time*) of physically distributed processes [@Agh99]. Agha, FrÃ¸lund and Kim formulated a simple and sound argumentation [@Agh93]: 

> "In a parallel computation some actions overlap in time; by implication these events must be distributed in space."

The argument suggests that parallel tasks are also distributed tasks in a certain sense. The major distinction is that we expect parallel tasks to be physically closer to each other (same CPU) than distributed tasks (distinct CPUs and machines). Due to this distance, distributed tasks cannot share share main memory directly [@Bus90;@Agh99]. Distribution therefore has to rely on message passing communication over the network. Of course, we can use the network to create abstractions for shared memory, so-called *spaces*. One example is the *Linda* model [@Ben90;@Sco06].

Due to this physical separation, tasks are executed on different locations (host machines). We will also refer to these hosts as *nodes* [@Ben90]. A single node can have one or more processors on which one or more tasks are being executed concurrently. In the case of multiple CPUs, tasks can possibly be also executed in parallel.

As a result, there are three fundamental observations to be made:

1. Concurrent systems can be parallel and distributed. However, we cannot assume them to be either parallel or distributed in general.
2. Parallel and distributed systems are inherently concurrent.
3. Distributed systems with two or more nodes are parallel. 

Baeten [@Bae05] gives a general definition incorporating these interrelations:

> "Concurrency theory is the theory of interacting, parallel and/or distributed systems."

In subsequent sections, we will merit attention to two selected task abstractions. Both conceive concurrent computation in general. Additionally, they are also able to provide parallelization and even distribution in a transparent way.


## Correctness Properties


On a fundamental level, the basic purpose of every program is the computation of a result. We expect a sequential program to always conclude the same results for the same inputs[^fn-sequential-side-effects]. The correctness of a program can be verified using theoretical methods, although these methods are not widely adopted in practice. For concurrent programs, this in not generally applicable anymore, due to the intrinsic interleaving of computations [@Ben90].

[^fn-sequential-side-effects]: All sorts of side effects, like IO, are of course also forms of input to a program and have to be stable as well. 

Many different issues regarding concurrent computation are well-known and described in the literature. Examples are deadlocks, livelocks, starvation, race conditions, mutual exclusion and fairness. Due to the high-level view on concurrency in this thesis, we will not immerse into detailed discussions on each of these issues, as is often found in other literature concerned with concurrency concepts. Here, simply two types of properties will be of relevance. Both types have an effect on the correctness of concurrent programs. All the issues given above can be classified in terms of these two property types [@Bus90;@Swa14;@Sin09]:

* Safety
  : asserts the operations that are allowed (safe) to be performed. As a result, given correct inputs will result in correct outputs, while never entering an undesired state. Examples of safety properties are race conditions, deadlock freedom and mutual exclusion. Informally, safety guarantees that "nothing bad will happen".

* Liveness
  : asserts the operations that have to be performed, such that a certain state will be reached eventually (progress). In other words, given correct inputs are guaranteed correct outputs in finite time (cf.\ termination in sequential programs). Examples of liveness properties are fairness, starvation and reliable communication. Informally, liveness guarantees that "something good will happen".

Safety is *invariant*, such that a property `P` holds in *every* state of *every* execution. In contrast, liveness of `P` demands it to hold in *some* state of *every* execution. As a result, there is a so-called *duality* between safety and liveness. The negation of a member of one type is a member of the other [@Ben90].

We've found the role of *deadlocks* (blocking operations which for some reason do not unblock) to be controversial. On the one hand, an execution path must not lead into a deadlock (safety), while they also threaten the progression of a program, thus its liveness. This is also somewhat reflected in safety being related to *partial correctness* (the result is correct if the program terminates), while liveness is to *total correctness* (the programs terminates with a correct result) [@Bus90]. In contrast, so-called *livelocks* (loops never meeting their termination condition) are, as the name suggest, clearly related to liveness. The operations of a livelock are safe while the program does not progress.


## Programming Abstractions {#sec-concurrency-programming-abstractions}


Most programs are concurrent in some way. An example of *implicit concurrency* is *input/output* (IO), where devices are triggered to perform operations simultanious to the executing program [@Bus90]. Also, compilers and interpreters can exploit the concurrency inherent to a language's constructs. On the other hand, *explicit concurrency* has to be indicated. We require appropriate programming abstractions. In general, concurrency abstractions are desired to be powerful and expressive models, fit harmoniously into the programming language in terms of their interface, and exploit the underlying hardware resources efficiently [@Shi97]. 


### Language-Construct Approach {#sec-concurrency-language-level}


Many different approaches to explicitly express concurrent computation on a programming language level have been proposed and used over the decades. A programming language can either provide concurrency constructs by design, or they can be utilized through libraries or frameworks [@Ben90;@Sco06]. Therefore, most concurrent task abstractions are available in most programming languages. As the remainder of this thesis will be in the context of Java and its virtual machine, a brief discussion of Java's basic approach towards concurrency is in order, as alternative abstractions have to build on it.   


#### Case Study: Concurrency in Java


Java is an object-oriented programming language with a C-inspired syntax for the JVM. It offers support for expressing concurrency via threads and basic concepts for handling access to shared resources between threads. Concurrent computation is to be defined through the so-called `Runnable`{language:java} interface. Its default implementation is available in the `Thread`{language:java} class [@Goe06;@Gos15]. The following example illustrates the use for the subsequent discussion:

```{language:java}
class State {
    public int x = 0;
}

final State s = new State();
final Runnable task = () -> {
    synchronized(s) {
        s.x += 1;
        out.println(Thread.currentThread().getName() + " " + s.x);
    }
};

new Thread(task).start();
new Thread(task).start();
```

The offered `synchronized`{language:java} primitive allows us to express mutual exclusion to a shared resource when concurrent modification to the resource is possible. In this case, some state denoted by `s` has to be managed between two threads having both access to `s` through the scope of the defined `Runnable`{language:java}. Note that though `s` is declared `final`{language:java}, its publicly visible member `x` remains mutable.

The mechanism behind Java's synchronization is *locking* on a common reference among all involved threads, the so-called *monitor* object [@Goe06;@Gos15]. When using `synchronized`{language:java} as a block construct, we must provide this monitor as an argument. In our example, the state variable can simply double as the monitor in addition to being the shared resource. Alternatively, we can use `synchronized`{language:java} keyword also as part of the signature of a method in `State`{language:java} holding the logic. Using a respective method signature is equal to defining `synchronized(this)`{language:java} around the whole method's body, where `this`{language:java} refers to the object `s`. The method's object reference then acts as the monitor, just as in our example.

A more modern alternative towards synchronization is offered by the `Lock`{language:java} interface. The previous `synchronized`{language:java} is an enforced synchronization primitive, i.e.\ locks are always exclusive to one thread at a time. In contrast, the various implementations of `Lock`{language:java} can be unenforced. `Lock`{language:java}s can therefore offer options of control on locking, e.g.\ simultaneous access for multiple readers [@Sub11;@Goe06]. To provide this degree of freedom, Java neither detects shared state nor requires its synchronization per se. As a result, programmers can easily introduce data races due to omitted access control. Alternative concurrency abstractions for Java, e.g.\ provided by libraries or frameworks, always have to take this into account.

Expressing concurrency on the programming language level has its perils due to the overlapping of  languages concepts. We've already explained the introduction of mutable state due to scopes or via visibility. Many concepts can have an influence on concurrency considerations. Shared mutability and synchronization especially require utmost care by the programmer for handling access to any data.


### Operating System Approach {#sec-concurrency-os-level}


Operating systems use the *process* as their computational model. A process describes the instantiation of a program image with associated *resources* (e.g.\ memory). Processes are used to express dynamic execution. In the most basic case, these computational units are alternately executed on a single processor. The activation and passivation of processes is called *scheduling* and is in the responsibility of the operating system. Scheduling results in a quasi-parallel execution of active processes. If multiple processors are utilized, the execution is truly parallel [@Bac03;@Sco06]. As a result, we can state that processes are inherently concurrent units due to their execution modality.

In contrast, *threads* are tasks *inside* a process. One process can have numerous threads which all share the same memory space [@Tan07]. Since threads can access the same memory locations, we are free to introduce shared state among them. In the Java case study we already demonstrated the synchronization required for shared state. In contrast, the memory boundaries between processes however are strict and enforced by the OS. Communication between two processes requires either an explicit arrangement of so-called *shared memory*, or another means of message passing communication subsumed as *inter-process communication*[^fn-ipc] (IPC) [@Bac03;@Lov07]. Since we will regard an extended focus on operating system conceived concurrent tasks relying on IPC in due course, a consolidating example for future reference is in order.

[^fn-ipc]: In concurrency theory, *process* is also the general term for a concurrent unit, as we've mentioned. Therefore, often all communication between concurrent units is subsumed as *inter-process communication* in the literature. To avoid confusion, we will use this designation only for communication mechanisms between OS processes.


#### Case Study: Concurrent Processes in C {#sec-concurrent-c}


Only with C11[^fn-c11] did the programming language add native support for expressing concurrency via threads. Prior to this, more operating system depending approaches like the POSIX[^fn-posix] threads binding `pthreads` had to be used. An additional strategy was to compose concurrent computation in an ad hoc way by relying on the operating systems scheduler to assign processes to processors in a concurrent way [@Bac03; @Fel90]. 

[^fn-c11]: The C standard revision of 2011, specifically *ISO/IEC 9899:2011*. It succeeds C99, the 1999 revision.
[^fn-posix]: __P__ortable __O__perating __S__ystem __I__nterface, a collection of standardized programming interfaces for operating systems. The **X** is a remnant from the original draft name "IEEE-IX".

An operating system allows a process to spawn new processes. The party referred to as the *parent* uses system calls provided by the OS to spawn a new process called the *child*. For example, the `exec`-family of Unix calls allows us to instantiate arbitrary processes from executables referenced by a filesystem path. However the spawned child replaces the parent process. `exec`-calls alone are therefore insufficient to compose concurrency [@Lov07]. 

The expedient path is the alternative `fork`-call. It replicates the current processes address space into a separate address space of a newly spawned child [@Bac03; @Fel90]. The following example illustrates the control flow:

```{language:cpp}
void parentBehavior(int fd[]);
void childBehavior(int fd[]);

int main(void) 
{
    int fd[2];
    pipe(fd);

    pid_t pid = fork();

    if (pid == 0) 
        childBehavior(fd);
    else
        parentBehavior(fd);
}
```

Both processes are based on the same program image. The parent receives the *process identifier* (PID) of the newly spawned process as a result of `fork()`. The child does not receive its own PID information. We can therefore use the PID to distinguish the further program flow of both processes, effectively allowing two separate behaviors. Consecutive forks are of course possible. 


By using additional Unix system calls, we can set up a so-called *pipe* as IPC between each two processes. Pipes are a form of byte stream across the memory boundaries of the respective processes [@Bac03]. The endpoints of the pipe are symbolized by two file descriptors, since Unix is following an *everything is a file* design principle. This principle makes interfaces simple and consistent [@Shi97]. The first descriptor `fd[0]`{language:cpp} is opened in read mode and the second `fd[1]`{language:cpp} in write mode. For example, the `parentBehavior` can write data to `fd[1]`{language:cpp} and the `childBehavior` will subsequently be able to read this data from `fd[0]`{language:cpp}. Therefore, that the data crosses memory boundaries. Pipes are a possible communication link for message passing that tasks can utilize to avoid the critical region problem.


### Network Approach {#sec-concurrency-network-level}


As we've outlined, distribution is another approach to the conception of concurrent execution within a system. Generally, besides the lack of shared memory, the distinguishing characteristic between parallel and distributed computing is the geographical distance between tasks. Therefore, the communication between distributed tasks has to happen via networked message passing mechanisms. Networks introduce a wide range of perils, as the communication links can neither be assumed reliable nor static. Also, messages are more costly in terms of time (latency) and effort (e.g.\ due to data serialization) [@Agh99]. The famous *Fallacies of Distributed Computing* by Deutsch subsume many of the problematic aspects [@Tan07]:

* Fallacy 1: *the network is reliable*.
* Fallacy 2: *latency is zero*.
* Fallacy 3: *bandwidth is infinite*.
* Fallacy 4: *the network is secure*.
* Fallacy 5: *topology doesn't change*.
* Fallacy 6: *there is one administrator*.
* Fallacy 7: *transport cost is zero*.
* Fallacy 8: *the network is homogeneous*[^fn-8th-fallacy].
{ list-style-type:none }

Fallacies 4.\ and 6.\ are outside the scope of this thesis, but the remaining aspects will be relevant to some concepts already or soon to be discussed. For example, Fallacy 2.\ is regarding the synchrony of communication. Asynchronous messaging is not concerned with any latencies delaying travel time of messages. Synchronous communication is constrained by time and therefore affected by network induced latencies.

[^fn-8th-fallacy]: Fallacy 8.\ was not part of Deutsch's original proposal, but later added by Gosling. Hence, the literature sometimes refers to merely seven fallacies. 

Distribution conceived concurrency is affected by the Fallacies in general. Network programming interfaces are depending on the used network mechanism. A very basic example is the concept of *sockets* that Unix introduced. The standard interface of sockets provides a means to write data that will be sent through the network. Alternatively, we can use a socket toread received data from the network [@Tan07;@Bac03]. Sockets are provided by practically every operating system and bindings exist for almost all programming languages. Therefore, sockets are a technology heterogenous mechanism, although rather low-level (transport layer). More high-level is for example HTTP (__H__yper__t__ext __T__ransfer __P__rotocol), a generic, originally stateless and text-based communication protocol providing platform-neutral data transfer on the application-level [@Cou05].