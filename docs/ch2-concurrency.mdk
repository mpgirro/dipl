
Concurrency {#ch-concurrency}
===========


~ Epigraph { caption: "Rick Hickey"}
What matters for simplicity is that there's no interleaving.
~


Computation is conceived through the execution of instructions. Such activities are called *sequential*, if their list of atomic statements are executed in a sequential manner. Given two or more sequential activities are executed either pseudo-simultaniously (in alternation on one single processing unit) or truely simultaneously (on multiple processing units), they are interleaving and thus called *concurrent*. This interleaving weakens the total ordering from sequential actions to a merely partial ordering, thus concurrency is *nondeterministic*, such that multiple invocations on the same input may result in different outputs in general [@Ben90;@Bus90;@Agh93].

The overall requisite for any kind of concurrency is therefore the simultaneous presence of multiple active computational units. Depending on the context in which concurrency is being discussed, different terminologies are established. On a programming language level, usually *thread* is used for a concurrent unit, while in concurrency theory the *process* construct is used. However, the later will interfere with other notions of executional units discussed in due course. In order to be able to refere to different concepts without any aliasing among the terminologies, we will follow the suggestion of Ben-Ari [@Ben90] and denote abstract units of activity as *tasks* through the remainder of this thesis, as a homage to the Ada programming language where "task" referes to an activity associated with a concurrent unit. The term is well known and offers a neutral way to refere to any kind of concurrently executed computation within a logical unit.


## Foundational Issues {#sec-foundational-issues}


Many different approaches to concurrency exist. Regardless of the chosen notion, there are always three basic concerns that have to be paid attention to [@And83]:

* Expression of concurrent execution

  : Concurrent computation has to be *indicated*. Various abstractions have been proposed, and some of them will be discussed in due course. All of them are required to provide the possibility to define as well as manage task units [@Bac03]. Examples are channels, coroutines, fork and joins, futures and threads. [FÄLLT MIR HIER NOCH EIN SATZ EIN? ODER ZWEI?]{.red}

* Communication
  : Interaction and coorporation among tasks has to be possible. *Communication* allows them to influence each other [@And83]. The *shared state* communication style is founded on memory (e.g. variables, objects, etc.) being accessible by multiple tasks, such that they interact by reading and writing to this state. In contrast, *message passing* communication forgos any access to shared data, and instead is based on the exchange of messages (fixed, immutable data) sent through communication *links*, which are additionally required. Shared memory is only possible among tasks who can gain access to a common memory sections, thus is limited to the same physical machine. Message passing on the other hand surmounts this limitation, as messages can transit multiple links between sender and recipient, such that the machine boundries are bypassed [@Fel90]. It can happen in either *synchronous* (messages are sent and execution is delayed until a response has been received) or *asynchronous* (execution is resumed immediately after sending a message) fashion [@And83].

* Synchronization
  : Although ordering in concurrent execution is merely partial, communication still requires some order constraints, i.e. an action must be performed before its effect can be detected. *Synchronization* referes to mechanisms used to ensure such constraints [@And83]. Semaphores, locks and transactional memory are prominent examples. They are mostly discussed regarding the shared state scenario, as the communication requires state to be modified by the sender before it gets read by the receiver. Also, only one task may modify state at a time to avoid unexpected behavior due to low level data races. The modification or evaluation of shared state is thus called the *critical section*, and synchronization mechanisms realize *mutual exclusion*, such that no task can access the shared state while another is within it's critical section [@Ben90;@Bus90].

  : Message passing on the other hand provides an implicit form of synchronization, as any message intrinsically has to be written and sent before it can be received, thus the order is constraint by design [@And83]. Synchronous passing additionally constraints the sender from resuming computation until the message has been answered.  

  : There are two kinds of synchronization mechanisms. *Enforced primitives* guarantee no access to the state outside the primitive, thus ensuring order. *Unenfored primitives*, on the other hand, grant a certain degree of freedom in their usage and therefore provide no guarantee of mutual exlcusion, as do e.g. semaphores [@Fel90]. 


## Concurrency, Parallelism and Distribution


So far, concurrency has been discussed as a generic term denoting a simultanious execution of activities. Yet there are more diverse notions regarding how concurrent execution is implemented. Strict distinction in terminology is therefore in order. *Concurrency* referes to the multiplexing of multiple tasks among one or more processors. No more specific assumptions can be made in general. On a single CPU, such interleaving of computation is merely pseudo-simultanious achieved via time slicing, as all computation is still sequential [@Roe15].

*Parallelism* referes to truely simultaneous execution on different CPUs. If such parallel execution is possible depends on the used concurrency abstraction and its implementation [@Bac03]. On a programming language level, referencing components is usually subject to physical limitations regarding the programs memory, i.e. objects can only reference objects inside the same programs memory space [citation needed]{.mind}. Though this does not affect providing decent approaches for writing concurrent code in general, it certainly hinders providing decent constructs for writing parallel code, e.g. when using shared state communication. Parallel execution requires code execution on different CPU cores *at the same time*, which usually means distinct process memory boundries. Inter-component communication has to happen across these boundries. It can be charged to the virtual machine used for executing the code if such is part of the language concept, as is done in Java where the JVM maps threads to system proccesses for parallelization [@Hal09]. But writing explicit parallel code, e.g. with a *Fork/Join* framework, can be painful and requires to explicitly prepare code segments that are ment to be and data that can be proccessed in parallel [@Lea00].

Distributed computation is regarded its own research discipline separate from parallel computation. However, both concepts are based on the same fundamental idea: Truely concurrent execution (as in *at the same time*) of physically distributed processes [@Agh99]. Agha, Frølund and Kim formulated a simple and sound argumentation [@Agh93]: 

> "In a parallel computation some actions overlap in time; by implication these events must be distributed in space."

This suggest that parallel tasks are also distributed tasks in a certain sense. The major distinction is that parallel tasks are expected to be "closer" to each other (same CPU) than distributed tasks are (distinct CPUs), which cannot share main memory due to this distance [@Bus90;@Agh99]. Distribution therefore has to rely on message passing communication over the network. Of course, such can be used to create network abstractions for shared memory, so-called *spaces*, e.g. by using the *Linda* model [@Ben90].

Due to this physical separation, different locations, that is host machines, do exist where tasks are being executed. In general, these will be refered to as *nodes* [@Ben90]. A single node may have one or more processors on which one or more tasks are being executed concurrently in general, and, in the case of multiple CPUs, possibly also parallel.

As a result, there are three fundamental observations to be made:

1. Concurrent systems can be neither assumed parallel nor distributed in general, but may be so their individual case.
2. Parallel and distributed systems are always concurrent.
3. Distributed systems with two or more nodes are parallel. 

A general definition incorporating these interrelations is given by Baeten [@Bae05]:

> "Concurrency theory is the theory of interacting, parallel and/or distributed systems."

Subsequent chapters will merit attention to abstractions not only able of the conception of concurrent execution in general, but also providing parallel as well as distributed capabilities in a transparent way. 


## Correctness Properties


On a fundamental level, the basic purpose of any program is the computation of a result. A sequential program should always conclude the same results for the same inputs[^fn-sequential-side-effects], and thus it's correctness can be verified using theoretical methods, although these are not widely adoped in practice. For concurrent programs, this in not generally applicable anymore, due to the intrinsic interleaving of computations [@Ben90].

[^fn-sequential-side-effects]: Any form of side effects, like IO, are of course also forms of input to a program, thus they have to be stable as well. 

Many different issues regarding concurrent computation are well known and described in the literature. Examples are deadlocks, livelocks, starvation, race conditions, mutual exclusion and fairness. Due to the high-level view on concurrency in this thesis, we will not immerse into detailed discussions on these, as is often found in other literature concerned with concurrency concepts. Here, simply two types of properties will be of relevance, which have an effect on the correctness of concurrent programs. All the issues given above can be classified in terms of these two property types:

* Safety
  : asserts the operations that are allowed (safe) to be performed [@Bus90]. As a result, given correct inputs will result in correct outputs [@Swa14]. Examples of safety properties are race conditions, deadlock freedom and mutual exclusion.

* Liveness
  : asserts the operations that have to be performed, such that a certain state will be reached eventually (progress) [@Bus90]. In other words, given correct inputs are guaranteed correct outputs in finite time (cf. termination in sequential programs) [@Swa14]. Examples of liveness properties are fairness, starvation and reliable communication.

Safety is *invariant*, such that a property `P` holds in *every* state of *every* execution. In contrast, liveness of `P` demands it to hold in *some* state of *every* execution. As a result, there is a so-called *duality* between safety and liveness, such that the negation of a member of one type is a member of the other [@Ben90].

We've found the role of *deadlocks* (blocking operations which for some reason do not unblock) to be controversial. One the one hand, an execution path must not lead into a deadlock (safety), while they also threaten the progression of a program, thus its liveness. This is also somewhat reflected in safety being related to partial correctness, while liveness is to total correctness [@Bus90]. In contrast, so-called *livelocks* (loops never meeting their termination condition) are, as the name suggest, clearly related to liveness, for their operations are safe while the program does not progress.


## Programming Abstractions


~ Todo
1-2 kurze Sätze wieso es hier 3 Unterkapitel gibt. 
~


### Language-Construct Approach {#sec-concurrency-language-level}


Many different approaches to explicitely express concurrent computation on a programming language level have been proposed and used over the decades. Such are regarded with the indication issue of concurrency and therefore relate to the examples given in [#sec-foundational-issues]. If not part of a language by design, they may generally be utilized via libraries or frameworks [@Ben90;@Fel90], such that in some way or another most concurrent task abstractions are available in most programming languages. As the remainder of this thesis will be in the context of Java and its virtual machine, a brief discussion of its basic approach towards concurrency is in order, as alternative abstractions have to build on it.   


#### Case Study: Concurrency in Java


Java is an object-oriented programming language with a C-inspired syntax for the so-called *Java Virtual Machine* (JVM). It offers support for expressing concurrency via threads and basic concepts for handling access to any shared resources between such. Concurrent computation is to be defined through the so-called `Runnable`{language:java} interface. Its default implementation is available as the `Thread`{language:java} class [@Goe06]. The following example illustrates the usage for the subsequent discussion:

```{language:java}
class State {
    public int x = 0;
}

final State s = new State();

final Runnable task = () -> {
    synchronized(s) {
        s.x += 1;
        out.println(Thread.currentThread().getName() + " " + s.x);
    }
};

new Thread(task).start();
new Thread(task).start();
```

The offered `synchronized`{language:java} primitive is used to express mutual exlusion to a shared resource when concurrent modification to it is possible. In this case, some state denoted by `s` has to be managed between two threads having both access to it through the scope of the defined `Runnable`{language:java}. Note that though `s` is declared `final`{language:java}, its publicly visible member `x` remains mutable.

The mechanism behind Java's synchronization is *locking* on a common reference among all involved threads, the so-called *monitor* object [@Goe06]. When using `synchronized`{language:java} as a block construct, this monitor has to be provided as an argument. In this case, the state variable can simply double as the monitor in addition to being the shared resource. Alternatively, the `synchronized`{language:java} keyword could also have been used as part of the signature of a method in `State`{language:java} holding the logic. Such signature usage is equal to defining `synchronized(this)`{language:java} around the whole method's body, where `this`{language:java} would then refere the object `s`. Thus the method's object reference acts as the monitor, just like in the example.

A more modern alternative towards synchronization is offered by the `Lock`{language:java} interface. In contrast to `synchronized`{language:java}, which is an enforced synchronization primitive, i.e. locks are always exclusive to one thread at a time, the various implementations of `Lock`{language:java} may be unenforced, such that they may offer options of control on locking, e.g. simultanious access for multiple readers [@Sub11;@Goe06]. To provide this degree of freedom, Java neither detects shared state nor requires its synchronization per se, thus programmers may easily introduce data races due to omitted access control. Any alternative concurrency abstraction for Java, e.g. through a library or framework, has to take this into account.

Expressing concurrency on the programming language level has its perils due to the overlapings of  languages concepts, e.g. the introduction of mutable state due to scopes or via visibility. Many concepts might have an influence on concurrency considerations. Shared mutability and synchronization especially require utmost care by the programmer for handling access to any data.


### Operating System Approach {#sec-concurrency-os-level}


The *process* is a concept for the computational model used by operating systems. It describes the instantiation of a program image, which has associated *resources* such as memory. Processes are used to express dynamic execution. In the most basic case, for which they have been historically conceived, these computational units are alternately executed on a single processor. This activation and passivation is called *scheduling* and the responsibility of the operating system. It results in a quasi-parallel execution of active processes. If multiple processors are utilized, it is truely parallel [@Bac03;@Fel90]. Thus, processes are inherently concurrent units due to their execution modality.

In contrast, *threads* are tasks *inside* a processes, such that one process can have multiple threads which share its memory space [@Tan07]. Therefore, threads can easily access the same memory location (shared state), which requires synchronization, as has been demonstrated in the Java case study. The memory boundries between processes however are strict and enforced by the OS. Communication between any two processes therefore requires either an explicit arrangement of so-called *shared memory* between them, or another means of message passing communication subsumed as *inter-process communication*[^fn-ipc] (IPC) [@Bac03;@Lov07]. As operating system conceived concurrent tasks relying on IPC will be regarded with extended focus in due course, a consolidating example for future reference is in order.

[^fn-ipc]: In concurrency theory, *process* is also the general term for a concurrent unit, as has been mentioned. Thus, often all communication between concurrent units is subsumed as *inter-process communication* in the literature. To avoid confusion, we will use this designation only for message passing communication mechanisms between OS processes.


#### Case Study: Concurrent Processes in C {#sec-concurrent-c}


Only with C11[^fn-c11] did the C programming language add native support for expressing concurrency via threads. Prior to this, more operating system depending approaches like the POSIX[^fn-posix] threads binding `pthreads` had to be utilized. An additional strategy was to compose concurrent computation in an ad hoc way be relying on the operating systems scheduler to assign proccesses to processors in a concurrent way [@Bac03; @Fel90]. 

[^fn-c11]: The C standard revision of 2011, more specifically *ISO/IEC 9899:2011*. It succeeds C99, the 1999 revision.
[^fn-posix]: __P__ortable __O__perating __S__ystem __I__nterface, a collection of standardized programming interfaces for operating systems. The **X** is a remnant from the original draft name "IEEE-IX".

An operating system allows a process to spawn new processes. The party refered to as the *parent* uses system calls provided by the OS to spawn a new process called the *child*. For example, the `exec` family of Unix calls allows to instantiate arbitrary proccesses from executables referenced by a filesystem path. However the spawned child replaces the parent process, thus such calls alone are insufficient to compose concurrency [@Lov07]. 

The expedient path is the alternative `fork()` call. Such replicates the current processes address space into a spearate address space of a newly spawned child [@Bac03; @Fel90]. The following example illustrates the control flow:

```{language:cpp}
void parentBehavior(int fd[]);
void childBehavior(int fd[]);

int main(void) 
{
    int fd[2];
    pipe(fd);

    pid_t pid = fork();

    if (pid == 0) 
        childBehavior(fd);
    else
        parentBehavior(fd);
}
```

Both processes are based on the same program image. The parent receives the *Process Identifier* (PID) of the newly spawned process as a result of `fork()` while a child gets no information. This can be used to destinct the further program flow of both processes, effectivly allowing two separate behaviors. Consecutive forks are of course possible. 

A so-called *pipe* can be set up as IPC between any such two processes by utilizing additional Unix system calls. They are a form of byte stream across the memory boundries of the respective processes [@Bac03]. It's endpoints are symbolized by two file descriptors, the first `fd[0]`{language:cpp} opened in read mode, and the second `fd[1]`{language:cpp} in write mode. For example, the `parentBehavior` could write data to `fd[1]`{language:cpp}, and `childBehavior` would then be able to it from `fd[0]`{language:cpp}, thus the data crosses memory boundries. Pipes are a communication link for message passing, and therefore tasks can utilize it to avoid the critical region problem.


### Network Approach {#sec-concurrency-network-level}


As has been outlined, distribution is another approach to the conception of concurrent execution within a system. Generally, besides the lack of shared memory, the distinguishing characteristic between parallel and distributed computing is the geographical distance between tasks. Therefore, the communication between distributed tasks has to happen via networked message passing mechanisms. Such introduce a wide range of perils, as the communication links can neither be assumed reliable nor static, and messages are more costly in terms of time (latency) and effort (e.g. due to data serialization) [@Agh99]. The famous *Fallacies of Distributed Computing* by Deutsch subsume many of the problematic aspects [@Tan07]:

1. The network is reliable.
2. Latency is zero.
3. Bandwidth is infinite.
4. The network is secure.
5. Topology doesn't change.
6. There is one administrator.
7. Transport cost is zero.
8. The network is homogeneous[^fn-8th-fallacy].

Fallacies 4. and 6. are outside the scope of this thesis, but the remaining will be relevant to some concepts already or soon to be discussed. For example, fallacy 2. is regarding the synchrony of communication. While asynchronous messaging is not concerned with any latencies delaying travel time of messages, synchronous communication is by definition constraining time and thus affected by network induced latencies.

[^fn-8th-fallacy]: The eighth fallacy was not part of Deutsch's original proposal, but later added by Gosling. Hence, the literature sometimes merely referes to seven fallacies. 

Any distribution conceived concurrency has to concern itself with them when dealing with the network programming interfaces. These are depending on the used network mechanism, and can take arbitrary form in general. A very basic example is the concept of *Sockets* as they have been introduced in Unix. Their standard interface provides a means to write data that will be sent through the network, and from which received data from the network can be read [@Tan07]. Socket bindings exist for practically every programming language, such that they are a technology heterogenous mechanism, although rather low-level.

More high-level is HTTP (__H__yper__t__ext __T__ransfer __P__rotocol), which provides [...TODO 1-2 sätze erklärung, dass HTTP weniger bare metal ist, weil structural text --> dann ist schluss mit dem kapitel]{.red}
