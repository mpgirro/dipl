
Implementation of a Concurrent System {#ch-scenario}
============================



~ Epigraph { caption: "Eric S. Raymond"}
Every good work of software starts by scratching a developer's personal itch.
~


~todo
Einführungs Blabla was hier in den Unterkapitel alles kommt.
~

In this chapter we cover the practical aspects of programming with actors and microservices. Section [#sec-scenario] describes a scenario for a concurrent system. Section [#ch-actor-impl] covers the strategies/approaches to implement this scenario using the actor model. Subsequently Section [#ch-microservice-impl] describes the implementation approach for the same scenario using the microservice model.


## A Concurrent System Scenario {#sec-scenario}

~ lit 
* Introduction to Information Retrieval <https://nlp.stanford.edu/IR-book/>
~


In this section, we outline a domain-specific search engine we call *Echo*[^fn-echo-name]. The search engine serves as an illustration of a concurrent system and provides us a baseline for evaluating programming with actors and microservices for concurrent execution.



relevant concerns and issues regarding the conception of concurrency using actors and microservices. Subsequent Chapters [#ch-actor-impl] and [#ch-microservice-impl] will address the respective implementations.

[^fn-echo-name]: The name "Echo" was chosen for its wonderful characteristics of providing a short package name and the analogy to *recalling spoken words*.  

... We will use Echo for evaluating the conceptial differences of programming with actors and microservices. Subsequent Chapters will address.... . Furthermore, the Echo will serve us as the benchmark to evaluate the efficiency of actor- and microservice-based programming. 


...

Search engines cause the illusion of being monolithic systems, but are actually a composition of rather loosely coupled subsystems [@Cou05].

~ todo
Hier fehlen noch ein paar verbindende Sätze
~

Search engine construction is generally led by two basic requirements [@Man08]:

* Effectiveness
  : is about the quality of search results, and the sole concern of the scientific discipline called *information retrieval* (IR). *Precision* and *recall* are distinguished by IR as the two aspects affecting effectiveness. The optimization of effectiveness is not within the scope of this thesis. We therefore merely apply a basic scoring method provided by the utilized information retrieval library for determining search results.

* Efficiency
  : is about response time and throughput, therefore highly affected by the concurrent processing capabilities of the system. Efficiency is not only a concern of the retrieval phase. The data aggregation phase requires efficiency to increase the up-to-dateness of search results. 

### Domain Description

~ todo
kurze Beschreibung der technischen Realisation von Podcasts = Decentralized media syndication via RSS/Atom feeds, sodass daraus ersichtlich wird welche Aufgaben eine entsprechende Suchmaschine (= Datenaggregierungs- und Abruf-System) erfüllen muss um ihre Dienstleistung anbieten zu können.
~

We build our domain specific search engine for the *Podcast* domain. On the one hand, the term is used to refere to an episodic series of digital media. The media is usually audio, sometimes also video.  On the other hand, Podcast can also refer to the distribution mechanism. The distribution builds upon XML-based web feeds. RSS [@Win03] (__R__ich __S__ite __S__ummary) or Atom [@RFC4287] are the established formats. Since RSS has always been the more widely format, we keep referring to *RSS feeds* from here on. Both formats gained popularity in the 2000s as an effective, decentralized mechanism to publish the updates of websites. Podcasts build upon the same principle, but utilize an otherwise optional field for items of an RSS feed, the so-called `<enclosure>` tag. This tag is intended to provide an URL (__U__niform __R__esource __L__ocator) to the media file. Subscribers to the feed can download the file behind the URL and listen/watch the media, usually through a specialized client application software. The `<enclosure>` is therefore the main content of an RSS item [@Pat06]. Additionally, there are other fields within an RSS feed. Some of these feeds contain human readable information about the linked media file, so-called metadata.

Our search engine is designed to regularly analyze RSS feeds for podcasts. The metadata allows us to add information for every media file to a so-called *search index*. Although we do not analyze the media itself, we can still provide search functionality. The domain is very suitable for concurrent processing, since the RSS feed files are decentralized. Every Podcast content creator is publishing a separate feed. There is no interrelation between feeds. We can therefore not only process each feed separately, but also concurrently.


### System Components


At the core, the basic architecture and components are inspired by the work of Brin & Page [@Brin98] on large scale web search engine "anatomy" and a more modern interpretation of associated design principles given by Coulouris *et al.*\ [@Cou05].  

...

We provide both backends with a web interface that they have to support. The Web application is written with Angular [@GoogleAngular], so the backends have to provide conform REST interfaces to allow interaction from the outside. The Web application serves as a proof of concept for intended functionality of the backend implementations, as well as a [meerschweinchen]{.important} for finding problems during development.

In order to prevent any domain-specific code from being implemented twice unnecessarily, we supply both backends with the Core library offering functionality that is required by either backend. For example, the actual searching is done through a specialized data structure, the so-called *reverse index*. We use Lucene [@ApacheLucene] to create this structures. Lucene offers a Java interface that is interoperable with most JVM-based programming languages. RSS/Atom feed parsing is done using ROME [@ROME], enriched by an extension we wrote to support extracting *Simple Chapter* [@PotloveSimpleChapters] information.

...

* CatalogStores (C)
  : hold a catalog of all podcasts, with respective feeds, episodes and chapter marks thereof. Such information is persisted in relational databases based on a simple, straightforward domain model. CatalogStores hold the complete amount of information about any cataloged entity

* IndexStores (I)
  : hold the data structure that is used for searching, the so-called *reverse index*. Registered information entities are called *documents*, relating to one podcast or episode in our case. A reverse index maps the previous *document-term* relationship into a *term-document* structure. IndexStores only have relevant information that are needed to match the documents to search queries. Useful textual information is e.g.\ title, subtitle, multiple general information tags (`<description>` or `<content:encoded>`), chapter marks that are found within feeds, as well as transcripts and the HTML source of linked websites. 

* Web Crawlers (W)
  : aquire the information stores by the search engine, hence they download data from URLs. These URLs can relate to feeds, general websites, or APIs of other existing directories which can be used to discover new feeds. A major concern of web crawlers in general is robustness against the perils of the web [@Man08].

* Parsers (P)
  : transform the acquired information into internal representation formats by analyzing feed XML data. The extracted data is what we take into account when running search queries and subsequently display in the Web application. An important requirement on parsers is robustness regarding flawed information, because all considered data is encoded in a markup language, viz.\ XML or HTML.

* Searchers (S)
  : are tasked with performing search operations. However, they merely apply some basic query pre-processing themselves and delegate the retrieval of relevant documents to an IndexStore using its inverted index. The respective results are communicated back via the Searcher to the Gateway.

* Gateway (G)
  : provides the link for the web interface to the system internal capabilities. It exposes a REST interface to the outside and uses respective mechanisms to interact with other internal system components. The REST interface allows us to request cataloged information or perform searches. Therefore, the Gateway communicates with either a CatalogStore, or a Searcher. 

* Updater (U)
  : determines which feeds to re-download next in order to register new episodes, and update existing metadata, and discovery new feed from existing directories, on a regular basis. To ensure consistency, only one unit must exist within the system (singleton). 

~ Figure { #fig-task-units; caption: "Task unit interaction model"; width:45%; float:left; margin-right: 1ex; }
![img-task-units]
~

[img-task-units]: graphics/interaction-model.[svg,png] "Image about task units and message flow" { width: 70%; vertical-align:middle; padding-bottom:1em; }

Task units ending in *\*Store* are stateful, all others stateless. When dataflow examples are discussed involving arbitrary components `X` and `Y`, that will be substituted with the component abbreviations (`C`, `I`, `W`, `P`, `S`, `G`, `U`) in due course, the simplistic notation used is: `X` &rarr; `Y` is expressing `X` sending a message to `Y` (push), while `X` &larr; `Y` denotes `X` fetching a message from `Y` (pull). `X` &rightleftarrows; `Y` is short for `X` sending a request message to `Y` which in turn replies with a response (synchronous call, RPC).

...

~ green

... means of generating globally unique IDs without requiring any kind of consensus mechanism

...

~

### Concurrent Concerns and Message Flow


In general, all previously described components have to be executed concurrently. However, from a more general perspective, there are certain concerns that have to be met in a concurrent fashion, in order to ensure a "running" search engine. These involve multiple task units, and therefore these have to corporate in terms of an information flow between them.

Major work flows

* Processing feeds
  : Feeds are either processed when they are initially indexed, or updated to check for new episodes (and updated metadata). Thus processing can be triggered by adding a yet unknown feed, or on demand by the Updater. Such find new updating candidates by regularly consolidating the data of Catalogs. Feeds are then fetched by the Crawlers, and the received data is passed on to Parsers. Those extract podcast and episode data from the XML. The podcast data can be used to update (or first time set) the metadata in all CatalogStores. Each episode metadata record is used to check whether an episode is yet known, or if the feed contained a new entry. This can only be detected by a CatalogStore (and its record of all metadata). New episodes metadata is sent to the IndexStores, where it added to the Lucene index data structures. The overall flow is thus; `U` &rarr; `C` &rarr; `W` &rarr; `P` &rarr; `C` &rarr; `I`

* Searching
  : The essential purpose of the engine is search. The web UI offers an interface similar to well-known search providers on the world wide web. Search requests are registered on the REST interface and forwarded to a Searcher (`G` &rarr; `S`), who is doing some basic query processing and then realizes a concrete query to the IndexStore (`S` &rarr; `I`). Its found results are then propagated back via the Searcher (`I` &rarr; `S`) and the Gateway (`S` &rarr; `G`) to the web UI. This has to happen in a timely manner. The complete synchronous flow can be described as `G` &rightleftarrows; `S` &rightleftarrows; `I`. 

Minor work flows:

* Exploring the Catalog
  : The search engine provides besides searching index data records also a database of all known podcast and episode metadata. The web UI offers basic support for exploring such data, e.g.\ by viewing info pages for podcasts or episodes. Thus, REST requests have to be forwarded from the Gateway to the CatalogStore (`G` &rightleftarrows; `C`) in a timely fashion.

* Adding and updating feeds
  : Feeds can be proposed so that the engine will check whether it is already known and issue an indexation if not. Proposing can either be done manually by a user, or as a result of crawled data from existing directories. Thus the Updater issues regular crawling for supported Podcast directories.



...


For example, the initial fetching or updating of a feed, and subsequent processing of the feed's data involves nearly all system components. Updating is a regular task. In a large enough database of feeds, we can expect that nearly all the time there is an update in progress. These update tasks however should not interfere or limit other regular tasks, especially those with time constraints. All user interactions is constraint by time, therefore [ACHTUNG: hier habe ich den satz verändet, daher liest sich das jetzt so als würde sich die sync. und trans. negation of die time constraint tasks beziehen]{.red} synchronization or transactions are generally an undesirable and less expedient path  [@Cou05].


### Scope and Limitations

~ todo
Hier beschreiben was die Implementierungen leisten können sollen, und was nicht
~

~ green
* static configuration of components
  * KEINE elasticity
  * KEINE mobility at runtime 
  * keine notwendigkeit von event sourcing
~

~ todo
Hier könnte eine hübsche +/- Tabelle stehen
~