## Benchmark

~ todo
Ich soll Zahlen produzieren, also mus sich Dinge "messen". Also bitte...
~

We've compared the expressiveness and capabilities of actors and microservices and demonstrated that both can meet similar concerns. Now we are interested in the efficiency of each programming model. The Echo system implementations provide us with the foundation to conduct benchmarks. The results will allow us to draw conclusions about the underlying concurrency concepts. Therefore, we require some metrics to benchmark. As we've already mentioned, information retrieval traditionally uses the metrics of precision and recall to evaluate search engines. However, precision and recall assess the effectiveness of the retrieval techniques. IR effectiveness is not withing the scope of this thesis. Instead, we want to measure the efficiency in terms of system performance. The common metrics we need to gather must be viable to both programming models in order to be comparable.

*Savina* is a benchmark suit specifically designed for evaluating actor libraries [@Ima14]. Akka-specific profiling studies have used Savina to gather detailed metrics for Akka [@Ros16a;@Ros16b]. However, the benchmark suit as well as the profiling tools are actor model and Akka specific. Hence, the metrics provided by the profiling is tailored to the actor model. 

There are few benchmark applications for microservice architectures available as of yet ....


...

Recent works report that microservice benchmarks are yet at an early stafe. There is yet a lack of benchmark systems as well as commonly established evaluation metrics. [REF?!]{.red}

Recent works report that there is no widely established benchmark suit available for microservices archiectures [@Zho18] --> [STEHT DAS WIRKLICH SO HIER> DRINNEN?!]{.red}

To the authors knowledge, there is no widely established benchmark suit available for microservice architectures available as of yet. Due to the technological diversity, there is also no general microservice profiling tool available. 

Hence, we cannot refere to the literature for common metrics to use for evaluation. 

... General performance evaluation uses...

... There is one project on GitHub that benchmarks latency, throughput, and transfered data of popular microservices frameworks[^fn-msa-framework-benchmark], including Spring Boot and Akka HTTP. The benchmark experiment setup is rather simple in that the frameworks are merely used to serve "*Hellow World!*" on a REST interface as the workload. Though the experiment is simple, we use the general design idea and measure latency and throughput for evaluation. Since Akka does not necessarily serialize data [REF]{.red}, we refrain from gathering the transfered bytes as an additional metric. 

[^fn-msa-framework-benchmark]: <https://github.com/networknt/microservices-framework-benchmark>

Our search engine has two essential subsystems: the indexing subsystem and the retrieval subsystem. We evaluate both subsystems separately. The workloads of each subsystem consist of jobs. 


As the performance metric of each subsystem we use the *latency* $L$ of the subsystem to complete a job. For the indexing subsystem, a job is the complete processing of a given feed. For the retrieval subsystem, a job is the retrieval of relevant documents for a given query.

If we know $L$, we can calculate the *throughput* $T$ of the subsystem simply with $T = 1 / L$. 

~ todo
images of the subsystem interaction models
~

All task units are based on the JVM and use the same `core` library for domain specific functionality. Platform and domain specific effects on latency and throughput are therefore uniform. All differences of latency and throughput between the two system implementations are a result of the unterlying programming models for communication and coordination [CITATION?]{.red}.

Latency is effected by:

* Eomployed communication mechanism (Akka runtime message delivery vs. REST/RabbitMQ)

---

Another 

...

### Workload similation data

DATASET:

We need to simulate a workload scenario to benchmark each subsystem of the two system implementations. Although we are not interrested in evaluating the effectiveness, we can still look to information retrieval for workload data. IR uses standardized dataset collections for evaluations. These dataset collections usually consist of three parts [@Man08]:

* Part 1: Set of documents
* Part 2: Set of queries
* Part 3: Relevance assessment between queries and documents 
{ list-style-type:none }

We are interrested in Part 1 as input for the indexing subsystem and Part 2 as input for the retrieval subsystem. Part 3 is required for effectivness evaluation, hence we do not require this data. To our knowledge, there is only one available dataset collection provided by Spina *et al.* [@Spi17] for the Podcast domain. This collection contains audio files, manual and automatic audio transcripts, queries, and relevance assessments. Since the collection misses RSS feed data, the input documents are incompatible for the Echo implementations.

Hence we must design our own dataset collection. In general, we are faced with two problems: Selecting documents (RSS feeds) and determining suitable queries. Latency and throughput are affected by the execution time. The literature usually assesses execution time with respect to (wrt.) the input size. Real world RSS feeds have arbitrary data and therefore we have no control over the input size per feed. Since we do not pay any concern the actual information within either feeds nor queries, we can simply create custom feeds and queries. Simple placeholder text is sufficient for text data. Well-known is the so-called *Lorem ipsum* as a general placeholder text snippet [^fn-lorem-ipsum]. Appendix [#ch-benchmark-feed] shows the resulting feed structure.

[^fn-lorem-ipsum]: <https://lipsum.com>

~ todo
Hier kann ich eine größenverteilung über alle feeds als bild einfügen, wenn mir ganz fad sein sollte
~

### Software Artifact Metrics

DAS HIER SOLLTE ICH EINFACH SCHON BEI DER "conceptual comparison" OBEN SCHREIBEN. 

~ todo
LoC ist interessant weil es Rückschlüsse auf die Expressiveness zulässt. Byte code size ist interessant weil ich einen indirekt proportionalen Zusammenhang vermute (d.h. für weniger LoC nimmt man viel mehr eingebrachten byte code durch dependency injection in Kauf). Startup time soll nur illustrieren wie arg langsam so ein Service ist
~

Akka actors are part of a monolithic application, therefore the metrics are given for the whole application. The Spring microservices are independent programs, so the metrics can be given for each service engine separately.


...

~ todo
Tabelle: Byte code size der JAR files vom Akka Monolithen und aller MS. "Normales" JAR und all-in-one JAR?
~

... JAR = __Java__ __Ar__chive

...Fat JAR = self-sufficient archive, contains classes and dependencies, executable

...

We define a *skinny JAR* as ...

...and a *fat JAR* as ...

~ center
|---------------------|------|-----------------| -------------|
| Application         | LoC  | Skinny JAR (KB) | Fat JAR (KB) |
+---------------------+------+----------------:+-------------:+
| Actor engine (Akka) | 4445 | 1577.871        | 77301.418    |
| --------------------|------|-----------------|--------------|
| CatalogStore (MS)   | 1739 | 53.897          | 89176.382    |
| --------------------|------|-----------------|--------------|
| IndexStore (MS)     | 619  | 21.501          | 83468.697    |
| --------------------|------|-----------------|--------------|
| Searcher (MS)       | 570  | 20.118          | 81705.127    |
| --------------------|------|-----------------|--------------|
| Web Crawler (MS)    | 614  | 21.310          | 83468.508    |
| --------------------|------|-----------------| -------------|
| Parser (MS)         | 606  | 22.010          | 83469.654    |
| --------------------|------|-----------------|--------------|
| Updater (MS)        | 577  | 20.903          | 83468.101    |
| --------------------|------|-----------------| -------------|
{  }
~

~ todo
Hinweis das alle Executables die selbe `core` library verwenden, diese daher auch in jedem fat JAR enthalten ist.
~

### Profiling

~ lit
* Actor Utilization in Akka:
    * [@Moa17] "Supporting Resource Control for Actor Systems in Akka"
    * [@Ros16a] "Profiling actor utilization and communication in Akka"
    * [@Ros16b] "AkkaProf- A Profiler for Akka Actors in Parallel and Distributed Applications"
~

### Evaluating simulation results




### Software and Hardware Environment


### Document and Query Sets


Therefore, we require a workload similation to test both implementations against. A recent work [@Gri17] points out that there is no general simulation approach for MSAs as of yet. We have to design a custom experiment. Our search engine has two essential subsystems: the indexing subsystem and the retrieval subsystem. We will evaluate both subsystems separately.

Nevertheless, we are faced with the problem of a dataset for running the similation experiments. The indexing subsystem requires a collection of podcast RSS/Atom feeds. The retrieval subsystem requires filled Lucene datastructures and a collection of test queries. Domain-specific IR has specialised domain-specific datasets. To the authors knowledge, the only available dataset for podcast IR is given by [TODO]. This dataset was designed for evaluating extracted audio transcript. Therefore, the dataset contains URLs to audio files, but no RSS feeds. 

...

We've already mentioned that information retrieval uses the precision and recall metrics to evaluate IR techniques. IR uses standardized test dataset collections in order to make evaluation results compareable across scientific works. 


TABLE OF DATASET PARAMETERS:

~ todo
Tabelle
~

### Test environment




### Experiment Setup / Evaluation infrastructure

~ lit
* traditionally IR uses *test collections* for evaluation. 
* To our knowledge there is not Podcast related test collection available as of yet (since podcast is a rather unexplored domain in IR)
* We use a custom selection of feeds for the indexing subsystem (initial + update)
* We require a list of queries to test the retrieval subsystem
* IR has standardizes lists, but those are usually annotated for better effectiveness evaluation and limited to a specialized domain.
* normally query statistics form large Engines are not used, but we will
* we only test on a single machine (because we dont have more), therefore distribution and related scalability variants are not evaluated
~

For a performance evaluation, we require a workload simulation that we can use to test both the actor- and microservice implementations against. A recent work [@Gri17] points out that there is no general simulation approach for MSAs as of yet. Therefore, we have to design a custom reference scenario. 

---

Setup:

[Machine: XY GHz, XY cores, XY RAM, etc, running macOS XY]{.red}

* __Processor__: 3,1 GHz Intel Core i5
* __Memory__: 16 GB 2133 MHz LPDDR3
* Processor Name:	Intel Core i5
* Processor Speed:	3,1 GHz
* Number of Processors:	1
* Total Number of Cores:	2
* L2 Cache (per Core):	256 KB
* L3 Cache:	4 MB
* Memory:	16 GB
* Storage: 
    * Type: SSD
    * Connection: PCI-Express
    * File system: APFS

* JVM 8


### Simulation Experiment 1: Indexing Subsystem

...

### Simulation Experiment 2: Retrieval Subsystem

...



### Observations

* Slow RabbitMQ 
    * should be way faster (link to official benchmarks)
    * Declarative style with Spring's `@RabbitListener` has overhead, but should not be so much (REST is not that effected)

### Relevance of the Benchmark

Especially a lack of different interaction modes in microservice architecture benchmarks has been reported [@Zho18]. Most available benchmarks merely focus on one interaction mode, while MSA-related problems have been found to originate from asynchronous and mixed communication setups. Echo's subsystems engage this circumstance, since the indexing subsystem is modeled in an asynchronous fashion, and the retrieval subsystem in a synchronous fashion.

~ todo
Hier noch ein paar Wörter ob ich etwas gefunden habe bzgl der sync/async Geschichte
~

### Threats to Validity

* test data set not fixed size (in bytes and no of episodes); arbitrary selected collection of XML feeds
    * throughput usually w.r.t. size of messages
* Only one machine, with only 2 cores (4 virtual)
    * Akka only serializes messages when msg leaves local node -> every message effectively in-memory message passing
    * Every MS uses either RabbitMQ or REST, and always serializes data due to network interface
* Custom developed benchmark framework, not tools available for Akka and Spring that report compatibles values; custom framework might be inefficient 