
# Crawler

- lädt neue (= bisher unbekannte) feeds und gibt sie an indexer
- updated bereits bekannte feed (nach schlauen methoden sieht er nach ob sich was geändert hat, = content-seen test), und gibt die neuen info ggf an indexer
- verschickt nachrichten über den status seiner feedlade-versuche
- lädt die webseiten der podcasts/episoden
- sucht nach neuen feeds (**wie tut er das?? der indexer verarbeitet erst die daten**), und gibt diese zum test auf bekanntheit **an welche komponente??** weiter (= url-seen test; entdeckte feeds werden aber vorerst als nicht-freigegeben markiert)

# Indexer

- verarbeitet (dh parst) feed informationen die er vom crawler bekommt

# Searcher



# IndexStore

- verwaltet den zugriff auf den lucene index

# FeedStore

- hält eine liste von indizes vor
- bestimmt welche feeds neu verarbeitet werden von crawler?

