## Efficiency and Benchmark Results


~ todo
Ich soll Zahlen produzieren, also mus sich Dinge "messen". Also bitte...
~


We've compared the expressiveness and conceptual capabilities of actors and microservices and demonstrated that both can meet similar concerns. Now we are interested in the efficiency of each programming model. The Echo system implementations provide us with the foundation for a benchmark of the two models. The results will allow us to draw conclusions about the underlying concurrency concepts. Therefore, we require some metrics to gather in the benchmark. As we've already mentioned, information retrieval traditionally uses the metrics of precision and recall to evaluate search engines. However, precision and recall assess the effectiveness of the retrieval techniques. IR effectiveness is not withing the scope of this thesis. Instead, we want to measure the efficiency in terms of system performance. The common metrics we gather must be viable in both programming models, such that be can compare these metrics.


### Performance Metrics


*Savina* is a benchmark suit specifically designed for evaluating actor libraries [@Ima14]. Profiling studies have used Savina to gather detailed metrics for Akka [@Ros16a;@Ros16b]. However, the benchmark suit as well as the profiling tools are actor model and Akka specific. Hence, the metrics provided by the profiling are tailored to the actor model. 

Recent works point out that there is still a lack of microservice benchmarks [@Zho18]. Due to the technological diversity, there is also no general microservice profiling tool available. Hence, no widely agreed upon metrics have established in the literature. Since we cannot identify common metrics for the two programming  models from the literature, we have to revert to model unspecific metrics. Besides a lack of established common metrics between actors and microservices, there is also no general simulation approach for MSAs as of yet [@Gri17]. Additionally, we have to design a custom experiment too.

The Echo search engine resembles a web-server application. Therefore we can simply issue the services provided by an Echo system for experimentation. Performance indicators for servers have long been e.g.\ connections per second, throughput and round trip time (RTT) [@ElS06]. More recent work compared concurrency in Erlang, Go and Scala with Akka for server applications. The benchmark used communication latency, creation time & maximum process[^fn-concurrent-process] support and throughput as metrics [@Val18]. 

[^fn-concurrent-process]: In this context, *process* referes to a task unit in concurrency theory, and not an OS process.

A project we've found on GitHub [^fn-msa-framework-benchmark] benchmarks popular microservices frameworks using latency (RTT of a single request/work package), throughput and transfered data. The benchmark results include Spring Boot and Akka HTTP. The used experiment setup is rather simple. The frameworks are merely used to serve "*Hellow World!*" on a REST interface as the workload, which does not resemble a real world workload scenario

[^fn-msa-framework-benchmark]: <https://github.com/networknt/microservices-framework-benchmark>

From the described metrics, we chose to use latency and throughput as benchmark metrics. Since Akka does not serialize data [REF]{.red} if actors are on the same JVM, we refrain from gathering the transfered bytes as an additional metric. 


### STEHT HIER NOCH WAS BRAUCHBARES?!

...


---

As the performance metric of each subsystem we use the *latency* $L$ of the subsystem to complete a job. For the indexing subsystem, a job is the complete processing of a given feed. For the retrieval subsystem, a job is the retrieval of relevant documents for a given query.

If we know $L$, we can calculate the *throughput* $T$ of the subsystem simply with $T = 1 / L$. 

~ todo
images of the subsystem interaction models
~

All task units are based on the JVM and use the same `core` library for domain specific functionality. Platform and domain specific effects on latency and throughput are therefore uniform. All differences of latency and throughput between the two system implementations are a result of the unterlying programming models for communication and coordination [CITATION?]{.red}.

Latency is effected by:

* Eomployed communication mechanism (Akka runtime message delivery vs. REST/RabbitMQ)

---

Another 

...


### Simulation Workloads 


Our search engine has two essential subsystems: the indexing subsystem and the retrieval subsystem. We evaluate both subsystems separately. Each subsystem requires a different kind of input data. For benchmarking a subsystem, we need to simulate a workload scenario with appropriate input data. Although we are not interrested in evaluating the effectiveness, we can still look to information retrieval for workload data. IR uses standardized dataset collections for evaluations. These dataset collections usually consist of three parts [@Man08]:

* Part 1: Set of documents
* Part 2: Set of queries
* Part 3: Relevance assessment between queries and documents 
{ list-style-type:none }

We are interrested in Part 1 as input for the indexing subsystem and Part 2 as input for the retrieval subsystem. Part 3 is required for effectivness evaluation, hence we do not require this data. To our knowledge, there is only one available dataset collection provided by Spina *et al.* [@Spi17] for the Podcast domain. This collection contains audio files, manual and automatic audio transcripts, queries, and relevance assessments. Since the collection misses RSS feed data, the input documents are incompatible for the Echo implementations.

Therefore we must design our own dataset collection. In general, we are faced with two problems: selecting documents (RSS feeds) and determining suitable queries. Latency and throughput are affected by the execution time of operations (e.g.\ parsing a feed). The literature usually assesses execution time with respect to (wrt.) the input size. Real world RSS feeds have arbitrary data and therefore we have no control over the input size per feed. Since we do not pay any concern the actual information within either feeds nor queries, we can simply create custom feeds and queries using simple placeholder text. Well-known is the so-called *Lorem ipsum* as a general placeholder text snippet [^fn-lorem-ipsum]. Appendix [#ch-benchmark-feed] shows the resulting feed structure.

[^fn-lorem-ipsum]: <https://lipsum.com>

~ todo
Hier kann ich eine größenverteilung über alle feeds als bild einfügen, wenn mir ganz fad sein sollte
~


### Experiment Setup and Test Environment

~ lit
* traditionally IR uses *test collections* for evaluation. 
* To our knowledge there is not Podcast related test collection available as of yet (since podcast is a rather unexplored domain in IR)
* We use a custom selection of feeds for the indexing subsystem (initial + update)
* We require a list of queries to test the retrieval subsystem
* IR has standardizes lists, but those are usually annotated for better effectiveness evaluation and limited to a specialized domain.
* normally query statistics form large Engines are not used, but we will
* we only test on a single machine (because we dont have more), therefore distribution and related scalability variants are not evaluated
~

For a performance evaluation, we require a workload simulation that we can use to test both the actor- and microservice implementations against. A recent work [@Gri17] points out that there is no general simulation approach for MSAs as of yet. Therefore, we have to design a custom reference scenario. 

---

Setup:

[Machine: XY GHz, XY cores, XY RAM, etc, running macOS XY]{.red}

* __Processor__: 3,1 GHz Intel Core i5
* __Memory__: 16 GB 2133 MHz LPDDR3
* Processor Name:	Intel Core i5
* Processor Speed:	3,1 GHz
* Number of Processors:	1
* Total Number of Cores:	2
* L2 Cache (per Core):	256 KB
* L3 Cache:	4 MB
* Memory:	16 GB
* Storage: 
    * Type: SSD
    * Connection: PCI-Express
    * File system: APFS

* JVM 

...

* Indexing experiments are started on empty catalog/index
* Retrieval experiments are started on filled index

...

* Spring Boot uses the thread pool of the embedded web server. In our case, *Undertow* [REF?]{.red} is the web server.


### Simulation Experiment 1: Indexing Subsystem

...

### Simulation Experiment 2: Retrieval Subsystem

...



### Observations

* Slow RabbitMQ 
    * should be way faster (link to official benchmarks)
    * Declarative style with Spring's `@RabbitListener` has overhead, but should not be so much (REST is not that effected)

### Relevance of the Benchmark

Especially a lack of different interaction modes in microservice architecture benchmarks has been reported [@Zho18]. Most available benchmarks merely focus on one interaction mode, while MSA-related problems have been found to originate from asynchronous and mixed communication setups. Echo's subsystems engage this circumstance, since the indexing subsystem is modeled in an asynchronous fashion, and the retrieval subsystem in a synchronous fashion.

~ todo
Hier noch ein paar Wörter ob ich etwas gefunden habe bzgl der sync/async Geschichte
~

### Threats to Validity

* test data set not fixed size (in bytes and no of episodes); arbitrary selected collection of XML feeds
    * throughput usually w.r.t. size of messages
* Only one machine, with only 2 cores (4 virtual)
    * Akka only serializes messages when msg leaves local node -> every message effectively in-memory message passing
    * Every MS uses either RabbitMQ or REST, and always serializes data due to network interface
* Custom developed benchmark framework, not tools available for Akka and Spring that report compatibles values; custom framework might be inefficient 