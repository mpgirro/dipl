
A Concurrent System Scenario {#ch-problem-scenario}
=======================================================


~LitNote
* [@Bes10] "Podcast search: user goals and retrieval technologies"
* [@Brin98] "The Anatomy of a Large-Scale Hypertextual Web Search Engine"
* [@Cho04] "Agent Space Architecture for Search Engines"
~


In this chapter we will motivate a problem scenario that requires to be solves in a concurrent manner. It also opens up the opportunities to salvage the benefits that come with parallelization and distribution:  scalability, reliability, etc. To takle the tasks, chapters ? and ? will describe how it can be solved using the programming models of Actors and microservices in orther to satisfy the concurrent requirements.

The proposed search engine is dubbed *Echo*[^fn-echo-name]. For easier destinction, the respective implementations will be referenced to as *Echo:Actors* and *Echo:Microservices*.

[^fn-echo-name]: The name "Echo" was choosen for its wonderful characteristics of providing a short namespace and its analogy to *recalling spoken words*.  

---

The next two chapters will cover implementations based on Actors and Microservices of this scenario for a concurrent system. These implementations together with the scope of the requirements of this scenario will act as the guidpost for our discussion on comparing Actors and Microservices at the end of this thesis.

...

It offers us the opportunity to discuss commonalities, differences and general conceptualities regarding Actors and Microservice when such are utilized to implement concurrent systems. 

...

Search Engines cause the illusion to be closed monolithic systems, but actually are traditionally composed of many components that act in an decoupled and asynchronouse manner, such that they can perform...

Search engine construction in science the field of *information retrieval*, that focuses on *precision* and *recall* [zitieren?]{.mind}, which we will not pay attention to in this work. We will merely use some simple scoring methods provided by the information retrieval library used and trust that its results will sufficient.

## Domain Description

Podcasts = Decentralized media syndication

## System Components

~ Important
Diese hier als Description Lists aufzählen
~

* Crawlers
  : are tasked with downloading data from feed's URLs, as well as gathering information to discover new feeds. They can achieve this e.g. by crawling existing directories. 

* Parsers
  : analyze feed XML data, and extract Echo relevant data into internal system representation format.

* IndexStore 
  : TODO

* DirectoryStore 
  : TODO

* Gateway 
  : TODO

* Searcher 
  : TODO

* Updater 
  : TODO

...

... Lucene[^fn-lucene] for actual search index

....

A web interface is provided that both backends have to support. It is written with Angular[^fn-anngular], so the backends have to provide conform REST interfaces to interact with them from the outside. The web UI serves as a proof of concept for intended functionality of the backend implementations, as well as a [guini pig]{.important} for finding problems during development.

In order to prevent any domain specific code from being implemented twice unnecessarily, we supply both backends with a `core` library offering functionality that is required by either backend, e.g. the actual searching in the Lucene index or parsing RSS/Atom feed files. The library offers a Java interface, such that it is interoperable with most JVM based programming languages. 

...

[^fn-anngular]: <https://angular.io>
[^fn-lucene]: <https://lucene.apache.org>

### Crawler { -; toc:clear }

- lädt neue (= bisher unbekannte) feeds und gibt sie an indexer
- updated bereits bekannte feed (nach schlauen methoden sieht er nach ob sich was geändert hat, = content-seen test), und gibt die neuen info ggf an indexer
- verschickt nachrichten über den status seiner feedlade-versuche
- lädt die webseiten der podcasts/episoden
- sucht nach neuen feeds (**wie tut er das?? der indexer verarbeitet erst die daten**), und gibt diese zum test auf bekanntheit **an welche komponente??** weiter (= url-seen test; entdeckte feeds werden aber vorerst als nicht-freigegeben markiert)

- allg siehe https://cs.nyu.edu/courses/spring16/CSCI-GA.2580-001/crawler.html
- allg siehe https://web.njit.edu/~alexg/courses/cs345/OLD/F15/solutions/f2345f15.pdf

- bekommen URLs (entweder neue oder aus einem URL-repository) um diese herunterzuladen
- URLs sind entweder RSS/Atom-Feed oder die Homepages von Podcasts (allgemein) oder von Episoden (diese können zusatzinfos enthalten die in den Feeds nicht enthalten sind)
- Crawlers produzieren Arbeitspakete (für Parser) mit den Informationen die sie heruntergeladen haben
- Brauchen eine Refresh-Strategie mit der sie die URLs erneut besuchen
    - sollen sie diese selbst errechnen (ja, da sie sonst kaum Arbeit haben) oder in einen eigenen Actors/MS ausgelagert werden
- halten sich an Robots Exclusion Standard (robots.txt) etc
- normalisieren URLs 
- haben "policies":
    - selection policy: "only a fraction of the web-pages on the Web will be accessed, retrieved and subsequently processed/parsed and indexed. That fraction would depend on file extensions, multimedia handling, languages and encoding, compression and other protocols supported" --> nur Podcast relevante sachen
    - visit policy: "are variants of the two fundamental graph search operations: Breadth-First Search (BFS) and Depth-First Search (DFS): the former uses a queue to maintain a list of to be visited sites, and the latter a stack. Between the two the latter (DFS) is more often used in crawling"
        - Hyperlinked Web-pages: sollen zB in shownotes verlinkte seiten auch besucht und indexiert werden? --> an sich interessante informationen
    - observance policy: robots.txt beachten, nicht zu oft besuchen um dem server zu überlasten
    - parallelization/coordination policy: mehrere crawler, daher synchronisation welcher gerade welche web-page bearbeitet (synchronisierte queue/stack?)

- siehe "Mercator: A scalable, extensible Web crawler" https://dl.acm.org/citation.cfm?id=598733

### Parser { -; toc:clear }

### IndexStore { -; toc:clear }

- verwaltet den zugriff auf den lucene index

- Verarbeiten die Arbeitspakete die Crawler produzieren
- nimmt die web-pages und erzeugt den index (= effiziente datenstruktur die schnelles suchen erlaubt)
- in der literatur kommunizieren Crawler und Indexer durch ein gemeinsames Repository miteinander
    - da MS keine gemeinsamen DBs teilen, und Actors das bei mir auch nicht sollen, wie löse ich das in beiden Fällen in der Architektur?
- vorerst auszulassende Verbesserungen (da für meine Dipl nicht relevant)
    - Case-folding
    - Datums-homogenisierung
    - Hyphenization
    - Accents and punctuation marks
    - Stemming
    - Synonyms
    - Acronyms
    - etc.

### DirectoryStore { -; toc:clear }

### Searcher { -; toc:clear }


### Gateway { -; toc:clear }


### Updater { -; toc:clear }

- siehe "Effective page refresh policies for Web crawlers" https://dl.acm.org/citation.cfm?id=958945


## Concurrent Tasks and Information Flow

~Todo
Fetching data (feeds/websites), parsing data (feeds,websites), registering new entities (podcast/feeds, episodes), extending stores directory/index, serving search requests
~

In general, all previously described components have to be executed concurrently. However, from a more general perspective, there are certain concerns that have to be met in a concurrent fashion, in order to ensure a "running" search engine. These involve multiple task units, and therefore these have to corporate in terms of an information flow between them.


* Searching
  : The essential purpose of the engine in search. The web UI offers an interface similar to well-known search providers on the world wide web. Search requests are registered on the REST interface, and forwarded to a Searcher, who is doing some basic query processing and then realizes a  concrete query to the IndexStore. Its found results are then propagated back via the Searcher and the Gateway to the web UI. This has to happen in a timely manner.
* Discovering the Directory
  : The search engine provides besides searching index data records also a complete database of all known podcasts and episodes. The web UI offers basic support for exploring such data, e.g. by viewing info pages for podcasts or episodes. Thus REST requests have to be forwarded to the DirectoryStore in a timely fashion.
* Adding new feeds
  : New feeds can be proposed so that the engine will check weither it is already known and issue an indexation if not. Proposing can be done by either a user manually, or are crawled data from existing directories. Thus the Updater issues regular crawling for supported Podcast directories.
* Processing Feeds
  : Feeds are either processed when they are initially indexed, or updated to check for new episodes (and updated metadata). Thus processing can be triggered by adding a yet unknown feed, or on demand by the Updater. [updater finds this out by regularly asking a DirectoryStore for the feeds that have not been updated the longest]{.mind}. Feeds are then fetched by the Crawlers, and the received data is passed on to Parsers. Those extract podcast and episode data from the XML. The podcast data can be used to update (or first time set) the metadata in all DirectoryStores. Each episode metadata record is used to check weiter an episode is yet known, or if the feed contained a new entry. This can only be detected by a DirectoryStore (and its record of all metadata). New episodes metadata is sent to the IndexStores, where it added to the Lucene index datastructures.

...


For example the initial fetching or updating of a feed, and subsequent processing of the feed's data involves nearly all system components. This is a regular task and to be expected that nearly all the time there is some updating process happening in an a relevantly big enough database. These should however not interfere or limit other regular tasks, especially those with time constraints as all involving user interacton do, thus synchronization or transactions are generally an undesireable and less expidient path. [stimmt schon, aber soll ich da was zitieren? oder das vll überhaupt gar nicht erwähnen?]{.important}