
A Concurrent System Scenario {#ch-scenario}
============================



~ Epigraph { caption: "Eric S. Raymond"}
Every good work of software starts by scratching a developer's personal itch.
~

<!--
~ Epigraph { caption: "Butler Lampson"}
I think the computer is the world's greatest toy. You can invent wonderful things and actually make them happen.
~
-->


~LitNote
* [@Bes10] "Podcast search: user goals and retrieval technologies"
* [@Brin98] "The Anatomy of a Large-Scale Hypertextual Web Search Engine"
* [@Cho04] "Agent Space Architecture for Search Engines"
~


In this chapter we will motivate a problem scenario that requires to be solves in a concurrent manner. It also opens up the opportunities to salvage the benefits that come with parallelization and distribution:  scalability, reliability, etc. To takle the tasks, chapters ? and ? will describe how it can be solved using the programming models of Actors and microservices in orther to satisfy the concurrent requirements.

The proposed search engine is dubbed *Echo*[^fn-echo-name]. For easier destinction, the respective implementations will be referenced to as *Echo:Actors* and *Echo:Microservices*.

[^fn-echo-name]: The name "Echo" was choosen for its wonderful characteristics of providing a short namespace and its analogy to *recalling spoken words*.  

---

The next two chapters will cover implementations based on Actors and Microservices of this scenario for a concurrent system. These implementations together with the scope of the requirements of this scenario will act as the guidpost for our discussion on comparing Actors and Microservices at the end of this thesis.

...

It offers us the opportunity to discuss commonalities, differences and general conceptualities regarding Actors and Microservice when such are utilized to implement concurrent systems. 

...

Search Engines cause the illusion to be closed monolithic systems, but actually are traditionally composed of many components that act in an decoupled and asynchronouse manner, such that they can perform...

...

[@Man08]

Constructing search engines is led by two basic requirements:

* Effectiveness
  : is about the quality of search results. Information retrieval is distinquishing between two aspect regarding effectiveness, which are *precision* and *recall*. Optimizing effectiveness is not within the scope of this thesis, and therefore merely a simple scoring method provided by the used information retrieval library is applied.

* Efficiency
  : is about response time and throughput, therefore highly related to the concurrent processing capabilities of the system.

Before these requirements can be met, the information has to be gathered of course. This is another major aspect of the architecture, which is of more concern ...

## Domain Description

Podcasts = Decentralized media syndication

## System Components

The basic architectural components are based on the search engine architecture introduced by Brin and Page [@Brin98].


<!--
~ Center  {padding:1ex} 
~~ Snippet
% Start of code
\begin{pspicture}[linewidth=1bp](0.0bp,0.0bp)(176.0bp,252.0bp)
  \pstVerb{2 setlinejoin} % set line join style to 'mitre'
%%
\psset{linecolor=black}
  % Edge: P -> D
  \psbezier[arrows=->](57.049bp,35.992bp)(58.815bp,46.489bp)(61.065bp,59.989bp)(63.0bp,72.0bp)(66.348bp,92.783bp)(70.0bp,116.29bp)(74.268bp,144.06bp)
  % Edge: G -> S
  \psbezier[arrows=->](149.0bp,215.83bp)(149.0bp,208.13bp)(149.0bp,198.97bp)(149.0bp,180.41bp)
  % Edge: G -> D
  \psbezier[arrows=->](133.73bp,218.73bp)(123.8bp,208.8bp)(110.68bp,195.68bp)(92.247bp,177.25bp)
  % Edge: D -> I
  \psbezier[arrows=->](91.421bp,146.73bp)(100.8bp,136.8bp)(113.19bp,123.68bp)(130.6bp,105.25bp)
  % Edge: U -> D
  \psbezier[arrows=->](77.0bp,215.83bp)(77.0bp,208.13bp)(77.0bp,198.97bp)(77.0bp,180.41bp)
  % Edge: S -> I
  \psbezier[arrows=->](147.99bp,143.83bp)(147.56bp,136.13bp)(147.05bp,126.97bp)(146.02bp,108.41bp)
  % Edge: D -> C
  \psbezier[arrows=->](65.654bp,145.66bp)(59.414bp,136.68bp)(51.538bp,125.33bp)(38.648bp,106.77bp)
  % Edge: C -> P
  \psbezier[arrows=->](33.674bp,72.202bp)(36.748bp,64.006bp)(40.462bp,54.102bp)(47.403bp,35.593bp)
  % Node: C
{%
  \psset{linecolor=[rgb]{0.0,0.0,0.0}}
  \psellipse[](27.0bp,90.0bp)(27.0bp,18.0bp)
  \rput(27.0bp,90.0bp){C}
}%
  % Node: D
{%
  \psset{linecolor=[rgb]{0.0,0.0,0.0}}
  \psellipse[](77.0bp,162.0bp)(27.0bp,18.0bp)
  \rput(77.0bp,162.0bp){D}
}%
  % Node: G
{%
  \psset{linecolor=[rgb]{0.0,0.0,0.0}}
  \psellipse[](149.0bp,234.0bp)(27.0bp,18.0bp)
  \rput(149.0bp,234.0bp){G}
}%
  % Node: I
{%
  \psset{linecolor=[rgb]{0.0,0.0,0.0}}
  \psellipse[](145.0bp,90.0bp)(27.0bp,18.0bp)
  \rput(145.0bp,90.0bp){I}
}%
  % Node: P
{%
  \psset{linecolor=[rgb]{0.0,0.0,0.0}}
  \psellipse[](54.0bp,18.0bp)(27.0bp,18.0bp)
  \rput(54.0bp,18.0bp){P}
}%
  % Node: S
{%
  \psset{linecolor=[rgb]{0.0,0.0,0.0}}
  \psellipse[](149.0bp,162.0bp)(27.0bp,18.0bp)
  \rput(149.0bp,162.0bp){S}
}%
  % Node: U
{%
  \psset{linecolor=[rgb]{0.0,0.0,0.0}}
  \psellipse[](77.0bp,234.0bp)(27.0bp,18.0bp)
  \rput(77.0bp,234.0bp){U}
}%
%
\end{pspicture}
% End of code
~~
~ 
-->

<!--
~ Center  {padding:1ex} 
~~ Snippet
% Start of code
\begin{tikzpicture}[>=latex',line join=bevel,]
%%
\node (C) at (27.0bp,90.0bp) [draw,ellipse] {C};
  \node (D) at (77.0bp,162.0bp) [draw,ellipse] {D};
  \node (G) at (149.0bp,234.0bp) [draw,ellipse] {G};
  \node (I) at (145.0bp,90.0bp) [draw,ellipse] {I};
  \node (P) at (54.0bp,18.0bp) [draw,ellipse] {P};
  \node (S) at (149.0bp,162.0bp) [draw,ellipse] {S};
  \node (U) at (77.0bp,234.0bp) [draw,ellipse] {U};
  \draw [->] (P) ..controls (58.815bp,46.489bp) and (61.065bp,59.989bp)  .. (63.0bp,72.0bp) .. controls (66.348bp,92.783bp) and (70.0bp,116.29bp)  .. (D);
  \draw [->] (G) ..controls (149.0bp,208.13bp) and (149.0bp,198.97bp)  .. (S);
  \draw [->] (G) ..controls (123.8bp,208.8bp) and (110.68bp,195.68bp)  .. (D);
  \draw [->] (D) ..controls (100.8bp,136.8bp) and (113.19bp,123.68bp)  .. (I);
  \draw [->] (U) ..controls (77.0bp,208.13bp) and (77.0bp,198.97bp)  .. (D);
  \draw [->] (S) ..controls (147.56bp,136.13bp) and (147.05bp,126.97bp)  .. (I);
  \draw [->] (D) ..controls (59.414bp,136.68bp) and (51.538bp,125.33bp)  .. (C);
  \draw [->] (C) ..controls (36.748bp,64.006bp) and (40.462bp,54.102bp)  .. (P);
%
\end{tikzpicture}
% End of code
~~
~ 
-->

A web interface is provided that both backends have to support. It is written with Angular[^fn-anngular], so the backends have to provide conform REST interfaces to interact with them from the outside. The web UI serves as a proof of concept for intended functionality of the backend implementations, as well as a [meerschweinchen]{.important} for finding problems during development.

[^fn-anngular]: <https://angular.io>

In order to prevent any domain specific code from being implemented twice unnecessarily, we supply both backends with a `core` library offering functionality that is required by either backend. For example, the actual searching is done in a special data structure, the so-called *reverse index*. Lucene[^fn-lucene] is utilized to create such a structure. It offers a Java interface, such that it is interoperable with most JVM based programming languages. Also, parsing RSS/Atom feed files, especially using a custom written extensions to the ROME[^fn-rome] feed parsing libraries that allows to extract chapter marks.

[^fn-lucene]: <https://lucene.apache.org>
[^fn-rome]: <https://rometools.github.io/rome/>

* Crawlers (C)
  : are tasked with the aquisition of information, that is downloading data from URLs. These can relate to feeds, general websites or APIs of other existing directories, which can be used to discover new feeds. A major concern of web crawlers in general is robustness against the perils of the web [@Man08].

* Parsers (P)
  : are tasked with the transformation of aquired information, that is analyzing feed XML data, into internal  representation formats. The extracted data is what will be taken into account when running search queries and subsequently be displayed by the web interface. An important requirement on parsers is robustness regarding flawed information, for all considered data is encoded in a markup language, viz. XML or HTML.

* DirectoryStore (D)
  : hold a catalog of all podcasts, with respective feeds, episodes and chapter marks thereof. Such information is persisted in relational databases based on a simple, straightforward domain model. DirectoryStores hold the complete amount of information about any cataloged entity

* IndexStore (I)
  : hold the data structure that is used for searching, the so-called *reverse index*. Registered information entities are called *documents*, relating to one podcast or episode in our case. A reverse index maps the previous *document-term* relationship into a *term-document* structure. IndexStores only have revelant information that are needed to match the documents to search queries. Useful textual informations are e.g. title, subtitle, multiple general information tags (`<description>` or `<content:encoded>`), chapter marks that are found within feeds, as well as transcripts and the HTML source of linked websites. 

* Searcher (S)
  : are tasked with performing search operations. However, they simply perform some basic query pre-processing and delegate an IndexStore to retrieve relevant documents from the inverted index. The respective revelant results are communicated back via the Searcher to the Gateway.

* Gateway (G)
  : provides the link for the web interface to the system internal capabilities. It exposes a REST interface to the outside, and uses respective mechanisms to interact with other internal system components. The REST interface allows requesting cataloged entity information, or performing searches. Therefore the Gateway communicates with either a DirectoryStore, or a Searcher. 

* Updater (U)
  : [TODO]{.red}

Such task units ending in *\*Store* are stateful, all others stateless. When dataflow examples are given involving arbitrary components `X` and `Y`, that will be substituted with the component abbrevations (`C`, `P`, `D`, `I`, `S`, `G`, `U`) in due course, the simplistic notation is as follow: `X` &rarr; `Y` is expressing `X` sending a message to `Y` (push), while `X` &larr; `Y` denotes `X` fetching a message from `Y` (pull). `X` &rightleftarrows; `Y` is short for `X` sending a request message to `Y` which in turn replies with a response (synchronous call).

...

...

The overall component architecture with connections for information flow:

[TODO: grafik hier einfügen, sobald ich sie in LaTeX hübsch zeichnen kann]{.red}

...

... means of generating globally unique IDs without requiring any kind of consensus mechanism

...



~LitNote
Crawler:

- allg siehe https://cs.nyu.edu/courses/spring16/CSCI-GA.2580-001/crawler.html
- allg siehe https://web.njit.edu/~alexg/courses/cs345/OLD/F15/solutions/f2345f15.pdf
- haben "policies":
    - selection policy: "only a fraction of the web-pages on the Web will be accessed, retrieved and subsequently processed/parsed and indexed. That fraction would depend on file extensions, multimedia handling, languages and encoding, compression and other protocols supported" --> nur Podcast relevante sachen
    - visit policy: "are variants of the two fundamental graph search operations: Breadth-First Search (BFS) and Depth-First Search (DFS): the former uses a queue to maintain a list of to be visited sites, and the latter a stack. Between the two the latter (DFS) is more often used in crawling"
        - Hyperlinked Web-pages: sollen zB in shownotes verlinkte seiten auch besucht und indexiert werden? --> an sich interessante informationen
    - observance policy: robots.txt beachten, nicht zu oft besuchen um dem server zu überlasten
    - parallelization/coordination policy: mehrere crawler, daher synchronisation welcher gerade welche web-page bearbeitet (synchronisierte queue/stack?)

- siehe "Mercator: A scalable, extensible Web crawler" https://dl.acm.org/citation.cfm?id=598733
~

~LitNote
Updater:

- siehe "Effective page refresh policies for Web crawlers" https://dl.acm.org/citation.cfm?id=958945
~

## Concurrent Tasks and Information Flow

~Todo
Fetching data (feeds/websites), parsing data (feeds,websites), registering new entities (podcast/feeds, episodes), extending stores directory/index, serving search requests
~

In general, all previously described components have to be executed concurrently. However, from a more general perspective, there are certain concerns that have to be met in a concurrent fashion, in order to ensure a "running" search engine. These involve multiple task units, and therefore these have to corporate in terms of an information flow between them.


* Searching
  : The essential purpose of the engine in search. The web UI offers an interface similar to well-known search providers on the world wide web. Search requests are registered on the REST interface, and forwarded to a Searcher (`G` &rarr; `S`), who is doing some basic query processing and then realizes a concrete query to the IndexStore (`S` &rarr; `I`). Its found results are then propagated back via the Searcher (`I` &rarr; `S`) and the Gateway (`S` &rarr; `G`) to the web UI. This has to happen in a timely manner. The complete synchronous flow can be described as `G` &rightleftarrows; `S` &rightleftarrows; `I`. 
* Exploring the Directory
  : The search engine provides besides searching index data records also a complete database of all known podcasts and episodes. The web UI offers basic support for exploring such data, e.g. by viewing info pages for podcasts or episodes. Thus REST requests have to be forwarded from the Gateway to the DirectoryStore (`G` &rightleftarrows; `D`) in a timely fashion.
* Adding new Feeds
  : New feeds can be proposed so that the engine will check weither it is already known and issue an indexation if not. Proposing can be done by either a user manually, or are crawled data from existing directories. Thus the Updater issues regular crawling for supported Podcast directories.
* Processing Feeds
  : Feeds are either processed when they are initially indexed, or updated to check for new episodes (and updated metadata). Thus processing can be triggered by adding a yet unknown feed, or on demand by the Updater. [updater finds this out by regularly asking a DirectoryStore for the feeds that have not been updated the longest]{.mind}. Feeds are then fetched by the Crawlers, and the received data is passed on to Parsers. Those extract podcast and episode data from the XML. The podcast data can be used to update (or first time set) the metadata in all DirectoryStores. Each episode metadata record is used to check weiter an episode is yet known, or if the feed contained a new entry. This can only be detected by a DirectoryStore (and its record of all metadata). New episodes metadata is sent to the IndexStores, where it added to the Lucene index datastructures.

...


For example the initial fetching or updating of a feed, and subsequent processing of the feed's data involves nearly all system components. This is a regular task and to be expected that nearly all the time there is some updating process happening in an a relevantly big enough database. These should however not interfere or limit other regular tasks, especially those with time constraints as all involving user interacton do, thus synchronization or transactions are generally an undesireable and less expidient path. [stimmt schon, aber soll ich da was zitieren? oder das vll überhaupt gar nicht erwähnen?]{.important}


## Features and Limitations

[Hier beschreiben was die Implementierungen leisten können sollen, und was nicht]{.red}

* static configuration of components
  * KEINE elasticity
  * KEINE mobility at runtime 
  * keine notwendigkeit von event sourcing