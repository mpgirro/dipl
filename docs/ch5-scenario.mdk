
A Concurrent System Scenario {#ch-problem-scenario}
=======================================================


~LitNote
* [@Bes10] "Podcast search: user goals and retrieval technologies"
* [@Brin98] "The Anatomy of a Large-Scale Hypertextual Web Search Engine"
* [@Cho04] "Agent Space Architecture for Search Engines"
~


In this chapter we will motivate a problem scenario that requires to be solves in a concurrent manner. It also opens up the opportunities to salvage the benefits that come with parallelization and distribution:  scalability, reliability, etc. To takle the tasks, chapters ? and ? will describe how it can be solved using the programming models of Actors and microservices in orther to satisfy the concurrent requirements.

The proposed search engine is dubbed *Echo*[^fn-echo-name]. For easier destinction, the respective implementations will be referenced to as *Echo:Actors* and *Echo:Microservices*.

[^fn-echo-name]: The name "Echo" was choosen for its wonderful characteristics of providing a short namespace and its analogy to *recalling spoken words*.  

---

The next two chapters will cover implementations based on Actors and Microservices of this scenario for a concurrent system. These implementations together with the scope of the requirements of this scenario will act as the guidpost for our discussion on comparing Actors and Microservices at the end of this thesis.

## Why is a search engine a concurrent/parall./distr. task?

~LitNote
* Search Engines appears as a closed monolithic system (frontend), but is actually composed of many components that act in an decoupled and asynchronouse manmer (if the searc engine should perform)
* search engine construction in science the field of *information retrieval*, that focuses on *precision* and *recall*, which we will not pay attention to in this work (we will only use some simple scoring methods and trust that the results will be fine)
~

## Domain Description

Podcasts = Decentralized media syndication

## System Components

### Crawler

- lädt neue (= bisher unbekannte) feeds und gibt sie an indexer
- updated bereits bekannte feed (nach schlauen methoden sieht er nach ob sich was geändert hat, = content-seen test), und gibt die neuen info ggf an indexer
- verschickt nachrichten über den status seiner feedlade-versuche
- lädt die webseiten der podcasts/episoden
- sucht nach neuen feeds (**wie tut er das?? der indexer verarbeitet erst die daten**), und gibt diese zum test auf bekanntheit **an welche komponente??** weiter (= url-seen test; entdeckte feeds werden aber vorerst als nicht-freigegeben markiert)

- allg siehe https://cs.nyu.edu/courses/spring16/CSCI-GA.2580-001/crawler.html
- allg siehe https://web.njit.edu/~alexg/courses/cs345/OLD/F15/solutions/f2345f15.pdf

- bekommen URLs (entweder neue oder aus einem URL-repository) um diese herunterzuladen
- URLs sind entweder RSS/Atom-Feed oder die Homepages von Podcasts (allgemein) oder von Episoden (diese können zusatzinfos enthalten die in den Feeds nicht enthalten sind)
- Crawlers produzieren Arbeitspakete (für Parser) mit den Informationen die sie heruntergeladen haben
- Brauchen eine Refresh-Strategie mit der sie die URLs erneut besuchen
    - sollen sie diese selbst errechnen (ja, da sie sonst kaum Arbeit haben) oder in einen eigenen Actors/MS ausgelagert werden
- halten sich an Robots Exclusion Standard (robots.txt) etc
- normalisieren URLs 
- haben "policies":
    - selection policy: "only a fraction of the web-pages on the Web will be accessed, retrieved and subsequently processed/parsed and indexed. That fraction would depend on file extensions, multimedia handling, languages and encoding, compression and other protocols supported" --> nur Podcast relevante sachen
    - visit policy: "are variants of the two fundamental graph search operations: Breadth-First Search (BFS) and Depth-First Search (DFS): the former uses a queue to maintain a list of to be visited sites, and the latter a stack. Between the two the latter (DFS) is more often used in crawling"
        - Hyperlinked Web-pages: sollen zB in shownotes verlinkte seiten auch besucht und indexiert werden? --> an sich interessante informationen
    - observance policy: robots.txt beachten, nicht zu oft besuchen um dem server zu überlasten
    - parallelization/coordination policy: mehrere crawler, daher synchronisation welcher gerade welche web-page bearbeitet (synchronisierte queue/stack?)

- siehe "Mercator: A scalable, extensible Web crawler" https://dl.acm.org/citation.cfm?id=598733

### Parser

### IndexStore

- verwaltet den zugriff auf den lucene index

- Verarbeiten die Arbeitspakete die Crawler produzieren
- nimmt die web-pages und erzeugt den index (= effiziente datenstruktur die schnelles suchen erlaubt)
- in der literatur kommunizieren Crawler und Indexer durch ein gemeinsames Repository miteinander
    - da MS keine gemeinsamen DBs teilen, und Actors das bei mir auch nicht sollen, wie löse ich das in beiden Fällen in der Architektur?
- vorerst auszulassende Verbesserungen (da für meine Dipl nicht relevant)
    - Case-folding
    - Datums-homogenisierung
    - Hyphenization
    - Accents and punctuation marks
    - Stemming
    - Synonyms
    - Acronyms
    - etc.

### DirectoryStore

### Searcher


### Gateway


### Updater

- siehe "Effective page refresh policies for Web crawlers" https://dl.acm.org/citation.cfm?id=958945

## Information Flow

## Concurrent Tasks

Fetching data (feeds/websites), parsing data (feeds,websites), registering new entities (podcast/feeds, episodes), extending stores directory/index, serving search requests

## Scalability Opportunities

### Scalability through ??? (scaling up = changing allocated resources: memory, CPU)

### Scalability through Parallelization and Distribution (scaling out)

## Elasticity Considerations

## Polyglot Persistence (based on Isolation)