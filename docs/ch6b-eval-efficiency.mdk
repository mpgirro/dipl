
## Benchmark and Efficiency {#sec-eval-efficiency}


~ todo
Ich soll Zahlen produzieren, also mus sich Dinge "messen". Also bitte...
~


We've compared the expressiveness and conceptual capabilities of actors and microservices and demonstrated that both can meet similar concerns. Now we are interested in the efficiency of each programming model. The Echo system implementations provide us with the foundation for a benchmark of the two models. The results allow us to draw conclusions about the underlying concurrency concepts. Therefore, we require some metrics to gather in the benchmark. As we've already mentioned, information retrieval traditionally uses the metrics of precision and recall to evaluate search engines. However, precision and recall assess the effectiveness of the retrieval techniques. IR effectiveness is not withing the scope of this thesis. Instead, we want to measure the efficiency in terms of system performance. The common metrics we gather must be viable in both programming models, such that we can compare the values.


### Performance Metrics


~ todo
In [@Lil09] wird die Performance eines Search Engine mit einem Agent approach getestet! Hier 1) darauf hinweisen und 2) einfach die selben Metriken zeigen
~

~ todo
In [@Ped06] werden auch Sachen gemessen für einen Search Engine Cluster
~


*Savina* is a benchmark suit specifically designed for evaluating actor libraries [@Ima14]. Profiling studies have used Savina to gather detailed metrics for Akka [@Ros16a;@Ros16b]. However, the benchmark suit as well as the profiling tools are actor model and Akka specific. Hence, the metrics provided by the profiling are tailored to the actor model. 

Recent works point out that there is still a lack of microservice benchmarks [@Zho18]. Due to the technological diversity, there is also no general microservice profiling tool available. Hence, no widely agreed upon metrics have established in the literature. Since we cannot identify well-established metrics both programming  models share from the literature, we have to revert to model unspecific metrics. Besides a lack of established metrics between actors and microservices, there is also no general simulation approach for MSAs as of yet [@Gri17]. Additionally, we have to design a custom experiment too.

The Echo search engine resembles a web-server application. Therefore we can simply issue the services provided by an Echo system for experimentation. Performance indicators for servers have long been e.g.\ connections per second, throughput and round trip time (RTT) [@ElS06]. More recent work compared concurrency in Erlang, Go and Scala with Akka for server applications. The benchmark used communication latency, creation time & maximum process[^fn-concurrent-process] support and throughput as metrics [@Val18]. 

[^fn-concurrent-process]: In this context, *process* referes to a task unit in concurrency theory, and not an OS process.

A project we've found on GitHub [^fn-msa-framework-benchmark] benchmarks popular microservices frameworks using latency (RTT of a single request/work package), throughput and transfered data. The benchmark results include Spring Boot and Akka HTTP. The used experiment setup is rather simple. The frameworks are merely used to serve "*Hello World!*" on a REST interface as the workload, which does not resemble a real world workload scenario.

[^fn-msa-framework-benchmark]: <https://github.com/networknt/microservices-framework-benchmark>

From the described metrics, we chose to use latency and throughput as benchmark metrics. Since Akka does not serialize data [REF]{.red} if actors are on the same JVM, we refrain from gathering the transfered bytes as an additional metric. 

...

* Def. Throughput: "requests are executed successfully (accepted, processed and responded properly)."

* The CPU load value is the percentage of CPU used in the range 0-1, normalized to a single CPU. We run the simulations on a multicore machine. 

---

Pedzai & Suleman [@Ped06] evaluate the resource utilization of 


Lillis *et al.* [@Lil09] used an agent-based approach to improve the performance of search engines by utilizing techniques that fall into our broad definition of concurrency. They also incorporate functionalities we've discussed in this work, like synchronous/asynchronous and one-to-one/one-to-many communication styles, broker functionalities, lifecycle management (cf. Akka runtime, Spring IoC), 

...

Lillis *et al.* [@Lil09] as well as Pedzai & Suleman [@Ped06] each use concurrent programming variants to improve the performance of search engines. Among other things, they evaluate the performance by measuring the time it takes a system to index a given number of documents. The retrieval subsystem's performance is the time it takes to process a given number of queries.

...

All task units are based on the JVM and use the same Core library for domain specific functionality. Platform and domain specific effects on influencing the performance are therefore uniform between both implementations. All differences in the performance are therefore a result of the unterlying programming models for concurrency, hence communication and coordination [CITATION?]{.red}.


### Simulation Workloads 


Our search engine has two essential subsystems: the indexing subsystem and the retrieval subsystem. Since the indexing subsystem is not affected by the rate of search requests, we can evaluate both subsystems separately [@Lil09]. Each subsystem requires a different kind of input data. For benchmarking a subsystem, we need to simulate a workload scenario with appropriate input data. Although we are not interrested in evaluating the effectiveness, we can still look to information retrieval for workload data. IR uses standardized dataset collections for evaluations. These dataset collections usually consist of three parts [@Man08]:

* Part 1: Set of documents
* Part 2: Set of queries
* Part 3: Relevance assessment between queries and documents 
{ list-style-type:none }

We are interrested in Part 1 as input for the indexing subsystem and Part 2 as input for the retrieval subsystem. Part 3 is required for effectivness evaluation, hence we do not require this data. To our knowledge, there is only one available dataset collection provided by Spina *et al.* [@Spi17] for the Podcast domain. This collection contains audio files, manual and automatic audio transcripts, queries, and relevance assessments. Since the collection misses RSS feed data, the input documents are incompatible for the Echo implementations.

Therefore we must design our own dataset collection. In general, we are faced with two problems: selecting documents (RSS feeds) and determining suitable queries. Performance is affected by the execution time of operations (e.g.\ parsing a feed). The literature usually assesses execution time with respect to (wrt.) the input size. Real world RSS feeds have arbitrary data and therefore we have no control over the input size per feed. Since we do not pay any concern the actual information within either feeds nor queries, we can simply create custom feeds and queries using simple placeholder text. Appendix [#ch-benchmark-feed] shows the feed structure we use for evaluation.

[^fn-lorem-ipsum]: <https://lipsum.com>

~ todo
Hier kann ich eine größenverteilung über alle feeds als bild einfügen, wenn mir ganz fad sein sollte
~


### Experiment Setup and Test Environment

~ lit
* traditionally IR uses *test collections* for evaluation. 
* To our knowledge there is not Podcast related test collection available as of yet (since podcast is a rather unexplored domain in IR)
* We use a custom selection of feeds for the indexing subsystem (initial + update)
* We require a list of queries to test the retrieval subsystem
* IR has standardizes lists, but those are usually annotated for better effectiveness evaluation and limited to a specialized domain.
* normally query statistics form large Engines are not used, but we will
* we only test on a single machine (because we dont have more), therefore distribution and related scalability variants are not evaluated
~

For a performance evaluation, we require a workload simulation that we can use to test both the actor- and microservice implementations against. A recent work [@Gri17] points out that there is no general simulation approach for MSAs as of yet. Therefore, we have to design a custom reference scenario. 

---

Setup:

[Machine: XY GHz, XY cores, XY RAM, etc, running macOS XY]{.red}

* __Processor__: 3,1 GHz Intel Core i5
* __Memory__: 16 GB 2133 MHz LPDDR3
* Processor Name:	Intel Core i5
* Processor Speed:	3,1 GHz
* Number of Processors:	1
* Total Number of Cores:	2
* L2 Cache (per Core):	256 KB
* L3 Cache:	4 MB
* Memory:	16 GB
* Storage: 
    * Type: SSD
    * Connection: PCI-Express
    * File system: APFS

* JVM 

...

In this benchmark, we will merely explore effects related to vertical scalability (efficient resource utilization of a host machine), load scalability, and structural scalability [MUSS ICH DANN AUCH WIRKLICH AUSTESTEN -> MEHR MS!]{.red}. Horizontal scalability is beyond our capabilities due to the limitation of hardware resources available to us. As we've pointed out, the actor and microservice programming models are also capable of mobility and elasticity in principle. However, we will not explore these two forms of scalability, since the Echo implementations are not capable of them.

...

All measurements we report are for cold starts of each system. We restart all involved JVMs for each simulation. 


...

* Indexing experiments are started on empty catalog/index
* Retrieval experiments are started on filled index
* To avoid interferences of non-model specific parameters (latencies, etc.), we use the same database API in both architecture. Hence, the Akka implementation uses the Spring Data JPA binding that fits naturally into the CatalogStore microservice.  

...

* Spring Boot uses the thread pool of the embedded web server. In our case, *Undertow* [REF?]{.red} is the web server.

### Benchmark Results

#### Experiment 1: Indexing Subsystem

~ Figure { #fig-eval-index-overall; caption: "Overall processing time of indexing feeds"; width:100%; }
![img-eval-index-overall]
~

[img-eval-index-overall]: graphics/eval-index-overall.[pdf,png] "Image about eval-index-overall" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

~ Figure { #fig-eval-index-rtt-overall; caption: "Round trip time of messages in the indexing phase"; width:100%; }
![img-eval-index-rtt-overall]
~

[img-eval-index-rtt-overall]: graphics/eval-index-rtt-overall.[pdf,png] "Image about eval-index-rtt-overall" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

~ Figure { #fig-eval-index-rtt-meanMsgL; caption: "Mean message latency of messages in the indexing phase"; width:100%; }
![img-eval-index-rtt-meanMsgL]
~

[img-eval-index-rtt-meanMsgL]: graphics/eval-index-rtt-meanMsgL.[pdf,png] "Image about eval-index-rtt-meanMsgL" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

#### Experiment 2: Retrieval Subsystem

~ Figure { #fig-eval-search-overall; caption: "Overall processing time of search requests"; width:100%; }
![img-eval-search-overall]
~

[img-eval-search-overall]: graphics/eval-search-overall.[pdf,png] "Image about eval-search-overall" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

~ Figure { #fig-eval-search-rtt-overall; caption: "Round trip time of messages in the retrieval phase"; width:100%; }
![img-eval-search-rtt-overall]
~

[img-eval-search-rtt-overall]: graphics/eval-search-rtt-overall.[pdf,png] "Image about eval-search-rtt-overall" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

~ Figure { #fig-eval-search-rtt-meanMsgL; caption: "Mean message latency of messages in the retrieval phase"; width:100%; }
![img-eval-search-rtt-meanMsgL]
~

[img-eval-search-rtt-meanMsgL]: graphics/eval-search-rtt-meanMsgL.[pdf,png] "Image about eval-search-rtt-meanMsgL" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

#### Experiment 3: Thread Utilization Efficiency


#### Observations

* Slow RabbitMQ 
    * should be way faster (link to official benchmarks)
    * Declarative style with Spring's `@RabbitListener` has overhead, but should not be so much (REST is not that effected)

### Relevance of the Benchmark

Especially a lack of different interaction modes in microservice architecture benchmarks has been reported [@Zho18]. Most available benchmarks merely focus on one interaction mode, while MSA-related problems have been found to originate from asynchronous and mixed communication setups. Echo's subsystems engage this circumstance, since the indexing subsystem is modeled in an asynchronous fashion, and the retrieval subsystem in a synchronous fashion.

~ todo
Hier noch ein paar Wörter ob ich etwas gefunden habe bzgl der sync/async Geschichte
~

### Threats to Validity

* test data set not fixed size (in bytes and no of episodes); arbitrary selected collection of XML feeds
    * throughput usually w.r.t. size of messages
* Only one machine, with only 2 cores (4 virtual)
    * Akka only serializes messages when msg leaves local node -> every message effectively in-memory message passing
    * Every MS uses either RabbitMQ or REST, and always serializes data due to network interface
* Custom developed benchmark framework, not tools available for Akka and Spring that report compatibles values; custom framework might be inefficient 
