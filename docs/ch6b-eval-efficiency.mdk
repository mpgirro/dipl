
## Benchmark and Efficiency {#sec-eval-efficiency}


We've compared the expressiveness and conceptual capabilities of actors and microservices, and demonstrated that both can meet similar concerns. Now we are interested in the efficiency we can leverage from each programming model. The Echo system implementations provide us with the foundation for a benchmark of the two models. 


### Performance Metrics


A benchmark requires measureable and comparable metrics. As we've already mentioned, information retrieval traditionally uses precision and recall metrics to evaluate search engines. However, precision and recall assess the effectiveness of the retrieval techniques. IR effectiveness is not withing the scope of this thesis. We require metrics which reflect efficiency. These metrics must be applicable to actors, microservices, and our scenario. 

*Savina* is a benchmark suit specifically designed for evaluating actor libraries [@Ima14]. Profiling studies have used Savina to gather detailed metrics for Akka [@Ros16a;@Ros16b]. However, the benchmark suit as well as the profiling tools are actor model and Akka specific. Hence, the metrics provided by the profiling are tailored to the actor model. 

Recent works point out that there is still a lack of microservice benchmarks [@Zho18]. Due to the technological diversity, there is also no general microservice profiling tool available. Hence, no widely agreed upon metrics have established in the literature. We have to revert to model unspecific metrics. Besides a lack of common metrics established between actors and microservices, there is also no general simulation approach for MSAs as of yet [@Gri17]. Additionally, we have to design a custom experiment too.

<!-- TOOD vll brauche ich hier noch etwas?!
The Echo search engine resembles a web-server application. Therefore we can simply issue the services provided by an Echo system for experimentation. Performance indicators for servers have long been e.g.\ connections per second, throughput and round trip time (RTT) [@ElS06]. More recent work compared concurrency in Erlang, Go and Scala with Akka for server applications. The benchmark used latency, creation time & maximum process[^fn-concurrent-process] support, and throughput as metrics [@Val18]. 

[^fn-concurrent-process]: In this context, *process* referes to a task unit in concurrency theory, and not an OS process.
-->

Lillis *et al.* [@Lil09] as well as Pedzai & Suleman [@Ped06] each use techniques we've discussed in this work, like synchronous/asynchronous and one-to-one/one-to-many communication styles, message brokers, and lifecycle management (cf. Akka runtime, Spring IoC) to improve the performance of search engines. Among other things, they evaluate the performance by measuring the time it takes a system to index a given number of documents. The retrieval subsystem's performance is the time it takes to process a given number of queries. We take this experiment design and use it to assess the efficiency of our actor and microservice implementations.


### Simulation Workloads 


Echo has two essential subsystems: the indexing subsystem and the retrieval subsystem. Since the indexing subsystem is not affected by the rate of search requests to the retrieval subsystem and vice versa, we can evaluate both subsystems separately [@Lil09]. Each subsystem requires a different kind of input data. For benchmarking a subsystem, we need to simulate a workload scenario with appropriate input data. Although we are not interrested in evaluating the effectiveness, we can still look to information retrieval for workload data. IR uses standardized dataset collections for evaluations. These dataset collections usually consist of three parts [@Man08]:

* Part 1: Set of documents
* Part 2: Set of queries
* Part 3: Relevance assessment between queries and documents 
{ list-style-type:none }

We are interrested in Part 1 as the input for the indexing subsystem and in Part 2 as the input for the retrieval subsystem. Effectivness evaluation uses Part 3, hence we do not require this data. To our knowledge, there is only one available dataset collection provided by Spina *et al.* [@Spi17] for the podcast domain. This collection contains audio files, manual and automatic audio transcripts, queries, and relevance assessments. Since the collection misses RSS feed data, the input documents are incompatible for the Echo implementations.

Therefore we must design our own dataset collection. In general, we are faced with two problems: selecting documents (RSS feeds) and determining suitable queries. Performance is affected by the execution time of operations (e.g.\ parsing a feed). The literature usually assesses execution time with respect to the input size. Real world RSS feeds have arbitrary data and therefore we have no control over the input size per feed. Since we do not pay any concern to the actual information within either the feeds, the queries, nor the quality of search results, we can simply create custom feeds and queries using placeholder text. Appendix [#ch-benchmark-feed] shows the feed structure we use for evaluation. We've analyzed 500 arbitrary feeds obtained from the Fyyd Podcast Directory [@FyydDirectory] and found that the average feed has 70 episodes. To make the simulation workloads more realistic, the test feeds also have 70 elements.


### Experiment Setup {#sec-experiment-setup}


All evaluation results are measured on a multicore platform (Intel Kaby Lake Core i5 3.1 GHz with 2 cores, 64 MB of eDRAM, 4 MB shared level 3 cache, 16 GB of 2133 MHz LPDDR3 SDRAM, Java HotSpot 64-bit server VM build 25.172-b11, macOS 10.13.6, 1 TB SDD flash storage, APFS file system). All code is compiled with Java compiler version 1.8.0 update 172 and Scala compiler version 2.12.6. The components with persistent states are deployed with an H2 [@Mue04] database engine version 1.4.196 (in-memory persistence mode) and Lucene version 7.2 respectively.

All measurements we report are for cold starts of each system. We restart all involved JVMs for each simulation. Indexing experiments are started on empty catalog/index. Retrieval experiments are started on a filled index. Crawlers load the benchmark feeds from the local file system, and are therefore not subject to network induced latencies. 

To eliminate a threat to the validity of the benchmark (discussed in Section [#sec-threats-to-validity] below), the CatalogStores of each system implementation use the Spring Data JPA library for database interaction. Usually, Spring Data JPA expects a Spring IoC container to handle concurrent connections and transaction management. To be able to handle concurrent database interaction directly via actors, the Akka implementation uses Spring Data JPA without an IoC container. The respective CatalogStore therefore has to manage transactions manually.

<!--
...

In this benchmark, we will merely explore effects related to vertical scalability (efficient resource utilization of a host machine), load scalability, and structural scalability [MUSS ICH DANN AUCH WIRKLICH AUSTESTEN -> MEHR MS!]{.red}. Horizontal scalability is beyond our capabilities due to the limitation of hardware resources available to us (single machine). As we've pointed out, the actor and microservice programming models are also capable of mobility and elasticity in principle. However, we will not explore these two forms of scalability, since the Echo implementations are not capable of them.

...
-->

To ensure that both implementation variants have the same resources for concurrent execution available to them, we assign each architecture component with a fixed number of threads. The Akka components use a `Dispatcher`{language:scala} backed by a `ThreadPoolExecutor`{language:java} (in contrast to the default `ForkJoinExecutor`{language:scala} with a dynamic pool size). The Spring IoC containers of the microservices are configured to use a `ThreadPoolTaskExecutor`{language:java}, where the `corePoolSize` is equal to the `maxPoolSize`. If not stated otherwise, we use 16 threads per component. This way, the benchmark results indicate which programming model better utilizes the available thread resources.


### Benchmark Results


To reduce the effects of outliers, we've conducted each experiment 3 times. Each datapoint in the following results is the mean of the three measured values. Note that all experiments are for static system configurations. Therefore the results do not reflect any forms of structural scalability (mobility, elasticity). Also, due to the single multicore host, we cannot draw conclusion to the horizontal scalability behavior of the architectures. 


#### Experiment 1: Indexing Subsystem


Figure [#fig-eval-index-overall] shows the benchmark results of the indexing subsystems for the overall time it takes the implementations to process the workload with respect to the input size. 

~ Figure { #fig-eval-index-overall; \
           caption: "Benchmark results for the overall processing time of the indexing subsystem with respect to the amount of feeds"; \
           width:100%; page-align:here; }
![img-eval-index-overall]
~

[img-eval-index-overall]: graphics/eval-index-overall.[pdf,png] "Image about eval-index-overall" { width:60%; vertical-align:middle; padding-bottom:1em; }

The figure also describes the load scalability behavior of the systems. With an increasing load for the indexing subsystem, both implementations scale uniformly, which is the desired behavior. This uniform behavior is the result of good vertical scalability, since the architectures are able to uniformly leverage the resources of the single multicore host used for the benchmark.

In Section [#sec-conception-concurrent-execution] we've discussed the issue of fairness and the implications on resource consumption. Figure [#fig-eval-index-mem] shows the mean memory resource consumption of the Echo implementations in the indexing phase. We've measured the memory usage (heap memory $+$ non-heap memory) using the `java.lang.management.MemoryMXBean`{language:java} that every JVM provides. The results illustrates how every microservice consumes separate system resources, even those who do not perform any work in the indexing phase (Gateway, Searcher). The Akka monolith that implements the entire Echo system consumes only slightly more memory resources as a single Spring-based microservice. The CatalogStore MS in an exception. The author suspects that the reason for the CatalogStore service's memory demand is that the IoC container of this service has to extend the STM to the database. The entire MSA has a considerably higher memory requirement than the entire actor-based system. The figure also illistrates the impact of the JVM as a microservice plattform. The JVM is a relatively heavy-weight VM. Every process incarnation of a Java-based microservices must be deployed in its own separate VM, which poses a considerable impact on the system resources. The operating system always allocates resources towards every process on a regular basis. In contrast, Akka actors, besides being more lightweight constructs in general, only get scheduled and thus consume resources when they have messages in their mailbox.


~ Figure { #fig-eval-index-mem; \
           caption: "Memory consumption of the executable artifact's VMs in the indexing phase"; \
           width:100%; page-align:here; }
![img-eval-index-mem]
~

[img-eval-index-mem]: graphics/eval-index-mem.[pdf,png] "Image about eval-index-mem" { width:60%; vertical-align:middle; padding-bottom:1em; }


<!--

~ begin green
Figure [EINFÜGEN]{.red} is suprising at first glance. However, recall the indexing pipeline from Section [#sec-subsystem-pipelines]. The most computationally expensive domain-specific jobs have the Crawler (loading XML feeds, from local SSD storage in this benchmark), and the Parser (process relatively large XML documents). We expect these two tasks to take the most time for each message. The Parser has another effect to the pipeline. Due to the Parser's domain logic and our test feeds, for each message this component receives it produces 71 new messages (70 for the episodes metadata and 1 for the podcast metadata) for the subsequent component (CatalogStore) in the pipeline. Since the Catalog forwards parts of the metadata to the IndexStore, the IndexStore also receives 71 times more messages that an Updater, Crawler or Parser.

We can draw several conclusions from Figure [EINFÜGEN]{.red}. The messages per second of CatalogStore is the lower bound of the capacity potential of each progrogramming model. We do not know the full amount of their MPS capacity, since the domain scenario provides a natural message limit. We can also conclude that all other components could reach this lower bound of the Catalog, if they were not hindered by the internal domain logic.  

Since we know which components cannot exploit their model's potential, we can use these limited components to evaluate structural scalability effects. Especially the Crawler is suitable, since its domain logic is basically IO. The actor-based implementation of the Crawler utilizes child delegation to improve the IO efficiency, and with more threads can more children execute in parallel. For the microservice-based component, the IoC container can consume more messages and execute respective IO tasks through the `TaskExecutor`{language:java} with more threads available to it.

We reconfigure the previous components in the pipeline with significantely more threads to ensure that the Crawler never suffers from temproral starvation, and benchmark the MPS for the Crawler with different underlying thread-pool sizes. The result is shown in Figure [EINFÜGEN]{.red}  
~ end green

-->



#### Experiment 2: Retrieval Subsystem


Figure [#fig-eval-search-rtt-overall] shows the results of the experiment to assess the retrieval subsystem's performance. It clearly indicates that the request/asynchronous response style of Akka actors is superior to the synchronous REST-based communication of the microservices. Since the Akka implementation processes heavy loads of requests considerably faster, this subsystem is more available and thus has better liveness. The figure also describes the load scalability behavior of the retrieval subsystems. Both implementations scale uniformly as desired. Again, we can trace this result back to the fact that the implementations leverage the available system resources of the single multicore host well (vertical scalability). However, the microservice variant shows considerably lower overall efficiency. Section [#sec-scalability-modularity] gave the synchronous nature of RPC as a major factor limiting the scalability of an MSA. Our benchmark results support this claim. The request/asynchronous response style of Akka is clearly more efficient than REST-based communication.

~ Figure { #fig-eval-search-rtt-overall; \
           caption: "Benchmark results for the overall processing time of the Retrieval subsystem with respect to the amount of search requests"; \
           width:100%; page-align:here; }
![img-eval-search-rtt-overall]
~

[img-eval-search-rtt-overall]: graphics/eval-search-rtt-overall.[pdf,png] "Image about eval-search-rtt-overall" { width:60%; vertical-align:middle; padding-bottom:1em; }


In Section [#sec-actor-communication-abstractions] we've actually discussed two different variants to model request/asynchronous response communication with Akka. One variant is based on futures, the other on custom child actors and delegation. The author expected that the delegation approach would show better performance, since a future always stresses the thread-pool, while the actor runtime must only schedule a delegation child once the response is available in the mailbox. The Akka result in Figure [#fig-eval-search-rtt-overall] therefore shows the delegation performance. To evaluate the future- and delegation-based modelling of (semi-)synchronous communication, we've conducted the retrieval experiment also with futures. Figure [#fig-eval-search-comparison-akka-delegation-future] illustrates the future results in contrast to the delegation results.

~ Figure { #fig-eval-search-comparison-akka-delegation-future; \
           caption: "Comparison of the benchmark reasults of the retrieval subsystem using either delegation or futures for request/response communication in the Akka-based implementation"; \
           width:100%; page-align:here; }
![img-eval-search-comparison-akka-delegation-future]
~

[img-eval-search-comparison-akka-delegation-future]: graphics/eval-search-comparison-akka-delegation-future.[pdf,png] "Image about eval-search-comparison-akka-delegation-future" { width:60%; vertical-align:middle; padding-bottom:1em; }

We see, neither request/async. response style shows a considerable performance advantage. Recall that the future and delegation strategies are also applicable for database interaction and IO. The results of Figure [#fig-eval-search-comparison-akka-delegation-future] suggest that each strategy is also equally efficient for database interaction. Therefore, we assume that the actor implementation does not suffer from a negative impact because it has to use the Spring Data JPA library for database access in these benchmark experiments.


### Relevance of the Benchmark


To the authors knowledge, there exists no benchmark comparing Akka actors and Spring-based microservices yet. We were only able to find one project on GitHub [^fn-msa-framework-benchmark] which benchmarks popular microservices frameworks. The benchmark results include Spring Boot with the Undertow webserver and Akka HTTP. The used experiment setup is rather simple. The frameworks merely serve "*Hello World!*" on a REST interface as the workload, which does not resemble a real world workload scenario. [Their results show a 3.18 times faster average latency and 2.98 times more throughput.]{.red}

[^fn-msa-framework-benchmark]: <https://github.com/networknt/microservices-framework-benchmark>

Especially a lack of different interaction modes in microservice architecture benchmarks has been reported [@Zho18]. Most available benchmarks merely focus on one interaction mode, while MSA-related problems have been found to originate from asynchronous and mixed communication setups. Echo's subsystems engage this circumstance, since the indexing subsystem is modeled in an asynchronous fashion, and the retrieval subsystem in a synchronous fashion. Our experiences do not reflect any problems from the asynchronous style. In contrast, we've found that the synchronous style is considerably less performant than the asynchronous style.

Interrestingly, Bonér, the creator of Akka, advocates in his recent works [@Bon16;@Bon17] for what he calls *reactive microservices*. Essentially, a reactive microservice is a service that orients itself on the actor principles, especially asynchronous messaging and the lack of global consistency (cf. eventual consistency). Bonér argues that reactive microservices are more performant than tightly-coupled synchronous services facilitating global consistency (e.g.\ via a transactions mechanism). However, he provides no benchmark results to back his claim. The subsystems of our microservice implementation reflect a reactive microservice style (Indexing pipeline) and a more "traditional" synchronous style (Retrieval pipeline). The Akka implementation provides the reference to a purely reactive (asynchronous) system. Our benchmark results support Bonér's argumentation that the reactive style is more performant.


### Threats to Validity {#sec-threats-to-validity}


Like all experimentals, we are also subject to some factors threatening the validity of our results.


#### External Threats to Validity 


The external threats concern how much our results are generalizable. The domain-specific actions of our scenario have an influence on the performance. Examples are the HTTP retrieval of RSS feeds, XML parsing and database IO. These actions dampen e.g.\ the throughput. The performance of these actions is influenced by the utilized underlying technologies (HTTP library, XML parser, database system). The comparability of benchmark metrics across systems is threatened. We mitigated this threat by founding all task units on the JVM. We provide all components with the same Core library, which implements the domain-specifiy functionality. The CatalogStores access the same kind of database system, and both implementations use the Spring Data JPA library for database interaction. Platform- and domain-specific effects are therefore uniform across the system implementations. Nevertheless, due to the domain influence, no general statement about the performance of Akka or Spring-based services is possible. Other metrics are not even measureable in the scenario. Examples are the creation time and maximum process support as reported in [@Val18]. The static configuration of our scenario does not intend elasticity, i.e.\ a dynamic creation of task units. Hence these metrics require an experiment outside the bounds of our scenario. 

Additionally, we conducted the experiments merely on a single multicore machine. The experiments therefore only incorporate the effect of vertical scalability leveraged from general concurrency and parallelism on one single machine. Also, the multicore machine has a very small number of cores. The test setup does not take into account the horizontal scalability effects of distribution-induced concurrency.

We eliminated the threat of selection bias by not using real world RSS feeds for simulation. Instead, we used test data with uniform size and feed structure.

<!--
Microservices are not a model of classic concurrency theory, and common benchmark metrics are difficult to apply. An example is maximum process support as reported in [@Val18]. The static configuration of our scenario does not intend elasticity, i.e.\ a dynamic creation of task units. Hence this metric requires an experiment outside of our scenario. Furthermore, from a theoretical point of view, we'd expect a very high upper limit of Akka actors, since an actors are modeled in the object space of OOP. Microservices are OS processes and in our scenario they require network communication. The maximum number of network ports is therefore also an upper limit for the microservices we can instantiate. The network port range is operating system dependend, but we expect it somewhere in the tens of thousands in general.
-->


#### Internal Threats to Validity 

The internal threats to the validity of this benchmark concern the accuracy of our data collection tool. Since we did not find a tool that is sufficient to collect the data for Akka actors and Spring-based microservices, we developed a custom benchmark framework. This toolkit is implemented to the best of our knowledge, but we have to assume that its efficiency is not state of the art. This threat is mitigated since both systems are subject to the same suspected inefficiency, which does not distort the relative comparison of metrics.  

An additional threat is resulting from the fixed amount of threads we provide for each component in the benchmark. Actor-based components can fully leverage all these threads, e.g.\ for child actors or futures. We've discussed in Section [#sec-internal-service-concurrency] that the microservices use a different `TaskExecutor`{language:java} for asynchronous tasks than the standard thread-pool of Spring's IoC container. To ensure the thread limit for the component, we have to split up the threads between the two thread-pools. There is the threat that a service's implementation does not distribute the computational load equally between the two thread-pools. We did not measure the thread-partitioning required for ideal thread utilization for each microservice. Instead, we distributed the available threads evenly between both pools for all services. This threat is only mitigated for services which do not apply asynchrony internally. Then there is only the standard `TaskExecutor`{language:java} with the full amount of threads in place. 

<!--
Furtermore, Akka does not serialize messages [REF?]{.red} if the sending and receiving actors are inside the same JVM. Every actor message is then simple in-memory reference sharing of immutable objects, while each microservice message is definitely serialized and sent via a network interface (although the message is always delivered on the same machine).
-->
