
## Benchmark and Efficiency {#sec-eval-efficiency}


~ todo
Ich soll Zahlen produzieren, also mus sich Dinge "messen". Also bitte...
~


We've compared the expressiveness and conceptual capabilities of actors and microservices and demonstrated that both can meet similar concerns. Now we are interested in the efficiency of each programming model. The Echo system implementations provide us with the foundation for a benchmark of the two models. The results allow us to draw conclusions about the underlying concurrency concepts. Therefore, we require some metrics to gather in the benchmark. As we've already mentioned, information retrieval traditionally uses the metrics of precision and recall to evaluate search engines. However, precision and recall assess the effectiveness of the retrieval techniques. IR effectiveness is not withing the scope of this thesis. Instead, we want to measure the efficiency in terms of system performance. The common metrics we gather must be viable in both programming models, such that we can compare the values.


### Performance Metrics


A benchmark is essentially a measureable and comparable metric for systems [REF?!]{.red}. In order to benchmark our system implementations, we therefore require metrics. We require these metrics to be applicable to actors, microservices, and our scenario. 

*Savina* is a benchmark suit specifically designed for evaluating actor libraries [@Ima14]. Profiling studies have used Savina to gather detailed metrics for Akka [@Ros16a;@Ros16b]. However, the benchmark suit as well as the profiling tools are actor model and Akka specific. Hence, the metrics provided by the profiling are tailored to the actor model. 

Recent works point out that there is still a lack of microservice benchmarks [@Zho18]. Due to the technological diversity, there is also no general microservice profiling tool available. Hence, no widely agreed upon metrics have established in the literature. Since we cannot identify well-established metrics both programming  models share from the literature, we have to revert to model unspecific metrics. Besides a lack of common metrics established between actors and microservices, there is also no general simulation approach for MSAs as of yet [@Gri17]. Additionally, we have to design a custom experiment too.



The Echo search engine resembles a web-server application. Therefore we can simply issue the services provided by an Echo system for experimentation. Performance indicators for servers have long been e.g.\ connections per second, throughput and round trip time (RTT) [@ElS06]. More recent work compared concurrency in Erlang, Go and Scala with Akka for server applications. The benchmark used latency, creation time & maximum process[^fn-concurrent-process] support, and throughput as metrics [@Val18]. 

[^fn-concurrent-process]: In this context, *process* referes to a task unit in concurrency theory, and not an OS process.



[^fn-msa-framework-benchmark]: <https://github.com/networknt/microservices-framework-benchmark>

Lillis *et al.* [@Lil09] as well as Pedzai & Suleman [@Ped06] each use techniques we've discussed in this work, like synchronous/asynchronous and one-to-one/one-to-many communication styles, broker functionalities, lifecycle management (cf. Akka runtime, Spring IoC), to improve the performance of search engines. Among other things, they evaluate the performance by measuring the time it takes a system to index a given number of documents. [We can use this simulation to benchmark the general efficiency as specifically the messages per second]{.green}. The retrieval subsystem's performance is the time it takes to process a given number of queries. [We can use this simulation to benchmark the response time (= response latency / round trip time)]{.green}

...




### Simulation Workloads 


Our search engine has two essential subsystems: the indexing subsystem and the retrieval subsystem. Since the indexing subsystem is not affected by the rate of search requests, we can evaluate both subsystems separately [@Lil09]. Each subsystem requires a different kind of input data. For benchmarking a subsystem, we need to simulate a workload scenario with appropriate input data. Although we are not interrested in evaluating the effectiveness, we can still look to information retrieval for workload data. IR uses standardized dataset collections for evaluations. These dataset collections usually consist of three parts [@Man08]:

* Part 1: Set of documents
* Part 2: Set of queries
* Part 3: Relevance assessment between queries and documents 
{ list-style-type:none }

We are interrested in Part 1 as input for the indexing subsystem and Part 2 as input for the retrieval subsystem. Part 3 is required for effectivness evaluation, hence we do not require this data. To our knowledge, there is only one available dataset collection provided by Spina *et al.* [@Spi17] for the Podcast domain. This collection contains audio files, manual and automatic audio transcripts, queries, and relevance assessments. Since the collection misses RSS feed data, the input documents are incompatible for the Echo implementations.

Therefore we must design our own dataset collection. In general, we are faced with two problems: selecting documents (RSS feeds) and determining suitable queries. Performance is affected by the execution time of operations (e.g.\ parsing a feed). The literature usually assesses execution time with respect to (wrt.) the input size. Real world RSS feeds have arbitrary data and therefore we have no control over the input size per feed. Since we do not pay any concern the actual information within either feeds nor queries, we can simply create custom feeds and queries using simple placeholder text. Appendix [#ch-benchmark-feed] shows the feed structure we use for evaluation. We've analyzed 500 arbitrary feeds obtained from the Fyyd[^fn-fyyd] podcast directory and found that the average feed has 70 episodes. To make the simulation workloads more realistic, the test feeds also have 70 elements.

[^fn-lorem-ipsum]: <https://lipsum.com>
[^fn-fyyd]: <https://fyyd.de>


### Experiment Setup and Test Environment


All evaluation results are measured on a multicore platform (Intel Kaby Lake Core i5 3.1 GHz with 2 cores, 64 MB of eDRAM, 4 MB shared level 3 cache, 16 GB of 2133 MHz LPDDR3 SDRAM, Java HotSpot 64-bit server VM build 25.172-b11, macOS 10.13.6, 1 TB SDD flash storage, APFS file system). All code is compiled with Java compiler version 1.8.0 update 172 and Scala compiler version 2.12.6.

All measurements we report are for cold starts of each system. We restart all involved JVMs for each simulation. Indexing experiments are started on empty catalog/index. Retrieval experiments are started on filled index. To avoid interferences of non-model specific parameters (latencies, etc.), we use the same database API in both architectures. Hence, the Akka implementation uses the Spring Data JPA binding that fits naturally into the CatalogStore microservice. Crawlers load the benchmark feeds from the local file system, and therefore are not subject to network induced latencies. 

<!--
...

In this benchmark, we will merely explore effects related to vertical scalability (efficient resource utilization of a host machine), load scalability, and structural scalability [MUSS ICH DANN AUCH WIRKLICH AUSTESTEN -> MEHR MS!]{.red}. Horizontal scalability is beyond our capabilities due to the limitation of hardware resources available to us (single machine). As we've pointed out, the actor and microservice programming models are also capable of mobility and elasticity in principle. However, we will not explore these two forms of scalability, since the Echo implementations are not capable of them.

...
-->

To ensure that both implementation variants have the same resources available, we assign each architecture component with a fixed number of threads. The Akka components use a `Dispatcher`{language:scala} backed by a `ThreadPoolExecutor`{language:java} (in contrast to a so-called `ForkJoinExecutor`{language:scala} with dynamic pool size). Subsequently, the Spring IoC container is configured to use a `ThreadPoolTaskExecutor`{language:java}, where the `corePoolSize` is equal to the `maxPoolSize`. If not stated otherwise, we use 16 threads per component.

~ todo
We use the H2 database system in the experiment setup.
~

### Benchmark Results

HINWEIS: Alle components haben 16 fixe threads

For the benchmark, we provide each architecture component with a fixed number of threads available to them. If not states otherwise, the default thread-pool size for every component is 16. This way, we can reason about which underlying programming model better utilizes the available thread resources. 


#### Experiment 1: Indexing Subsystem

~ Figure { #fig-eval-index-overall; caption: "Overall processing time of indexing feeds"; width:100%; }
![img-eval-index-overall]
~

[img-eval-index-overall]: graphics/eval-index-overall.[pdf,png] "Image about eval-index-overall" { height:8cm; vertical-align:middle; padding-bottom:1em; }

...


~ Figure { #fig-eval-index-rtt-overall; caption: "Round trip time of messages in the indexing phase"; width:100%; }
![img-eval-index-rtt-overall]
~

[img-eval-index-rtt-overall]: graphics/eval-index-rtt-overall.[pdf,png] "Image about eval-index-rtt-overall" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

~ Figure { #fig-eval-index-rtt-meanMsgL; caption: "Mean message latency of messages in the indexing phase"; width:100%; }
![img-eval-index-rtt-meanMsgL]
~

[img-eval-index-rtt-meanMsgL]: graphics/eval-index-rtt-meanMsgL.[pdf,png] "Image about eval-index-rtt-meanMsgL" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

---

Figure [EINFÜGEN]{.red} is suprising at first glance. However, recall the indexing pipeline from Section [#sec-subsystem-pipelines]. The most computationally expensive domain-specific jobs have the Crawler (loading XML feeds, from local SSD storage in this benchmark), and the Parser (process relatively large XML documents). We expect these two tasks to take the most time for each message. The Parser has another effect to the pipeline. Due to the Parser's domain logic and our test feeds, for each message this component receives it produces 71 new messages (70 for the episodes metadata and 1 for the podcast metadata) for the subsequent component (CatalogStore) in the pipeline. Since the Catalog forwards parts of the metadata to the IndexStore, the IndexStore also receives 71 times more messages that an Updater, Crawler or Parser.

We can draw several conclusions from Figure [EINFÜGEN]{.red}. The messages per second of CatalogStore is the lower bound of the capacity potential of each progrogramming model. We do not know the full amount of their MPS capacity, since the domain scenario provides a natural message limit. We can also conclude that all other components could reach this lower bound of the Catalog, if they were not hindered by the internal domain logic.  

Since we know which components cannot exploit their model's potential, we can use these limited components to evaluate structural scalability effects. Especially the Crawler is suitable, since its domain logic is basically IO. The actor-based implementation of the Crawler utilizes child delegation to improve the IO efficiency, and with more threads can more children execute in parallel. For the microservice-based component, the IoC container can consume more messages and execute respective IO tasks through the `TaskExecutor`{language:java} with more threads available to it.

We reconfigure the previous components in the pipeline with significantely more threads to ensure that the Crawler never suffers from temproral starvation, and benchmark the MPS for the Crawler with different underlying thread-pool sizes. The result is shown in Figure [EINFÜGEN]{.red}  





#### Experiment 2: Retrieval Subsystem

~ Figure { #fig-eval-search-overall; caption: "Overall processing time of search requests"; width:100%; }
![img-eval-search-overall]
~

[img-eval-search-overall]: graphics/eval-search-overall.[pdf,png] "Image about eval-search-overall" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

~ Figure { #fig-eval-search-rtt-overall; caption: "Round trip time of messages in the retrieval phase"; width:100%; }
![img-eval-search-rtt-overall]
~

[img-eval-search-rtt-overall]: graphics/eval-search-rtt-overall.[pdf,png] "Image about eval-search-rtt-overall" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...

~ Figure { #fig-eval-search-rtt-meanMsgL; caption: "Mean message latency of messages in the retrieval phase"; width:100%; }
![img-eval-search-rtt-meanMsgL]
~

[img-eval-search-rtt-meanMsgL]: graphics/eval-search-rtt-meanMsgL.[pdf,png] "Image about eval-search-rtt-meanMsgL" { height:7cm; vertical-align:middle; padding-bottom:1em; }

...


### Relevance of the Benchmark


To the authors knowledge, there exists not benchmark comparing Akka actors and Spring-based microservices yet. We were only able to find one project on GitHub [^fn-msa-framework-benchmark] which benchmarks popular microservices frameworks using latency (RTT of a single request/work package), throughput and transfered data. The benchmark results include Spring Boot with the Undertow webserver and Akka HTTP. The used experiment setup is rather simple. The frameworks are merely used to serve "*Hello World!*" on a REST interface as the workload, which does not resemble a real world workload scenario. [Their results show a 3.18 times faster average latency and 2.98 times more throughput.]{.red}

...

Especially a lack of different interaction modes in microservice architecture benchmarks has been reported [@Zho18]. Most available benchmarks merely focus on one interaction mode, while MSA-related problems have been found to originate from asynchronous and mixed communication setups. Echo's subsystems engage this circumstance, since the indexing subsystem is modeled in an asynchronous fashion, and the retrieval subsystem in a synchronous fashion.

~ todo
Hier noch ein paar Wörter ob ich etwas gefunden habe bzgl der sync/async Geschichte
~


### Threats to Validity

~ todo
hier ein paar Blabla worte
~

#### External Threats to Validity 


The external threats concern how much our results are generalizable. The domain-specific actions of our scenario have an influence on the performance. Examples are the HTTP retrieval of RSS feeds, XML parsing and database IO. These actions dampen e.g.\ the throughput. The performance of these actions is influenced by the utilized underlying technologies (HTTP library, XML parser, database system). The comparability of benchmark metrics across systems is threatended. We mitigated this threat by founding all task units on the JVM. We provide all components with the same Core library, which implements the domain-specifiy functionality. The Catalog access the same kind of database system, and both implementations use the Spring Data JPA library for database interaction. Platform- and domain-specific effects are therefore uniform across the system implementations. Nevertheless, due to the domain influence, no general statement about the performance of Akka or Spring-based services is possible. Other metrics are not even measureable in the scenario. Examples are the creation time and maximum process support as reported in [@Val18]. The static configuration of our scenario does not intend elasticity, i.e.\ a dynamic creation of task units. Hence these metrics require an experiment outside the bounds of our scenario. 

Additionally, we conducted the experiments merely on a single multicore machine. The experiments therefore only incorporate the effect of vertical scalability leveraged from general concurrency and parallelism on one single machine. Also, the multicore machine has a very small number of cores. The test setup does not take into account the horizontal scalability effects of distribution-induced concurrency.

We eliminated the threat of selection bias by not using real world RSS feeds for simulation. Instead, we used test data with uniform size and feed structure.

<!--
Microservices are not a model of classic concurrency theory, and common benchmark metrics are difficult to apply. An example is maximum process support as reported in [@Val18]. The static configuration of our scenario does not intend elasticity, i.e.\ a dynamic creation of task units. Hence this metric requires an experiment outside of our scenario. Furthermore, from a theoretical point of view, we'd expect a very high upper limit of Akka actors, since an actors are modeled in the object space of OOP. Microservices are OS processes and in our scenario they require network communication. The maximum number of network ports is therefore also an upper limit for the microservices we can instantiate. The network port range is operating system dependend, but we expect it somewhere in the tens of thousands in general.
-->


#### Internal Threats to Validity 

The internal threats to the validity of this benchmark concern the accuracy of our data collection tool. Since we did not find a tool that is sufficient to collect the data for Akka actors and Spring-based microservices, we developed a custom benchmark framework. This toolkit is implemented to the best of our knowledge, but we have to assume that its efficiency is not state of the art. This threat is mitigated since both systems are subject to the same suspected inefficiency, which does not distort the relative comparison of metrics.  

An additional threat is resulting from the fixed amount of threads we provide for each component in the benchmark. Actor-based components can fully leverage all these threads, e.g.\ for child actors or futures. We've discussed in Section [#sec-internal-service-concurrency] that the microservices use a different `TaskExecutor`{language:java} for asynchronous tasks than the standard thread-pool of Spring's IoC container. To ensure the thread limit for the component, we have to split up the threads between the two thread-pools. There is the threat that a service's implementation does not distribute the computational load equally between the two thread-pools. We did not measure the thread-partitioning required for ideal thread utilization. Instead, we distributed the available threads evenly between both pools for all services. This threat is only mitigated for services which do not apply asynchrony internally. Then there is only the standard `TaskExecutor`{language:java} with the full amount of threads in place. 

Furtermore, Akka does not serialize messages [REF?]{.red} if the sending and receiving actors are inside the same JVM. Every actor message is then simple in-memory reference sharing of immutable objects, while each microservice message is definitely serialized and sent via a network interface (although the message is always delivered on the same machine).

